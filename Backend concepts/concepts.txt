## Explain in brief what is node.js?

=> Node.js is an open-source, cross-platform, server-side JavaScript runtime environment built on the V8 engine of Google Chrome.
=> It enables developers to use JavaScript for both server-side and client-side scripting. Node.js was created to provide a more efficient way to build scalable network applications, as it allows developers to write JavaScript code that runs outside of a web browser, which makes it ideal for developing high-performance and event-driven applications such as web servers, real-time chat applications, and APIs.
=> Node.js is built on top of JavaScript and provides a rich set of built-in modules that make it easy to build scalable and reliable applications.
=> Some popular frameworks that are built on top of Node.js include Express.js, Koa.js, and Nest.js.
=> Node.js has gained significant popularity in recent years and is widely used by developers worldwide for building web applications and services.

-----------------------------------------------------

## What is non-blocking vs blocking?

Blocking and non-blocing are the terms used in CS and refer to how the system / process behaves when waiting for a particular event or resource.

#BLOCKING:-

-In a blocking system , when a process requests a resource , its blocked or put on hold untill the resources becomes available. 
During this time process cannot perform any other tasks.
-In the backend, blocking code can cause significant delays and poor performance, 
especially when the server is handling multiple requests simultaneously. 
-If a request is blocked due to long-running code, it can prevent other requests from being processed until the blocking code has completed. This can result in slower response times, longer wait times, and increased resource usage.
-In Node.js, JavaScript that exhibits poor performance due to being CPU intensive rather than waiting on a non-JavaScript operation, such as I/O, isn't typically referred to as blocking.
-All of the I/O methods in the Node.js standard library provide asynchronous versions, which are non-blocking, and accept callback functions. Some methods also have blocking counterparts, which have names that end with Sync.
  eg.
const fs = require("fs");
const data = fs.readFileSync("./data.txt");

#NON-BLOCKING

-In contrast, in a non-blocking system, when a process requests a resource, it continues to run and perform other tasks while waiting for the resource to become available. If the resource is not available, the process may receive an error message or a signal indicating that the resource is not available, and it can choose to retry the request later or perform some other action.
Non-blocking code can help improve server performance and responsiveness. By using non-blocking techniques such as asynchronous programming, the server can handle multiple requests simultaneously without being blocked by long-running code.
-In non-blocking i/o operations allow the calling thread to continue executing while the I/O operation is in progress. This can be seen in scenarios such as web servers, where a request can be handled asynchronously without blocking the thread.
eg.

const fs = require("fs");
fs.readFile("/file.md", (err, data) => {
  if (err) throw err;
  console.log(data)
});

--------------------------------------------------

## 2. What is Throughput?

Defination:-Throughput refers to the amount of data or work that can be processed by a system or component within a given period of time. It is a measure of the efficiency and capacity of a system to handle a certain workload or traffic.
-It can understood by number of transactions or operations that can be processed by a system or application in a given period of time. For example, a database system may have a high throughput if it can handle a large number of transactions per second.
In the context of Node.js backend development, throughput refers to the rate or volume of work that a Node.js server can handle within a given timeframe. It is a measure of the server's ability to process incoming requests and respond to them efficiently.

Throughput in a Node.js backend can be influenced by several factors, including processing power of the system, available memory, network bandwidth, and software architecture

## Techniques can used to high throughput:

(1) "Event-driven, non-blocking I/O": Node.js uses an event-driven, non-blocking I/O model, which allows it to handle multiple concurrent connections without blocking the main event loop. This enables high concurrency and efficient handling of incoming requests, leading to higher throughput.
    eg. fetching the data using async and await by event 'onclick'.

(2) "Asynchronous programming": Node.js encourages the use of asynchronous programming techniques, such as callbacks, Promises, and async/await, which allow for efficient handling of I/O-bound operations. This prevents the server from blocking while waiting for I/O operations to complete, resulting in higher throughput.

(3) "Scalability": Node.js applications can be designed to scale horizontally, meaning they can be distributed across multiple instances or nodes to handle increased load. This can be achieved using techniques such as clustering or load balancing, which allow for better utilization of resources and increased throughput.

(4) "Performance optimizations": Properly optimizing the performance of a Node.js application can significantly impact its throughput. This may involve techniques such as optimizing database queries, caching, minimizing unnecessary computations, and optimizing resource utilization.

(5) "Hardware resources": The hardware resources available to the Node.js server, such as CPU, memory, and network bandwidth, can also impact throughput. Higher hardware resources generally allow for higher throughput, as the server has more capacity to handle incoming requests.

(6) "Application design": The design of the Node.js application, including its architecture, code organization, and efficiency, can also impact throughput. Well-structured and optimized code, along with efficient handling of requests and responses, can contribute to higher throughput.

It's important to note that throughput is not the only consideration in evaluating the performance of a Node.js backend. Response time, latency, and resource utilization are also important factors to consider. A well-optimized Node.js backend should aim for a balance between high throughput, low latency, and efficient resource utilization to provide optimal performance for the specific requirements of the application.

----------------------------------------------------------

## What are CPU intensive tasks?

=> CPU-intensive tasks are computing tasks that require a lot of processing power from the central processing unit (CPU) of a computer or server.
=> These tasks involve performing a large number of calculations or processing large amounts of data, which can cause the CPU usage to spike and slow down other processes running on the system.

=> Examples of CPU-intensive tasks include:

-- Video encoding or rendering
-- Image processing or editing
-- Machine learning or data analysis
-- Cryptography or encryption
-- Large-scale data processing or batch jobs
-- Audio or video transcoding
-- Simulation or modeling
-- 3D rendering or animation
-- Game development or graphics processing
-- Virtualization or containerization

=> In general, any task that involves manipulating large amounts of data or performing complex calculations can be CPU-intensive and may require specialized hardware or software optimizations to run efficiently.

----------------------------------------------------

## How can you end up blocking your main thread in node.js?

=>In Node.js, the main thread is the event loop, which is responsible for processing I/O operations and executing callbacks for events such as network requests, timers, and file system operations. "Blocking the main thread can cause the event loop to become unresponsive", which can lead to poor performance and slow down other processes running on the system.

=>Here are some common ways in which the main thread can be blocked in Node.js:

    -- "Synchronous I/O operations": If you use synchronous versions of I/O operations like fs.readFileSync or process.stdin.readSync, they will block the main thread until the operation is completed, causing the event loop to stall and become unresponsive.

    -- "Long-running computations": If you have computationally intensive tasks that take a long time to execute, such as complex mathematical calculations or data processing, these tasks can block the main thread and cause the event loop to become unresponsive.

    -- "Infinite loops": If you have an infinite loop in your code that runs continuously without yielding control back to the event loop, this can block the main thread and cause the application to become unresponsive.

    -- "Recursive functions": If you have a recursive function that calls itself repeatedly without returning control to the event loop, this can cause the call stack to grow indefinitely and eventually lead to a stack overflow error, blocking the main thread.

=>To avoid blocking the main thread in Node.js, it's important to use asynchronous I/O operations, break up long-running computations into smaller chunks, and avoid infinite loops or recursive functions that don't return control to the event loop. You can also use worker threads or child processes to offload CPU-intensive tasks to separate threads or processes and keep the main thread free to process I/O operations and events.

----------------------------------------------------

## What is rate limiting?

=> Rate limiting is a technique used to control the amount of traffic that is sent or received by a network or application.
=> It is a way of limiting the number of requests that can be made over a certain period of time to prevent overload and ensure fair usage of resources.

=> In web development, rate limiting is often used to control the amount of traffic sent to a web server by limiting the number of requests that can be made by a client or IP address within a certain period of time.

=> This helps prevent denial-of-service attacks and ensures that resources are used efficiently.

=> For example, a rate limit of 100 requests per minute could be set for a particular API endpoint. Once the client has made 100 requests within a minute, any further requests would be blocked or throttled until the next minute begins. This can be done using techniques such as "token bucket", "leaky bucket", or "sliding window algorithms".

=> Rate limiting can also be used to prevent brute-force attacks on password-protected resources by limiting the number of failed login attempts that can be made within a certain period of time. "This helps protect user accounts from being compromised by attackers who attempt to guess passwords through automated scripts".

Overall, rate limiting is an important technique for controlling traffic and preventing overload in applications and networks. It helps ensure that resources are used efficiently and can improve the overall performance and reliability of the system


  // rate limit middlewares that limit the request per minut is 100 and after a minute it will resume to take a request and if within a minute we exhausted all 100 limits then it will again unaccessable till next minute

      const express = require("express");
      const { rate } = require("./middlewares/rate");

      const app = express();

      app.get('/api/getdata',rate(100,60*1000,60*1000), (req,res) => {
          res.status(200).send({"Message":"Get a Data "})
      })

      app.listen(8000, () => {
          console.log(`server is listening to port 8000`);
      })


      **middlewares/rate.js

      const rate = (limit, time, blockedTime) => {
    // We define a new function called `rateLimit` which takes three arguments: 
    // `limit` - the maximum number of requests allowed within `time` milliseconds.
    // `time` - the duration of the time window in milliseconds.
    // `blockedTime` - the duration of the blocked period in milliseconds.

    const requests = {};
    // We create an object called `requests` to store the request history for each IP address.

    setInterval(() => {
        // We create a new interval which runs every `time` milliseconds to remove old requests from the `requests` object.

        for (const ip in requests) {
            // We loop through each IP address in the `requests` object.

            const now = Date.now();
            // We get the current timestamp.

            const timeWindow = requests[ip].timeWindow;
            // We get the start of the current time window for the IP address.

            const blockedUntil = requests[ip].blockedUntil;
            // We get the end of the blocked period for the IP address.

            requests[ip].requests = requests[ip].requests.filter((time) => {
                return time > now - timeWindow;
            });
            // We remove all requests from the `requests` array for the IP address which are older than `timeWindow` milliseconds.

            if (requests[ip].requests.length === 0) {
                requests[ip].timeWindow = now;
            }
            // If there are no requests in the current time window for the IP address, we reset the start of the time window to the current timestamp.

            if (blockedUntil && blockedUntil <= now) {
                requests[ip].blockedUntil = null;
                requests[ip].requests = [];
                requests[ip].timeWindow = now;
            }
            // If the blocked period for the IP address has ended, we reset the request count, the start of the time window, and the end of the blocked period.
        }
    }, time);
    // We set the interval to run every `time` milliseconds.

    return (req, res, next) => {
        // We return a middleware function which takes three arguments: `req`, `res`, and `next`.

        const ip = req.ip;
        // We get the IP address of the client making the request.

        const now = Date.now();
        // We get the current timestamp.

        requests[ip] = requests[ip] || { requests: [], timeWindow: now };
        // If there is no entry in the `requests` object for the IP address, we create one and initialize the `requests` array and the start of the time window.

        if (requests[ip].blockedUntil && requests[ip].blockedUntil > now) {
            const remainingTime = Math.ceil((requests[ip].blockedUntil - now) / 1000);
            res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
        }
        // If the client is currently blocked, we send a 429 status code with a message indicating how long the client must wait before making more requests.

        else if (requests[ip].requests.length >= limit) {
            requests[ip].blockedUntil = now + blockedTime;
            const remainingTime = Math.ceil(blockedTime / 1000);
            res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
            // If the client has reached the request limit, we block the client for the specified `blockedTime` and send a 429 status code with a message indicating how long the client must wait before making more requests
        } else {
            requests[ip].requests.push(now);
            next();
        }
    }
};

module.exports={rate}   


----------------------------------------------------

## What is a process.tick?

As we try to understand the Node.js event loop, one important part of it is process.nextTick(). Every time the event loop takes a full trip, we call it a tick.

When we pass a function to process.nextTick(), we instruct the engine to invoke this function at the end of the current operation, before the next event loop tick starts:

process.nextTick(() => {
  // do something
});

The event loop is busy processing the current function code. When this operation ends, the JS engine runs all the functions passed to nextTick calls during that operation.

It's the way we can tell the JS engine to process a function asynchronously (after the current function), but as soon as possible, not queue it.

=> Calling setTimeout(() => {}, 0) will execute the function at the end of next tick, much later than when using nextTick() which prioritizes the call and executes it just before the beginning of the next tick.

=> Use nextTick() when you want to make sure that in the next event loop iteration that code is already executed.

        => console.log("Hello => number 1");

        setImmediate(() => {
          console.log("Running before the timeout => number 3");
        });

        setTimeout(() => {
          console.log("The timeout running last => number 4");
        }, 0);

        process.nextTick(() => {
          console.log("Running at next tick => number 2");
        });

        // Output

        Hello => number 1
        Running at next tick => number 2
        Running before the timeout => number 3
        The timeout running last => number 4

-----------------------------------------------------

## When can process.tick starve your event loop?

The process.nextTick () is a method that adds a callback function to the start of the next event queue. It is used to defer the execution of a function until the next loop iteration.

The Node.js event loop is what allows Node.js to perform non-blocking I/O operations by offloading operations to the system kernel whenever possible. The event loop has several phases, each with a FIFO queue of callbacks to execute.

A tick is a term that refers to one iteration of the event loop, where it executes any callbacks that are in the queue for the current phase. A tick ends when the queue is emptied or the maximum number of callbacks has been reached.

Therefore, process.nextTick () can starve your event loop if you use it too much or if your callback functions are too long. This is because process.nextTick () callbacks are executed before the event loop moves to the next phase, and they can prevent other I/O operations from being processed in a timely manner. You should use process.nextTick () sparingly and only for cases where you need to run some code before anything else happens in the next tick.

------------------------------------------------------

## How can you create your own events?

// Create a new event target

const myEventTarget = new EventTarget();

// Define a custom event

const myEvent = new Event('myEvent');

// Add an event listener to handle the custom event

myEventTarget.addEventListener('myEvent', function(event) {
  console.log('Custom event triggered');
});

// Dispatch the custom event

myEventTarget.dispatchEvent(myEvent);


Example-2

    const event = new CustomEvent("build", { detail: elem.dataset.time });
    
    function eventHandler(e) {
  console.log(`The time is: ${e.detail}`);
}

------------------------------------------------------

## Creating and dispatching events dynamically:=>

Elements can listen for events that haven't been created yet:

<form>
  <textarea></textarea>
</form>


const form = document.querySelector("form");
const textarea = document.querySelector("textarea");

form.addEventListener("awesome", (e) => console.log(e.detail.text()));

textarea.addEventListener("input", function () {
  // Create and dispatch/trigger an event on the fly
  // Note: Optionally, we've also leveraged the "function expression" (instead of the "arrow function expression") so "this" will represent the element.

  this.dispatchEvent(
    new CustomEvent("awesome", {
      bubbles: true,
      detail: { text: () => textarea.value },
    })
  );
});


-----------------------------------------------------

## what is the difference between readFile and readFileSync ?

-The fs.readFileSync() and fs.readFile() methods are an 'inbuilt' applications programming interface of "fs module"which is used to read the file and return its content. 
Reading a file is a common thing in every progrmming languages and every language has its own way of reading files. here.., in node we have File system module (fs) which allows us to work with files.

=> both readFile() and readFileSync() reads the file at once and then delivers the content.
=> In fs.readFile() method, we can read a file in a non-blocking asynchronous way, but in fs.readFileSync() method, we can read files in a synchronous way, i.e. we are telling node.js to block other parallel process and do the current file reading process. 
-- The difference is that ,

readFile is non-blocking async.
readFileSync is blocking sync.

const fs=require('fs'); // imports file system module in variable fs

##readFile()

=>readFile() method uses a callback function. Due to its non-blocking nature, the program wont wait for the file to be read completely. So that the -->The end<-- message will be printed in first or in between the file content.

const fs=require('fs');
 fs.readFile('test.txt','utf-8',(err,data)=>{  //utf decoding to avoid binary code output.
        if(err){
            console.log(err);
        }
        else{
            console.log(data);
        }
    });
    console.log('-------the end------)

## readFileSync()
=>readFileSync() method uses a variable to store the file content and then delivers it. Due to its blocking nature, the program waits till the file is read completely. So that the -->The end<-- message will be printed only after the content is delivered.

  const fs=require('fs');
var contents = fs.readFileSync('test.txt', 'utf-8'); //utf decoding to avoid binary code output.
console.log(contents);
console.log(" -->The end<-- ");


### Conclusion

| fileRead()           | fileReadSync()            |
| -------------------- | ------------------------- |
| Non-Blocking process | Blocking process          |
| Uses callback uses   | variable to store content |
| Asynchronous nature  | Synchronous nature        |

----------------------------------------------------------------

##  How can you make a network request using http module?

-- Create the Server

1.Require the http module using require('http').

      const http = require('http');

2.Create an instance of the http.ClientRequest class using http.request(options, callback), where options is an object containing information about the request, and callback is a function that will be called when the response is received.

    const server = http.createServer((request, response) => {
      });


(3)The function that's passed in to createServer is called once for every HTTP request that's made against that server, so it's called the request handler. In fact, the Server object returned by createServer is an EventEmitter, and what we have here is just shorthand for creating a server object and then adding the listener later.
      When an HTTP request hits the server, node calls the request handler function with a few handy objects for dealing with the transaction, request and response.

      In order to actually serve requests, the listen method needs to be called on the server object. In most cases, we need to pass to listen is the port number you want the server to listen on. There are some other options too, so consult the API reference.

-- Method, URL and Headers

  When handling a request, the first thing we probably want to do is look at the method and URL, so that appropriate actions can be taken. Node.js makes this relatively painless by putting handy properties onto the request object.

  const { headers } = request;
const userAgent = headers['user-agent'];

  When receiving a POST or PUT request, the request body might be important to our application. Getting at the body data is a little more involved than accessing request headers. The request object that's passed in to a handler implements the ReadableStream interface. This stream can be listened to or piped elsewhere just like any other stream. We can grab the data right out of the stream by listening to the stream's 'data' and 'end' events.

  The chunk emitted in each 'data' event is a Buffer. If you know it's going to be string data, the best thing to do is collect the data in an array, then at the 'end', concatenate and stringify it.

-- HTTP Status Code

  -The HTTP status code on a response will always be 200. Of course, not every HTTP response warrants this, and at some point you'll definitely want to send a different status code. To do that, you can set the statusCode property.

-- Setting Response Headers

  Headers are set through a convenient method called setHeader.

  response.setHeader('Content-Type', 'application/json');
response.setHeader('X-Powered-By', 'bacon');

  When setting the headers on a response, the case is insensitive on their names. If you set a header repeatedly, the last value you set is the value that gets sent.

-- Sending Response Body

  response.write('<html>');
response.write('<body>');
response.write('<h1>Hello, World!</h1>');
response.write('</body>');
response.write('</html>');
response.end();


  It's important to set the status and headers before you start writing chunks of data to the body. This makes sense, since headers come before the body in HTTP responses.

### All togather

    const http = require('http');
    http.createServer((request, response) => {
    const { headers, method, url } = request;
    let body = [];
    request.on('error', (err) => {
    console.error(err);
    }).on('data', (chunk) => {
    body.push(chunk);
    }).on('end', () => {
    body = Buffer.concat(body).toString();
    // BEGINNING OF NEW STUFF

    response.on('error', (err) => {
      console.error(err);
    });

    response.statusCode = 200;
    response.setHeader('Content-Type', 'application/json');
    // Note: the 2 lines above could be replaced with this next one:
    // response.writeHead(200, {'Content-Type': 'application/json'})

    const responseBody = { headers, method, url, body };

    response.write(JSON.stringify(responseBody));
    response.end();
    // Note: the 2 lines above could be replaced with this next one:
    // response.end(JSON.stringify(responseBody))

    // END OF NEW STUFF

});
}).listen(8080);

## simple req 
    

      const http = require('http');

          const hostname = '127.0.0.1';
          const port = 3000;

      const server = http.createServer((req, res) => {
          //   res.statusCode = 200;
          //   res.setHeader('Content-Type', 'text/plain');

          res.writeHead(200, { 'Content-Type': 'text/plain' });

          if(req.url=="/"){
              res.end("Hello")
          }else if (req.url=="/hello"){
              res.end("Hello World")
          } else if (req.url === '/user') {
              res.end("User Data")
          }
          });

          server.listen(port, hostname, () => {
            console.log(`Server running at http://${hostname}:${port}/`);
          });


------------------------------------------------

## What are some HTTP response codes? what does it mean? 2xx, 3xx, 4xx, 5xx

HTTP status codes are classified into "5 broad categories". The status codes provide an easy way for clients to understand the results of their transactions.

As methods tell the server what to do, "status codes tell the client what happened". Status codes live in the start lines of responses. For example, in the line “HTTP/1.1 200 OK” the status code is 200.

HTTP response codes are three-digit status codes that are sent by a web server to "indicate the result of a client's request". These codes are a part of the HTTP protocol and provide information about the success, failure, or other conditions of the request-response cycle. Here are some common HTTP response codes and their meanings:

100 for informational purpose.

200 OK: This status code indicates that the request was successful, and the server has returned the requested data in the response.

201 Created: This code indicates that the request was successful, and a new resource has been created as a result. It is typically used in conjunction with POST requests.

204 No Content: This code means the request was successful, but there is no data to return in the response (e.g., for a successful DELETE request).

400 Bad Request: This code is sent when the server cannot process the request due to a client error, such as malformed syntax or invalid parameters.

401 Unauthorized: This status code indicates that the client must authenticate itself to get the requested response. It is used when the user's credentials are missing or invalid.

403 Forbidden: This code means that the client does not have the necessary permissions to access the requested resource.

404 Not Found: This status code is returned when the server cannot find the requested resource or endpoint.

500 Internal Server Error: This code indicates that an unexpected error occurred on the server while processing the request.

502 Bad Gateway: This status code is returned when the server acting as a gateway or proxy received an invalid response from the upstream server.

503 Service Unavailable: This code indicates that the server is currently unable to handle the request due to maintenance or overload. It is a temporary condition.

504 Gateway Timeout: This code is returned when the server acting as a gateway or proxy did not receive a timely response from the upstream server.

These are just a few examples of HTTP response codes. There are many other status codes with specific meanings that are used for different scenarios. Each status code provides valuable information to both clients and developers, helping to troubleshoot and handle requests appropriately. When working with APIs or web services, it's essential to understand the meaning of various HTTP response codes to handle errors and responses effectively.
-----------------------------------------------

## How to create a web server without express?**

-we can create server without using express by using the http module.
-We need to follow the steps like,
1.Require the http module using require('http').
2.Create an instance of the http.ClientRequest class using http.request(options, callback), where options is an object containing information about the request, and callback is a function that will be called when the response is received.
3.Send the request using the request.end([data]) method, where data is an optional argument containing data to send in the request body.
4.Handle the response in the callback function provided to the http.request() method.
=>Code
const http = require('http');
const options = {
  hostname: 'example.com',
  port: 8000,
  path: '/path/to/resource',
  method: 'GET'
};
const req = http.request(options, (res) => {
  console.log(`statusCode: ${res.statusCode}`);
  res.on('data', (chunk) => {
    console.log(`received ${chunk.length} bytes of data`);
  });

  res.on('end', () => {
    console.log('response ended');
  });
});

req.on('error', (error) => {
  console.error(error);
});

req.end();


---------------------------------------

## What are clusters?

-- When you run a Node.js program on a system with multiple CPUs, it creates a process that uses only a single CPU to execute by default. 
Since Node.js uses a single thread to execute your JavaScript code, all the requests to the application have to be handled by that thread running on a single CPU.
If the application has CPU-intensive tasks, the operating system has to schedule them to share a single CPU until completion. 
That can result in a single process getting overwhelmed if it receives too many requests, which can slow down the performance. 
If the process crashes, users won't be able to access your application.

-> As a solution, Node.js introduced the cluster module, "which creates multiple copies of the same application on the same machine and has them running at the same time". It "also comes with a load balancer that evenly distributes the load among the processes using the round-robin algorithm". 

-> In Node.js , a "cluster refer to built-in modules that allows you to create child processes", which are commonly reffered to as worker processes.
-> The cluster module enables you to run multiple instances of your Node.js application to take advantage of the multi-core systems commonly found in modern hardware.
-> By using clusters, you can distribute the incoming network traffic or workload across multiple child processes, thereby improving the overall performance and scalability of your Node.js application. Each child process in a cluster operates in its own single-threaded event loop, utilizing a separate CPU core. This allows the application to handle more requests concurrently and leverage the full power of the underlying hardware. 
-> It forks multiple instances of the application, and each instance becomes a worker process. The master process acts as a coordinator, distributing incoming connections or tasks among the workers.
-> Clusters also provide a mechanism for inter-process communication (IPC) between the master and worker processes. This allows them to share information and coordinate activities. For example, the master process can send messages to the workers to instruct them to perform certain tasks or update their state.
-> Clusters in Node.js can be particularly useful for building scalable web servers, real-time applications, or any application that can benefit from parallel processing. By utilizing multiple cores, you can significantly enhance the performance and responsiveness of your Node.js applications.

----------------------------------------

# What is Libuv?

-- "Libuv is a C library". 
-- "originally written for Node.js to abstract non-blocking I/O operations or to provide cross-platform support for features such as file system, acces, network programming and timers."
-- It is multi-platform support library with a focus on asynchronnus I/O, event driven programming and networking.
-- Libuv was initially developed by Bert Belder, Ryan Dahl, and others as part of the Node.js project to provide a consistent way to handle I/O operations across multiple platforms.
-- Event-driven asynchronous I/O model is integrated.
  It allows the CPU and other resources to be used simultaneously while still performing I/O operations, thereby resulting in efficient use of resources and network.
  It facilitates an event-driven approach wherein I/O and other activities are performed using callback-based notifications.

-- Libuv is a multi-platform library that provides asynchronous I/O operations, event loop, thread pool, and other low-level functionalities for building network and system applications. It was originally developed for the Node.js runtime and now serves as a core part of it.
-In addition to I/O operations, libuv also provides support for timers, signals, child processes, thread pool, and other features. It is licensed under the MIT license and is widely used in many projects, including Node.js, MongoDB, and Chromium.


-------------------------------------------------------------

### What are the different phases involved in event loop?

=> The event loop is a fundamental concept in Node.js that "allows for non-blocking I/O operations and event-driven programming". It is a mechanism that Node.js uses to handle I/O operations asynchronously, without blocking the execution of other code in the application.

=> The event loop runs in the main thread of the Node.js process and continuously monitors the event queue for incoming events. An event is simply a notification that a certain operation has completed or a certain condition has been met, such as a network request completing or a timer firing. When an event occurs, the event loop triggers the corresponding callback function and continues to process other events.

=> The event loop consists of several components:

    -- Event Queue: This is a queue of events waiting to be processed by the event loop. When an event is triggered, it is added to the end of the event queue.

    -- Event Loop: This is the core component of the event loop, which continuously checks the event queue for new events to process.

    -- Callback Queue: This is a queue of callback functions waiting to be executed. When an event is processed, its corresponding callback function is added to the callback queue.

    -- Call Stack: This is a stack of function calls that are currently being executed by the event loop. When a callback function is added to the callback queue, it is executed when the call stack is empty.

The event loop in Node.js enables asynchronous programming, which allows developers to write scalable and efficient applications that can handle a large number of I/O operations without blocking the main thread. By leveraging the event loop, Node.js applications can be built to handle high traffic, real-time applications such as web servers, chat applications, and APIs.


=> The event loop has four different phases(timers,i/o callback,idle/prepare,poll):

(1)Timers:->
  its responsible for the executing the callbacks registered by the setTimeout() and setInterval() functions.
  When the timer expires, the callback is added to the next phase of event loop.

(2)I/O callback:->
  This phase is responsible for executing the callback that were deffered during a previous I/O operation, such as reading/writing the data from a socket or file.
  When an I/O operations completes, its callback is added to the callback phases.

(3)Idle, prepare:->
  These phases are used for internal housekeeping tasks and are generally not used by most applications. 
  The idle phase runs callbacks registered by setImmediate(), while the prepare phase is an internal phase used to prepare for the next loop iteration.

(4)Poll:->
The poll phase is responsible for performing I/O operations, such as reading or writing data from a socket or file. 
If there are no I/O events to process, the poll phase will wait for events to occur.
Once the event loop completes all the phases, it will check if the setImmediate() queue has any pending callbacks to execute. If there are, the event loop will start over again from the first phase, otherwise, it will exit and wait for new events to occur.

#How the event loop perform in Node.js

When Node.js starts, it initializes the event loop, processes the provided input script which may make async API calls, schedule timers, or call process.nextTick(), then begins processing the event loop.
-- Event loop in Node.js consists of several phases that are executed sequentially in a loop to process incoming events and execute callbacks

-- Timers: In this phase, the callbacks of scheduled timers are executed. These timers can be set using the setTimeout() and setInterval() functions.

-- I/O callbacks: In this phase, the callbacks for I/O events such as network requests, file system operations, and other system events are executed.

------------------------------------------------------------

### What are timers in Node.js?

-> In Node.js, timers are a mechanism that allows you to schedule the execution of a function or piece of code at a specified time in the future. 
-> Timers module in Node.js contains functions that execute code after a set period of time.
-> Timers do not need to be imported via require(), since all the methods are available globally to emulate the browser JavaScript API.

->There are several types of timers in Node.js:

(1)setTimeout():->
-This function is used to execute a piece of code once after a specified delay, measured in milliseconds.
-- setTimeout(), the delay cannot be guaranteed because of operations that may hold on to the event loop.
-- setTimeout(), additional arguments can be added beyond the delay, and these will be passed on to the function call
eg.
function sayHello() {
  console.log('Hello, World!');
}

setTimeout(sayHello, 1000);


(2)setInterval():->
-This function is used to repeatedly execute a piece of code at a specified interval, also measured in milliseconds.
eg.
function doSomething() {
  console.log('Doing something...');
}

setInterval(doSomething, 5000);

(3)setImmediate():->
This function is used to execute a piece of code immediately "after the current event loop has finished".
eg.
setImmediate(() => {
  // run something
});

(4)process.nextTick():->
This function is used to "defer the execution of a piece of code until the next pass of the event loop".
eg.
function doSomething() {
  console.log('Doing something...');
}
process.nextTick(doSomething);

----------------------------------------------------

## What is NVM? how do you use it?

Node Version Manager(NVM)

-- "It is a tool that allows you to install and manage multiple versions of Node.js on your computer."
-Different projects on your device may be using different versions of Node. Using only one version (the one installed by npm) for these different projects may not give you accurate execution results.
-NVM allows you to switch between different Node.js versions and install new ones without affecting your existing setup.
-We can install NVM by following the instructions on the official NVM GitHub page. This will depend on your operating system.

 *How to Install NVM on Windows
-We can install NVM by following the instructions on the official NVM GitHub page. This will depend on our operating system.

1)Install a specific version of Node.js
-nvm install node
2)Switch the node versions
-nvm use <version>
3)- uninstall a specific version of Node.js using the nvm uninstall command.

--------------------------------------------------------------

## What is common.js? how is it different from es modules?

-> CommonJS modules are a popular way to modularize JavaScript code. They allow you to export and import modules using the "module.export" and "require".
-> ES Modules are a newer way to modularize JavaScript code.
-> CommonJS is a module specification for JavaScript used primarily in Node.js environments. It provides a way to define reusable code modules using a require() function to load dependencies and an exports object to expose functionality to other modules.
-> ES modules are a newer standard for JavaScript modules introduced in ECMAScript 2015 (ES6). ES modules use the import and export statements to load dependencies and expose functionality, respectively. ES modules can be used in both browser and Node.js environments, although some older browsers may not support them.
-> CommonJS (also known as CJS) is a module system used in Node.js and other environments that supports synchronous loading of modules. It is a standard for structuring JavaScript code into separate files, each containing a module that can be loaded and reused in other parts of an application.
It is the native module system of modern web browsers and is supported in Node.js since version 13.

It different from es modules :->

-- "CommonJS modules are loaded synchronously", which means that the "entire module is loaded before the code execution continues".

-- "ES modules, on the other hand, are evaluated asynchronously", which allows for better performance and more efficient loading of dependencies.

-- "CommonJS modules are evaluated at runtime", which means that the code inside the module is executed when it is loaded.

-- "CommonJS modules are not statically analyzable", which can lead to "slower performance" and 'more runtime errors'.

-- "ES modules are statically analyzable", which means that the 'JavaScript engine can parse and analyze the module structure at compile time', allowing for better optimization and error checking.

-- CommonJS is still widely used in Node.js environments due to its familiarity and backwards compatibility with older codebases.

-- Syntax: CommonJS uses require() and exports to load dependencies and expose functionality, respectively, while ES modules use import and export.

---------------------------------------------------------------------

### How does the crypto module work?

# crypto module is deprecated

-------------------------------------------------------------------

# What are web sockets?

-> WebSocket is 'bidirectional', a "full-duplex protocol" that is used in the same scenario of client-server communication, unlike HTTP ......
-> It provies  "real-time", "two-way communication "channels between a client (such as a web browser) and a server "over a single TCP connection".
-> Data can be sent and received from both ends at any time, without having to establish a new connection each time.
-> They provide a persistent connection that "allows for low-latency", "high-performance communication" between the client and the server, without the overhead of HTTP requests and responses.
-> With this API, you can send messages to a server and receive event-driven responses without having to poll the server for a reply.
-> To use WebSockets in a web application, the client and server both need to support the WebSocket protocol. The client typically uses a JavaScript library to create a WebSocket object and establish a connection to the server, while the server uses a WebSocket server library to handle incoming connections and communicate with clients.

  => uses :

-- Real-time web application like trading app.
-- Chat application
-- Gaming application

-----------------------------------------------------------------------

# What are microservices?

=> Microservices are an "architectural and organizational approach to software development" where "software is composed of small independent services that communicate over well-defined APIs".
-> In use of microservices large, monolithic applications are broken down into smaller, independent components that are responsible for a specific business capability and that can be developed, tested, and deployed independently.
-> Each microservice runs as a separate process and communicates with other microservices over a network, typically using lightweight protocols such as HTTP or message queues.

=> Benefits of using the microservices,

    (1) 'Scalability': Each microservice can be scaled independently, allowing organizations to handle spikes in traffic more easily.
    (2) 'Flexibility': Microservices can be developed and deployed independently, allowing for greater agility and faster time to market.
    (3) 'Resilience': A failure in one microservice does not necessarily affect the entire system, reducing the impact of downtime.
    (4)'Technology independence': Each microservice can be developed using different technologies and programming languages, allowing teams to use the best tools for each job.
    (5) 'Ease of maintenance': Each microservice is responsible for a specific business capability, making it easier to test, deploy, and maintain the system.

** However, implementing microservices can also introduce new challenges, such as increased complexity in managing the communication between services and the need for strong testing and monitoring practices. As with any development approach, there are trade-offs that need to be carefully considered and managed.

----------------------------------------------------

## How does express work?

  => Express is a popular 'web application framework' for Node.js that "simplifies the process of building robust and scalable web applications". It provides a set of features and tools that make it easier to handle routing, middleware, request/response handling, and more.

  => At its core, Express is built on top of the Node.js http module, which provides basic functionality for creating an HTTP server. Express enhances and extends the capabilities of the http module by providing a higher-level API and adding features like routing, middleware support, template engines, and error handling.

  => Here's a high-level overview of how Express works:

      (1) "Setting up the application":  
          To use Express, you first need to install it as a dependency in your Node.js project using npm or yarn. Then, you import the Express module into your application using require('express'). You create an instance of the Express application by invoking express() and store it in a variable.

      (2) "Defining routes": 
          Express allows you to define routes to handle different HTTP methods and URLs. You can use the app.get(), app.post(), app.put(), app.delete(), and other similar methods to define routes and specify the callback function that will be executed when a request matches a particular route. These routes can handle dynamic parameters and query parameters as well.

      (3) "Middleware": 
          Express uses middleware functions to perform various tasks during the request-response cycle. Middleware functions have access to the request and response objects and can modify them, execute additional code, or terminate the request-response cycle. You can use app.use() or specific HTTP method-based middleware functions to add middleware to your application. Express provides a wide range of middleware modules that handle common tasks such as parsing request bodies, handling sessions, authenticating requests, logging, and more. You can also create custom middleware functions as per your application's requirements.

      (4) "Handling requests and generating responses":
          When a request is received by the Express server, it goes through the defined routes in the order they are declared. If a matching route is found, Express executes the corresponding callback function, also known as a route handler, which has access to the request and response objects. You can use these objects to process the request data, interact with databases, perform calculations, and generate an appropriate response. The response can be in various formats, such as HTML, JSON, or files.

      (5) "Error handling":
          Express provides mechanisms for handling errors that occur during the request-response cycle. You can define error handling middleware functions that have an additional err parameter, allowing you to catch and handle errors. These middleware functions are typically defined after all the routes and other middleware.

      (6) Starting the server:
        Once you have defined the routes, middleware, and error handlers, you can start the Express server by calling app.listen() and specifying the port number on which the server should listen for incoming requests.

  => Express is highly flexible and can be extended with additional middleware and modules to suit your specific application needs. Its modular design and large ecosystem of middleware and plugins make it a powerful and widely used framework for building web applications with Node.js.


----------------------------------------------------

## Creating a CLI based app using Node.js and publish it

=> Steps need to follow to create the Command Line Intrface app :

      (1) create a new directory and initializes the app with npm

            -mkdir First-cli-app
            cd First-cli-app
            -initialise the app
            npm init

      (2) Install the dependencies

        npm i commander

      (3)Build app using cammander

        const { program } = require('commander');

        program
        .version('1.0.0')
        .description('A Simple CLI app')
        .option('-n,--name <name>', 'Your name')
        .option('-a, --age <age>', 'Your age')
        .action((options) => {
            console.log(`Hello, ` + options.name + ' !');
            console.log('Your age, ' + options.age + ' years old.');
        });

        program.parse(process.argv);
      
      (4) run the app locally

          node index.js --name Dilip --age 26

          // Hello, Dilip !
          // Your age, 26 years old.
      
      (5) to pusblish app we need to login the npm

      npm login

      (6) publish the app

        npm publish

      (6) Others can Install this app by 

      npm install -g First-cli-app

      (7) To run app globally

      First-cli-app Dilip
      //`Hello Dilip`

--------------------------------------------------------

## What are routes?

=> routes define how the server will handle incoming requests based on the URL path and HTTP method.
=> Routing refers to determining how an application responds to a client request to a particular endpoint, which is a URL (or path) and a specific HTTP request method GET, POST,DELETE and PATCH .

-> Routing allows targeting different routes or different URLs on our page.
-> Node.js has several popular frameworks such as Express.js that provide a simple and elegant way of defining and handling routes. Express.js allows you to define routes using its built-in routing system, which provides a clean API for defining and handling HTTP requests.
-> Express provides a simple and flexible way to create web applications and APIs using Node.js. By using its built-in middleware functions and third-party packages, you can quickly add common functionality to your app and focus on writing your own application logic.

    const express = require('express');
    const app = express();

      //GET
      app.get('/', (req, res) => {
        res.send('Hello chiku!');
      });
=> Routes can also include parameters that are specified in the URL path, such as IDs, usernames, or other dynamic data. 
    eg.
    app.get('/posts/:id', (req, res) => {
      // Render the blog post with the specified ID
    });

        //POST
        app.post('/submit', (req, res) => {
          // Process the form data and return a response
        });

        //PUT
        app.put('/update/:id',(req,res)=>{
        //update the data
        })

        //PATCH
        app.patch('/update/:id',(req,res)=>{
        //update the data
        })

        //DELETE
        app.delete('/delete/:id',(req,res)=>{
        //update the data
        })

    app.listen(6000, () => {
      console.log('App is live on 1010');
    });

------------------------------------------

## What are Middlewares?

=>A middleware is a software component that sits between an application's server and client to intercept, process, and "modify incoming and outgoing HTTP requests and responses".
=> - Middlewares are the functions that get executed before the request reaches the route handler and after the response is sent to the client.
=> It seats between request and response.
=> In the Node.js ecosystem, the Express framework uses middleware extensively to handle tasks such as authentication, logging, error handling, and more. 
=> - Because of the middleware, we get the chance to perform modification, updation, or deletion on request and response objects.
=> Using middlewares, we can :
    -- change the object
    -- check for security
    -- check for validation
    -- calculate the time taken etc.
=> Middlewares can be added to an Express application using the app.use() method, and they can be either built-in, third-party, or custom functions.

=> This middleware function takes three arguments: req (the request object), res (the response object), and next (a callback function that signals when the middleware has finished processing and the next middleware in the chain should be called). In this case, the middleware adds a timestamp property to the req object and calls next() to continue the request-response cycle.

    eg.
    function addTimestamp(req, res, next) {
      req.timestamp = Date.now();
      next();
    }
//use the middleware in a express app

    const express = require('express');
    const app = express();

    app.use(addTimestamp);

    app.get('/', (req, res) => {
      res.send(`Timestamp: ${req.timestamp}`);
    });

    app.listen(3000, () => {
      console.log('Server started on port 3000');
    });

=> There are `express inbuilt middlewares` :
    -- `express.json()` ⇒ it just basically parses the JSON. If we use it we don't need to parse it manually.
    -- `express.text()` ⇒ If we need to parse a text only.

-----------------------------------------------

## What is MVC framework?

 
 => MVC stands for Model-View-Controller, which is a software design pattern used to develop user interfaces and web applications.
 => (MVC) is an architectural pattern that separates an application into three main logical components: the model, the view, and the controller.
 =>In the MVC pattern, the application is divided into three components:

    (1) Model:
      The Model component corresponds to all the data-related logic that the user works with. This can represent either the data that is being transferred between the View and Controller components or any other business logic-related data
    (2) View:
      The View component is used for all the UI logic of the application. For example, the Customer view will include all the UI components such as text boxes, dropdowns, etc. that the final user interacts with.

    (3) Controller:
      Controllers act as an interface between Model and View components to 'process all the business logic' and incoming requests, manipulate data using the Model component and interact with the Views to render the final output.

----------------------------------------------

## How do you do validations?

Validations are an essential aspect of any web application to ensure that "the user input data is correct and meets the expected format and rules". In web development, validations can be performed on both the client-side and server-side.

(1) To begin, a sample of data is determined. ...
(2) Next, the dataset needs to be validated to confirm it contains all the required data.
(3) Finally, the source data's value, structure, format etc. is matched with the schema of the destination.
In the Node.js ecosystem, there are several libraries and tools available to handle validations in web applications, such as:
    (1) 'Validator': A library that provides a set of pre-built validation methods for 'strings, numbers, dates, and more'.
    (2) 'Joi': A powerful schema validation library that allows you to define complex rules and constraints for your data.
    (3) 'Express-validator': A middleware that provides a set of validation and sanitization methods for incoming data, and integrates with Express.

--------------------------------------------------

### How do you do static routing?**

-> Static routing is a form of "routing that occurs when a router uses a manually-configured routing entry, rather than information from dynamic routing traffic".
-> Static routing is a technique used in networking and web development to configure a routing table that "maps specific URLs or paths to static resources, such as HTML files, images, or scripts, that are stored on the server".

 => In Node.js, static routing can be implemented using the express.static() middleware function.
  eg.
    app.use(express.static('public'));

    app.use('/static', express.static(__dirname + '/public'));

-> The route consist of a destination prefix and a next-hop forwarding address. The static route is activated in the routing table and inserted into the forwarding table when the next-hop address is reachable.

--------------------------------------------------------

### What are some templating engines?***

Template engines are" software tools that help developers create dynamic web pages by allowing them to define templates and insert dynamic data into those templates".
These engines use a "combination of HTML, CSS, and programming logic to generate web pages" that can be rendered in a user's web browser.
Template engines are "mostly used for server-side applications that are run on only one server and are not built as APIs."
Template engines "help developers separate the presentation layer (the user interface) from the business logic layer" (the programming code that powers the application), making it easier to build scalable and maintainable web applications

EJS is one of the template engines used with Node JS to generate HTML markups with plain Javascript. EJS stands for ("Embedded JavaScript Templates"). It can be used both on the client and the server-side.

    They also provide features such as:

    (1) Conditional statements:
      allowing developers to conditionally display content based on certain conditions.
    (2) Loops:->
      enabling developers to iterate over collections of data and display them in a structured way.
    (3) Inheritance:->
      allowing developers to define a base template and extend it in child templates.
    (4) Variables and expressions:->
      allowing developers to define and manipulate variables within templates.
    (5) Partials:->
      allowing developers to define reusable portions of templates and include them in other templates.

=> Some popular templating engines include:

  (1) Mustache: A simple and lightweight engine that supports a variety of programming languages.

  (2) Handlebars: A popular engine that adds more features on top of Mustache, such as helper functions and partials.
  (3) Pug (formerly known as Jade): A powerful engine that uses indentation-based syntax to generate HTML.

  (4) EJS (Embedded JavaScript): An engine that allows you to embed JavaScript code directly into your templates.

  (5) Twig: A flexible and extensible engine that is used in the Symfony PHP framework.

--------------------------------------------------

## How do you manage sessions in express?

=> Session management is an important aspect of web application development, as it allows you to keep track of user activity and maintain user state across requests. In Express.js, session management can be implemented using the express-session middleware.

    //npm i express-session

    app.use(session({
      secret: 'secret-key',
      resave: false,
      saveUninitialized: false
    }));

-> In the example above, we are using the MemoryStore as the session store, which stores the session data in the server's memory. secret is used to sign the session ID cookie and should be a long random string. resave and saveUninitialized are optional options that control the behavior of the session middleware.

    //To set a value in the session, use the req.session object.

    app.get('/login', (req, res) => {
        req.session.username = 'John Doe';
        res.send('Logged in successfully');
    });


    full code snippet;

        // npm i express-session
        
          const session = require('express-session');

          const express = require('express');
          const app = express();

          // app.set('trust proxy', 1)

          app.use(session({
              secret: 'keyboard',
              resave: false,
              saveUninitialized: true,
              // cookie: { secure: true }
          }));

          app.get('/login', (req, res) => {
              req.session.username = 'Dilip';                    
              res.send('logged in successfully')                 Output   // logged in successfully
          });


          app.get("/profile", (req, res) => {
            const username = req.session.username;
            res.send(`Username: ${username}`);                    Output  // Username:Dilip
          });
          app.listen(8000, () => {
              console.log('Server is running on port 8000');
          })

---------------------------------------------------------------

## How do you manage cookies with express?

Cookies can be managed using the cookie-parser middleware. This middleware parses cookie headers and populates the req.cookies object with cookie values.

    //steps involved
    (1)
      -npm i cookie-parser

    (2)Initialise the cookie-parser

    var express = require('express');
    var cookieParser = require('cookie-parser');
    var app = express();
    app.use(cookieParser());

    (3)To set a cookie, use the res.cookie() method.

        app.get('/set-cookie', (req, res) => {
        res.cookie('username', 'John Doe', { maxAge: 900000, httpOnly: true });
        res.send('Cookie has been set');
        });

   //The httpOnly option ensures that the cookie cannot be accessed using JavaScript

  (4) To get a cookie, use the req.cookies object.

        app.get('/get-cookie', (req, res) => {
        const username = req.cookies.username;
        if (username) {
            res.send(`Hello, ${username}`);
        } else {
            res.send('Cookie not found');
        }
    });

  (5) To delete a cookie, use the res.clearCookie() method.
  app.get('/clear-cookie', (req, res) => {
      res.clearCookie('username');
      res.send('Cookie has been cleared');
  });

-- By default, cookies are stored in the client's browser memory.

-------------------------------------------

## What are common libraries you work with express?***

The common libraries you work with express are as follows :->
-- Gulp
-- Async.Js
-- Async
-- morgon: 
          Morgan is an HTTP request level Middleware. It is a great tool that logs the requests along with some other information depending upon its configuration and the preset used. It proves to be very helpful while debugging and also if you want to create Log files.
              
          // npm i morgan
              const logger = require('morgan');
              app.use(logger('dev'));

-- mongoose
-- Request.
-- Browserify
-- PM2
-- body-parser
-- Parser
-- cookie-parser
-- helmet
-- passport
-- multer
-- express-validator
-- nodemailer

------------------------------------------------

##   What is cors ?

-- CORS stands for Cross-Origin Resource Sharing. It is a mechanism that 'allows a web page to make requests to a different domain than the one that served the web page'.
-- Web browsers implement a security feature called the "Same-Origin Policy," which restricts web pages from making requests to a different domain than the one that served the web page. This security feature is intended to prevent malicious attacks such as cross-site scripting (XSS) and cross-site request forgery (CSRF).
-- CORS allows web pages to bypass the Same-Origin Policy and make requests to a different domain, but only if the server that receives the request explicitly allows it. CORS works by adding HTTP headers to the response that inform the web browser whether the request is allowed or not.

  CORS Headers: CORS is implemented through HTTP headers that are sent by the server in the response to a cross-origin request. 
      
      These headers include:

  (1) Access-Control-Allow-Origin: Specifies the domain(s) that are allowed to make cross-origin requests. This header indicates which domain(s) are allowed to access the requested resources. The value can be a specific domain, "*", which allows any domain to access the resource, or null, which indicates that no domain is allowed to access the resource.

  (2) Access-Control-Allow-Methods: Specifies the HTTP methods that are allowed for the cross-origin request. This header indicates which HTTP methods (such as GET, POST, PUT, DELETE, etc.) are allowed for the request.

  (3) Access-Control-Allow-Headers: Specifies the headers that are allowed in the cross-origin request. This header indicates which additional headers (such as Authorization, Content-Type, etc.) are allowed in the request.

  (5) Access-Control-Allow-Credentials: Specifies whether the cross-origin request can include credentials (such as cookies, HTTP authentication, and client-side SSL certificates). This header indicates whether the server allows the request to include credentials or not.

  (6) Access-Control-Max-Age: Specifies the maximum time (in seconds) that the results of a preflight request can be cached. This header indicates how long the CORS preflight response can be cached by the browser.

  (7) CORS Types: CORS can be classified into two types: simple requests and preflighted requests.

      (i) Simple requests: These are HTTP requests that meet certain criteria and do not require a preflight request. Simple requests include HTTP methods like GET, POST, and HEAD, and have certain content types. Simple requests do not trigger a preflight request and are directly handled by the server.

      (ii) Preflighted requests: These are HTTP requests that do not meet the criteria for simple requests and require a preflight request. Preflighted requests are used for more complex requests, such as those that use HTTP methods like PUT, DELETE, or have custom headers. The preflight request is an HTTP OPTIONS request that checks if the server allows the actual request from the domain of the web page, and the actual request is sent only if the preflight request is successful.

  (10) CORS in Server-side and Client-side: CORS is implemented both on the server-side and the client-side.

      (a) Server-side: Web developers need to configure their server to include the appropriate CORS headers in the response to cross-origin requests. This involves adding the necessary headers, such as Access-Control-Allow-Origin, Access-Control-Allow-Methods, etc., to the server's HTTP response.

        eg. 
            
            const express = require('express');

              const cors = require('cors');

              const app = express();

              app.use(cors());

              // Your routes and other middleware

              app.listen(3000, () => {
                console.log('Server is running on port 3000');
              });

      (b) Client-side: Web developers need to make sure that the client-side code, such as JavaScript, that sends cross-origin requests includes the necessary credentials and handles CORS-related errors appropriately. This may involve handling CORS errors, including the appropriate CORS headers in requests, and configuring the client-side code to work with cross-origin resources.

            eg. 
              
              fetch('http://localhost:3000/api/data', {
                headers: {
                  'Content-Type': 'application/json',
                  'Access-Control-Allow-Origin': 'http://localhost:3000'
                }
              })
                .then(response => {
                  // Process the response
                })
                .catch(error => {
                  // Handle the error
                });

  In conclusion, CORS is a security mechanism implemented in web browsers that controls cross-origin requests to prevent unauthorized access to resources. It involves both server-side configuration and client-side handling, and it is an important consideration for web developers when building modern web applications that interact with cross-origin resources.

---------------------------------------

## Explain all the different HTTP methods?
  
  HTTP (Hypertext Transfer Protocol) defines several methods or verbs that indicate the desired action to be performed on a resource. Each HTTP method has a specific purpose and behavior. Here are the most common HTTP methods:

    (1) GET: The GET method is used to retrieve a representation of a resource from the server. It requests the server to send back the specified resource in the response. "GET should be used for safe and idempotent operations, meaning 'it should not have any side effects on the server or the resource'."

    (2) POST: The POST method is used to submit data to the server to create a new resource. It sends data in the request payload, typically used for form submissions, file uploads, or other actions that result in a change on the server. "POST is not idempotent as multiple requests may create multiple resources."

    (3) PUT: The PUT method is used to update or replace an existing resource on the server. It sends the entire updated representation of the resource in the request payload. If the resource does not exist, PUT can create a new resource with the specified identifier. "PUT is idempotent, meaning that multiple identical requests should have the same result as a single request".

    (4) DELETE: The DELETE method is used to delete a specified resource on the server. It requests the server to remove the resource identified by the URL. "DELETE is idempotent, meaning that multiple identical requests should have the same result as a single request".

    (5) PATCH: The PATCH method is used to partially update a resource on the server. It sends a set of changes or instructions to be applied to the existing resource. Unlike PUT, which requires sending the entire updated representation, PATCH allows sending only the modified fields or instructions. "PATCH is not guaranteed to be idempotent."

    (6) HEAD: The HEAD method is similar to GET but only retrieves the response headers, without the response body. It is often used to retrieve metadata or check the status of a resource without downloading its entire contents.

    (7) OPTIONS: The OPTIONS method is used to retrieve the communication options available for a resource or server. It allows the client to determine the supported methods, headers, and other capabilities of the server.

    These are the most commonly used HTTP methods. It's important to note that the usage of these methods should align with their intended purpose and follow the principles of RESTful web services.

---------------------------------------

##  What are HTTP headers?

HTTP headers "are additional pieces of information sent along with an HTTP request or response". They provide metadata about the request or response and help in conveying important details and instructions between the client and the server. HTTP headers consist of a key-value pair and are included in the header section of an HTTP message.

There are two main types of HTTP headers:

    (1) Request Headers: Request headers are included in an HTTP request sent from the client to the server. They provide information about the request itself, the client, and any additional instructions.
        
        Some commonly used request headers include:

        (a) User-Agent: Specifies the client application or browser making the request.
        (b) Accept: Indicates the media types (MIME types) that the client can handle in the response.
        (c) Content-Type: Specifies the media type of the data sent in the request payload.
        (d) Authorization: Contains credentials or tokens for authentication purposes.
        (e) Cookie: Contains client-side cookies to be sent to the server.

    (2) Response Headers: Response headers are included in an HTTP response sent from the server to the client. They provide information about the response, its status, and any additional instructions.
    
    Some commonly used response headers include:

      (1) Content-Type: Specifies the media type of the response content.
      (2) Content-Length: Indicates the length of the response content in bytes.
      (3) Cache-Control: Provides caching instructions for the client or intermediary caches.
      (4) Location: Used in redirect responses to indicate the new location of a resource.
      (5) Set-Cookie: Sets a cookie on the client-side for future requests.

HTTP headers are flexible and can be extended to include custom headers specific to the application or domain. They play a crucial role in various aspects of HTTP communication, such as content negotiation, authentication, caching, and controlling the behavior of clients and servers.

Both clients and servers can include multiple headers in their respective requests or responses to convey specific information and facilitate effective communication between the two parties.

--------------------------------------

## Q.  What is testing?

  -- Testing in Node.js backend development involves the process of verifying the correctness, reliability, and functionality of Node.js applications through various types of tests, such as unit testing, integration testing, API testing, performance testing, security testing, and end-to-end testing.
  -- Testing is an essential part of the software development lifecycle and helps to identify and fix defects, validate requirements, and ensure that the backend logic and functionalities of a Node.js application are working as expected.
  -- There are various testing frameworks and tools available for Node.js backend testing, and it is crucial to ensure the quality and reliability of the application in production.

------------------------------------

## What is unit testing?

  -- Unit testing is a crucial practice in software development that involves testing individual units of code in isolation to ensure their correctness and reliability.
  -- It includes writing test cases, setting up a test environment, executing tests, and analyzing results.
  -- Unit testing catches defects early, improves code quality, speeds up debugging, development, and maintenance, and serves as documentation and a safety net for expected code behavior.
  -- It helps identify and fix issues promptly, leading to faster development cycles and shorter time-to-market for software products.
  -- Unit tests also aid in easier maintenance of software by acting as documentation of expected behavior and validating code changes.
  -- By incorporating unit testing into the development process, software teams can ensure the correctness and reliability of their code, leading to higher quality software and better customer satisfaction.

--------------------------------------------

## Q. What is functional testing?

  -- Functional testing is a type of software testing that focuses on "testing the functionality of a software application" or 'system' to ensure that it meets the requirements and specifications, performs the expected tasks accurately, and behaves correctly in real-world scenarios.
  -- It is essential for ensuring the reliability and quality of software applications and validating that they meet the expectations of end users.

-----------------------------------------------

## Q.  What is HTTPS? what is the difference between HTTP and HTTPS?***

  -- HTTPS stands for "HyperText Transfer Protocol Secure", and it is a secure version of HTTP.
  -- HTTP transmits data in plain text, while HTTPS encrypts data to protect privacy and integrity.
  -- HTTPS uses SSL/TLS certificates to establish a secure connection between a client and a server.
  -- HTTPS provides data privacy, integrity, and authentication, preventing data breaches and unauthorized access.
  -- Websites using HTTPS display a padlock icon, building trust with users.
  -- HTTPS is increasingly required for websites handling sensitive data or conducting online transactions.
  -- Main differences between HTTP and HTTPS are encryption, SSL/TLS certificates, and advantages of data protection, trust, and compliance.

--------------------------------------------------

## Q. What is SSL/TLS? ***

  (1)  SSL ("Secure Socket Layer"):

  -- SSL is "a cryptographic protocol used to establish a secure and encrypted connection between a client and a server" over the internet.
  -- SSL provides 'encryption', 'authentication', and 'integrity' to secure online communication.
  -- SSL encrypts data transmitted between the client and the server, ensuring privacy and confidentiality.
  -- SSL uses digital certificates for authentication, verifying the identity of the server and preventing man-in-the-middle attacks.
  -- "SSL ensures data integrity through message digests and digital signatures."
  -- SSL has been widely used for securing web browsing, email, file transfer, and other data transmission.
  -- SSL has largely been replaced by the more secure TLS (Transport Layer Security) protocol.
  -- The term "SSL" is often used colloquially to refer to both SSL and TLS, as they are closely related and share similar features.

  (2) TLS ("Transport Layer Security") :

  -- TLS is a "cryptographic protocol for secure and encrypted communication".
  -- It provides encryption, authentication, and integrity to protect data transmitted over the internet.
  -- TLS encrypts data to ensure privacy and confidentiality.
  -- It uses digital certificates for server authentication to prevent man-in-the-middle attacks.
  -- TLS ensures data integrity through message digests and digital signatures.
  -- It is widely used for securing web browsing, email, file transfer, and other data transmission.
  -- TLS has different versions, and it is important to use the latest and most secure version.
  -- TLS is a crucial technology for ensuring the security of online transactions and communication.


  *** How we can use SSL/TLS in Node.js
      
      1. Generate SSL/TLS certificates:
          We need to obtain SSL/TLS certificates, including a private key and public key certificate, either by generating self-signed certificates or obtaining them from a trusted certificate authority.  For testing and development purposes, you can generate self-signed certificates using tools like OpenSSL.
      
      2. Set up a Node.js server with HTTPS:
          Start by requiring the https module and creating an instance of the server:

          const https=require('https');
          const fs=require('fs');

          const options={
            key:fs.readFileSync('path/to/private/ket.pem'),
            cart:fs.readFileSync(path/to/public/certificates.pem)
          };

          const server=https.createServer(options,(req,res)=>{
            //handle requests
            res.writeHead(200, { 'Content-Type': 'text/plain' });
            res.end('Hello, world!');
          });

          server.listen(8000,()=>{
            console.log('Server is running on port 8000')
          })

--------------------------------------------

### Q. What is OWASP ?

  -- OWASP stands for the "Open Web Application Security Project".

  -- It is a non-profit organization that "focuses on improving the security of web applications and softwares" by providing resources, tools, and best practices to identify, prioritize, and mitigate common web application vulnerabilities.
  -- OWASP is a globally recognized authority on web application security and is widely used by security professionals, developers, and organizations to improve the security of their web applications.
  -- One of the key initiatives of OWASP is the "OWASP Top Ten Project", which identifies the "top ten most critical web application security risks".
  -- This list is updated periodically and serves as a reference for developers and organizations to prioritize and address common web application vulnerabilities, such as cross-site scripting (XSS), SQL injection, and insecure authentication.

-------------------------------------------

## Q.  What is the difference between SQL and NoSQL databases ?

  (1) SQL databases:

    -- SQL Database are 'relational databases' that "store data in tables with predefined schema".
    -- They 'use SQL' structured query language (SQL) to manage and manipulate data.
    -- SQL databases are known for their ability to handle "complex queries", "transactions", and "relationships between tables".
    -- They are often used for applications that require a "high degree of data consistency", such as "financial systems or inventory management systems".
    -- SQL databases typically have 'vertical scalability', which means they 'can handle increasing amounts of data by adding more hardware resources' (e.g., RAM, CPU, disk space). 
    -- SQL databases are typically ACID (Availability, Consistency, Isolation and Durability)-compliant, which means they 'provide a high level of data consistency and reliability'

  (2) NoSQL databases:

    -- NoSQL databasesare 'non-relational databases' that do "not use a fixed schema to store data".
    -- They are 'designed to be highly scalable and flexible', making them useful for applications that require large amounts of unstructured or semi-structured data, such as "social media platforms or e-commerce sites."
    -- NoSQL databases typically have their own query language, which is optimized for the specific data model they use.
    -- NoSQL databases can handle different types of data models, such as 'key-value, document-based, or graph-based', and are often used for distributed systems or cloud-based applications.
    -- NoSQL databses are typically have 'horizontal scalability', which means they "can handle increasing amounts of data by adding more servers to a cluster".
    -- NoSQL databases are often designed to s"acrifice some level of consistency and reliability for the sake of scalability and performance".

  "The choice between SQL and NoSQL databases depends on the specific requirements of the application". SQL databases are best suited for applications that require data consistency and a fixed schema, while NoSQL databases are ideal for applications that require scalability, flexibility, and the ability to handle unstructured data. In some cases, a combination of both SQL and NoSQL databases may be used to take advantage of their respective strengths.

--------------------------------------------------------

## What are some common queries in SQL?

SQL is a powerful language for managing and manipulating relational databases. Here are some common queries in SQL:

=> SELECT: This is the most basic query in SQL, used to retrieve data from one or more tables. The syntax is:

  SELECT column1, column2, ... FROM table_name;


=> WHERE: This is used to filter data based on a condition. The syntax is:
  SELECT column1, column2, ... FROM table_name WHERE condition;

=> ORDER BY: This is used to sort the results of a query by one or more columns. The syntax is:
  SELECT column1, column2, ... FROM table_name ORDER BY column1 ASC/DESC, column2 ASC/DESC, ...;

=> GROUP BY: This is used to group the results of a query by one or more columns. The syntax is:
  SELECT column1, column2, ... FROM table_name GROUP BY column1, column2, ...;

=> JOIN: This is used to combine data from two or more tables based on a related column. There are several types of joins, including INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL OUTER JOIN. The syntax for an INNER JOIN is:
  SELECT column1, column2, ... FROM table1 INNER JOIN table2 ON table1.column = table2.column;

-------------------------------------------------------

# Q. How do you do joins in SQL?

-In SQL, joins are used to combine data from two or more tables based on a related column between them.
-- There are several types of joins that can be used, including INNER JOIN, LEFT JOIN (or LEFT OUTER JOIN), RIGHT JOIN (or RIGHT OUTER JOIN), and FULL OUTER JOIN.
-- Here's an overview of how you can perform joins in SQL:

(1) INNER JOIN: An INNER JOIN returns only the "rows that have matching values in both tables".

  -- The syntax for an INNER JOIN is as follows:

      SELECT column1, column2 FROM table1
      INNER JOIN table2 ON table1.column_name = table2.column_name;

    In this example, table1 and table2 are the names of the tables being joined, and column_name is the common column between them that is used for the join.

(2) LEFT JOIN (or LEFT OUTER JOIN): A LEFT JOIN "returns all the rows from the left table (table1), and the matching rows from the right table (table2)."

  -- If there are no matching rows in the right table, NULL values are returned. The syntax for a LEFT JOIN is as follows:

      SELECT column1, column2 FROM table1
      LEFT JOIN table2 ON table1.column_name = table2.column_name;

  -- In this example, all the rows from table1 are returned, and the matching rows from table2 are included. If there are no matching rows in table2, NULL values are returned for the columns from table2.

(3) RIGHT JOIN (or RIGHT OUTER JOIN): A RIGHT JOIN returns all the rows from the right table (table2), and the matching rows from the left table (table1).
  -- If there are no matching rows in the left table, NULL values are returned. The syntax for a RIGHT JOIN is as follows:

      SELECT column1, column2 FROM table1
      RIGHT JOIN table2 ON table1.column_name = table2.column_name;

    In this example, all the rows from table2 are returned, and the matching rows from table1 are included. If there are no matching rows in table1, NULL values are returned for the columns from table1.

(4) FULL OUTER JOIN: A FULL OUTER JOIN returns all the rows from both tables (table1 and table2), including the matching rows as well as the unmatched rows from both tables.

  -- If there are no matching rows in either table, NULL values are returned. The syntax for a FULL OUTER JOIN depends on the specific SQL database management system being used, as it may vary. Here's an example:

      SELECT column1, column2 FROM table1
      FULL OUTER JOIN table2 ON table1.column_name = table2.column_name;

  In this example, all the rows from both table1 and table2 are returned, including the matching rows as well as the unmatched rows from both tables. If there are no matching rows in either table, NULL values are returned for the columns from the respective table.

----------------------------------------------------------

### Q. How do you use lookup in mongodb? *** 

  -- In MongoDB, the $lookup operator is used to perform a left outer join between two collections.
  -- The $lookup operator takes two parameters: the name of the collection to join with, and an object that specifies the fields to use for the join.
  To combine elements from two different collections, use the $unionWith pipeline stage.
  
db.orders.aggregate([
  {
      $lookup:
        {
          from: "customers",
          localField: "customer_id",
          foreignField: "_id",
          as: "customer_info"
        }
  }
])

-------------------------------------------

### Q. What is CAP theorem? ***

-- CAP theorem is a fundamental concept in distributed database systems.
-- It is also known as "Brewer's theorem."
-- CAP stands for Consistency, Availability, and Partition Tolerance.

(1) Consistency: "Every read receives the most recent write or an error".
    --  when a write operation occurs, all subsequent read operations should return the most recent write value. Achieving consistency ensures that all clients see a single, up-to-date version of the data

(2) Availability: "Every request receives a response, without guarantee that it contains the most recent version" of the information.
    --  the system remains operational and responsive even in the presence of failures. High availability ensures that the system is accessible to users at all times.

(3) Partition tolerance: "The system continues to function even when network partitions occur".
    --  "the system remains operational and responsive even in the presence of failures".
    -- High availability ensures that the system is accessible to users at all times.
    -- Partitioning refers to the concept of "splitting a distributed system into multiple parts" that can operate independently.
    -- Partition tolerance ensures that the "system can handle network partitions without complete failure".

-- According to the CAP theorem, it is impossible to achieve all three guarantees simultaneously in a distributed database system.
-- In a Node.js backend application, the CAP theorem impacts the design and behavior of the database system.
-- According to the CAP theorem, a distributed system can provide at most two of these guarantees at the same time.
  -- This means that if a system provides consistency and availability, it will not be able to tolerate network partitions.
  -- If it provides availability and partition tolerance, it will sacrifice consistency, and
  -- If it provides consistency and partition tolerance, it will sacrifice availability.
-- WE may need to prioritize and make trade-offs between consistency, availability, and partition tolerance based on your application requirements.

*** In the context of Node.js, let's consider an example using a "distributed database system":

Suppose you have a distributed database system that replicates data across multiple nodes for high availability. Each node serves read and write requests from clients.

If you prioritize consistency and partition tolerance, you might implement a strong consistency model, where all write operations must be propagated to all nodes before a response is sent. This ensures that all nodes have the same data but may increase latency and reduce availability during network partitions.

If you prioritize availability and partition tolerance, you might implement eventual consistency, where write operations are asynchronously propagated to other nodes. This allows for faster response times and better availability but may result in temporary inconsistencies between nodes.

Node.js itself doesn't enforce a specific choice regarding consistency, availability, and partition tolerance. It provides the flexibility to implement various distributed systems based on your requirements.

-------------------------------------------------

### Q. What is indexing ?

-- Indexing in a database is the "process of creating a data structure that allows for faster retrieval of data from a table".
--  An "index is a data structure that is created on one or more columns of a table", which enables the database management system to quickly find and retrieve rows that match a specific search condition.
-- Indexes are special data structures that "store a small portion of the collection's data set in an easy to traverse form". The "index stores the value of a specific field or set of fields", ordered by the value of the field. The ordering of the index entries supports efficient equality matches and range-based query operations.
 In addition, MongoDB can return sorted results by using the ordering in the index.


--  Two main types of indexing methods are ;

   (1)Primary Indexing : 

   -- Primary Index is an ordered file which is fixed length size with two fields.
   -- The primary Indexing is also further divided into two types (1)Dense Index  & (2)Sparse Index.
   -- In a dense index, a record is created for every search key valued in the database.
   -- A sparse indexing method helps you to resolve the issues of dense Indexing.

   (2) Secondary Indexing.
   
   -- The secondary Index in DBMS is an indexing method whose search key specifies an order different from the sequential order of the file.

-- Clustering index is defined as an order data file.
-- Multilevel Indexing is created when a primary index does not fit in memory.
-- "The biggest benefit of Indexing is that it helps you to reduce the total number of I/O operations needed to retrieve that data".
-- The biggest drawback to performing the indexing database management system, "you need a primary key" on the table with a unique value.

-- "Indexes support the efficient execution of queries in MongoDB". Without indexes, MongoDB must perform a collection scan, i.e. scan every document in a collection, to select those documents that match the query statement. If an appropriate index exists for a query, "MongoDB can use the index to limit the number of documents it must inspect."

-- "MongoDB has default indexing".
-- In a MongoDB we can create index like db.collection.createIndex(key and index specification);

(A) Single indexing,
  eg.

      db.users.createIndex({ email: 1 });

      // db.users.find({ email: "example@example.com" });

      db.collection.createIndex({name:-1})
      // descending index

-- For a single-field index and sort operations, the sort order (i.e. ascending or descending) of the index key does not matter because MongoDB can traverse the index in either direction.


(A) Compund index,
    
      eg.

      db.users.createIndex({ firstName: 1, lastName: 1 });


** Explain Query Execution:

We can use the explain() method to get insights into how MongoDB executes a query, including the index usage.

    For example:

db.users.explain().find({ email: "example@example.com" });


This will return "detailed information about the query execution, including the chosen index and its efficiency".

-
-- You can view index names using the

      db.collection.getIndexes();

-- You "cannot rename an index" once created. Instead, you must drop and re-create the index with a new name.

      db.users.dropIndex({ email: 1 })
---------------------------------------------

### Q.  What is DB replication ?

-- "Database replication" is the process of "copying data from one database server to another", or to multiple servers, in order to maintain a" redundant and distributed database" system.
-- helps:- improved availability, scalability, and reliability, as well as "better disaster recovery" and backup options.
-- In a replicated changes made to the data on one server are automatically propagated to the other servers, to maintain consistency.
-- Replication can be performed at different levels, such as at the transaction level, statement level, or even the data block level, depending on the specific database management system being used.
-- different replication topologies that can be used such as , 
      -- master-slave replication, 
      -- master-master replication, 
      -- and multi-master replication.
-- Each topology has its own advantages and trade-offs, and the choice of topology depends on the specific requirements of the application and the database system.
--  different ways, such as using built-in replication features provided by the database management system, or using third-party tools and services.
-- Some popular database management systems that support replication include MySQL, PostgreSQL, Oracle, and SQL Server.

Overall, database replication is an important technique for building scalable and reliable database systems, and it is commonly used in modern web applications, e-commerce sites, and other large-scale systems that require high availability and performance.

-------------------------------------------------------------------

### What is PACELC ?

-- PACELC is a concept related to distributed systems that expands on the CAP theorem (Consistency, Availability, Partition tolerance) to include additional factors that affect the behavior of distributed systems.

-- PACELC stands for:

 (1) Partition tolerance (P):
    -- The system must continue to function despite network partitions, which can occur when a network failure causes a group of nodes to become separated from the rest of the network.

 (2) Availability (A):
    -- Every request received by a non-failing node must receive a response, although the response may not contain the most recent data.

 (3) Consistency (C): 
    -- All nodes in the system see the same data at the same time, although this may require slowing down or blocking requests in order to maintain consistency.

 (4) Else (E):
    -- In the presence of a network partition, the system can choose to either prioritize Availability or Consistency.

 (5) Latency (L):
    -- The time it takes for a request to be processed.

 (6) Consistency (C): 
    -- All nodes in the system see the same data at the same time, although this may require slowing down or blocking requests in order to maintain consistency.

-- The PACELC theorem suggests that, in the presence of a network partition, a distributed system must choose between availability and consistency.
-- In particular, the system can provide both consistency and partition tolerance (CP), availability and partition tolerance (AP), or consistency and availability (CA), but not all three simultaneously.
-- The PACELC theorem provides a useful framework for designing distributed systems that must operate in the presence of network partitions, helping architects to make trade-offs between consistency, availability, and partition tolerance based on the specific needs of their application.

-----------------------------------------------------

### What is Normalization / Denormalization ?

-- Normalization and denormalization are techniques "used in database design to optimize the structure of tables and improve the efficiency and performance of queries".

Normalization:

  -- It's the process of "organizing the data in a database into tables" that have "well-defined relationships", "reducing data redundancy" and "improving data integrity".
  -- The goal of normalization is to "minimize the amount of duplicate data in a database by breaking up larger tables into smaller ones with more specific purposes".
  -- The normalization process typically "involves creating multiple tables and establishing relationships between them using foreign keys".

Denormalization:

  -- It's the process of "intentionally adding redundancy" to a database table in order "to improve performance".
  -- This can involve "combining tables or duplicating data", making it easier to query data without joining multiple tables.
  -- By denormalizing a database, "queries can be executed faster", but at the cost of increased storage requirements and reduced data consistency.

-- Both normalization and denormalization have their uses in database design, and choosing which technique to use depends on the specific requirements of the application.
-- 'Normalization' is generally 'preferred' for databases that require a 'high level of data integrity',
-- while 'denormalization' is often 'used' in data 'warehousing or reporting applications' where fast query performance is more important than data consistency.

-------------------------------------------------------------------------------------------------------

## What is Eventual Consistency?

  => Eventual consistency is a "consistency model that enables the data store to be highly available". It is also known as optimistic replication & is key to distributed systems. 

  => Real World Use Case :

        -- Think of a popular microblogging site deployed across the world in different geographical regions like Asia, America, and Europe. Moreover, each geographical region has multiple data center zones: North, East, West, and South. 
        -- Furthermore, each zone has multiple clusters which have multiple server nodes running. So, we have many datastore nodes spread across the world that micro-blogging site uses for persisting data. Since there are so many nodes running, there is no single point of failure.
        -- The data store service is highly available. Even if a few nodes go down persistence service is still up. Lets say a celebrity makes a post on the website that everybody starts liking around the world. 
        -- At a point in time, a user in Japan likes a post which increases the “Like” count of the post from say 100 to 101. At the same point in time, a user in America, in a different geographical zone, clicks on the post, and he sees “Like” count as 100, not 101.

        # Reason for the above Use case :

        -- Simply, because the new updated value of the Post “Like” counter needs some time to move from Japan to America and update server nodes running there. Though the value of the counter at that point in time was 101, the user in America sees old inconsistent values. 
        -- But when he refreshes his web page after a few seconds “Like” counter value shows as 101. So, data was initially inconsistent but eventually got consistent across server nodes deployed around the world. This is what eventual consistency is.


-----------------------------------------------------------

## What is Strong Consistency?

=> Strong Consistency simply means the data must be strongly consistent at all times. All the server nodes across the world should contain the same value as an entity at any point in time. And the only way to implement this behavior is by locking down the nodes when being updated.

=> Real World Use Case : 

    -- Let's continue the same Eventual Consistency example from the previous lesson. To ensure Strong Consistency in the system, when a user in Japan likes posts, all nodes across different geographical zones must be locked down to prevent any concurrent updates. 
    -- This means at one point in time, only one user can update the post “Like” counter value. So, once a user in Japan updates the “Like” counter from 100 to 101. The value gets replicated globally across all nodes. Once all nodes reach consensus, locks get lifted. Now, other users can Like posts. 
    -- If the nodes take a while to reach a consensus, they must wait until then. Well, this is surely not desired in the case of social applications. But think of a stock market application where the users are seeing different prices of the same stock at one point in time and updating it concurrently. This would create chaos. Therefore, to avoid this confusion we need our systems to be Strongly Consistent. 
    -- The nodes must be locked down for updates. Queuing all requests is one good way of making a system Strongly Consistent. The strong Consistency model hits the capability of the system to be Highly Available & perform concurrent updates. This is how strongly consistent ACID transactions are implemented.

------------------------------------------------------------

##  What is Entity Relationship Model? ( ER Model ) : 

-- The Entity Relationship Model (ER Model) is a conceptual data model used in database design.

-- It describes the relationships between data entities, attributes, and the constraints that govern the relationships.

-- The ER Model provides a graphical representation of the database structure and helps to identify relationships between data entities.

-- The ER Model consists of three main components:

  (1) Entities: An entity is a person, place, or thing that is relevant to the database. For example, in a database for a hospital, entities might include patients, doctors, and medical procedures.

  (2) Attributes: An attribute is a characteristic or property of an entity. For example, the attributes of a patient might include their name, date of birth, and medical history.

  (3) Relationships: Relationships are connections between entities that describe how they are related to each other. For example, a patient might be related to a doctor through a "treated by" relationship, or a medical procedure might be related to a patient through a "performed on" relationship.

-- The ER Model is typically represented graphically using ER diagrams, which show entities as boxes, attributes as ovals, and relationships as lines connecting the boxes.
-- The ER Model is an important tool for designing and understanding complex databases, and is widely used in database design and development.

----------------------------------------------------------

## What are Models? 

=> Model: Manages data and business logic.The model defines what data the app should contain. If the state of this data changes, then the model will usually notify the view (so the display can change as needed) and sometimes the controller (if different logic is needed to control the updated view).
    -- In Node.js 'models' typically refer to data that define the structure and behavior of data in application.
    -- A models is a representation of particular data entity, such as a user, a post, or a product.
    -- It defines the fields or properties that the entity contains, as well as any validation rules or methods associated with it.
    -- Node.js offers various libraries and frameworks for creating data models, such as Mongoose, Sequelize,and TypeORM.
    -- These libraries allow developers to define models using JS code, and provide features such as schema validation, data queying, and database migration.
    -- Using the models in Node.js can help make it easier to manage and manipulate data in an application, as well as provide a consistent interface for accesing and interacting with data throughout the application.

--------------------------------------------------------------------------

##  Explain why mongoose does not return a promise but has a .then 

  => The Mongoose does not return a promise direcly because it was "designed to support both the call-back base and promise bases API's". Insted of returning a promise, Mongoose provide a ".then()" methode on queries that allows us to chain promise based operations.
    -> Mongoose queries are executed asynchronously and return a query object that implements the Promise API. The `.then()` method on Query object allows you to chain additional asynchronous operations that will be executed when the query is complete. This is similar to how promise work, where you can chain additional operations using `.then()`.
    -> Mongoose also suppports callback-bases API's, which means we can pass callback function to a query insted using a promise.
    ->".then()" is provided to developers convinience who prefer to use promises, but Mongoose queries can be used with callback as well.

    *Example-1: Find operation with .then():
    const User = require('./models/user');
    User.find({ age: { $gte: 18 } }).then((users) => {
    console.log(users);
    }).catch((err) => {
    console.error(err);
    });
    ->In this example, we're using Mongoose's find() method to retrieve all users who are 18 years or older. The find() method returns a Query object that implements the Promise API, which allows us to use the .then() method to handle the result of the query.

    *Example-2: Save operation with .then():

    const User = require('./models/user');
    const newUser = new User({
    name: 'John Doe',
    email: 'john.doe@example.com',
    age: 25
   });
   newUser.save().then((savedUser) => {
   console.log(savedUser);
   }).catch((err) => {
   console.error(err);
   });
   ->In this example, 'we are using Mongoose's save() method to insert a new user into the database. The save() method returns a promise that resolves with the saved document, which allows us to use the .then() method to handle the result.

   *Example-3- Update operation with .then():
       const User = require('./models/user');
       User.updateOne({ name: 'John Doe' }, { age: 30 }).then((result) => {
        console.log(result);
        }).catch((err) => {
        console.error(err);
        });
        ->In this example, we're using Mongoose's updateOne() method to update the age of a user with the name "John Doe". The updateOne() method returns a Query object that implements the Promise API, which allows us to use the .then() method to handle the result of the query.

   ------------------------------------------------------------------------------

   ## What are aggregation pipelines with mongoose? **

  => An aggregation pipeline consists of one or more stages that process documents:

  -> Each stage performs an operation on the input documents.  For example, a stage can filter documents, group documents, and calculate values.

  -> The documents that are output from a stage are passed to the next stage.

  -> An aggregation pipeline can return results for groups of documents. For example, return the total, average, maximum, and minimum values.

  -> we can update documents with an aggregation pipeline if you use the stages shown in Updates with Aggregation Pipeline.

  => Aggregation pipelines in Mongoose are a way to perform advanced data processing and transformation on collections of documents in MongoDB.
  => An aggregation pipeline is a series of stages that documents pass through, each stage applying a transformation or computation to the data.
  => Mongoose provides a powerful API for building aggregation pipelines using its aggregate() method.
    -- The aggregate() method takes an array of pipeline stages as its argument and returns a Aggregate object that can be used to execute the pipeline.

   =>Example:
    db.orders.insertMany( [
   { _id: 0, name: "Pepperoni", size: "small", price: 19,
     quantity: 10, date: ISODate( "2021-03-13T08:14:30Z" ) },
   { _id: 1, name: "Pepperoni", size: "medium", price: 20,
     quantity: 20, date : ISODate( "2021-03-13T09:13:24Z" ) },
   { _id: 2, name: "Pepperoni", size: "large", price: 21,
     quantity: 30, date : ISODate( "2021-03-17T09:22:12Z" ) },
    ]);

    -> Calculate the total order quantity:->

       db.orders.aggregate( [
       // Stage 1: Filter pizza order documents by pizza size
      {
      $match: { size: "medium" }
      },
      // Stage 2: Group remaining documents by pizza name and calculate total quantity
      {
      $group: { _id: "$name", totalQuantity: { $sum: "$quantity" } }
      }
      ] )

      => The result of the aggregation pipeline will be an array of objects that contains the computed filterd by size and total quantity.
      => Mongoose's aggregation pipelines provide a powerful way to perform complex data processing and transformation in MongoDB. The API is flexible and expressive, allowing you to compose pipelines with multiple stages and operators to compute exactly the data you need.

------------------------------------------------------------------------

## I'm using an arrow function for a virtual, middleware, getter/setter, or method and the value of this is wrong. Why? 

=> Arrow functions in JS are lexically scopped, which means they "don't have their own 'this' scope" like traditional function expressions.
-- Insted, 'this' in  a arrow function is determined by the surrounding lexical scope, which can lead to unexpected behavior when used in certained contexts.
=> In the case of virtual, middleware, getter/setters and methods in Mongoose, the value of `this` is important because it refers to the document being operated on. If we use an arrow function for one of these purposes, 'this' will refer to surrounding lexical scope , which is undefined in document.
  =>Example:
        const mongoose = require('mongoose');
        const UserSchema = new mongoose.Schema({
        firstName: String,
        lastName: String
        });
        UserSchema.virtual('fullName').get(() => {
        return `${this.firstName} ${this.lastName}`;
        });

        //correct approch will be,

        UserSchema.virtual('fullName').get(function() {
        return `${this.firstName} ${this.lastName}`;
        });

        ->In this example, the arrow function used for the virtual's getter is lexically scoped, meaning this will not refer to the document being operated on. Instead, this will refer to the surrounding lexical scope, which is not the document, and this.firstName and this.lastName will be undefined.

---------------------------------------------------------------------------------

## Should I create/destroy a new connection for each database operation ?
    
    => NO. Open the connection when our application starts ups and leave open untill the application shuts down.

------------------------------------------------------------------------------------

## My query/update seems to execute twice. Why is this happening?

=> This can be hapen by not handling the asynchronous function properly, because Mongoose queries and upadates are asynchronous i.e. they dont block the event loop. Thats why the query/update is being executed twice.
=> It can also happen due to to midleware function executed before/ after the function.If a middleware function that is executing the query/update, it can cause the operation to be executed twice.

-----------------------------------------------------------------------------------

## How we can create indexes with mongoose **

  =>In Mongoose we can create indexes on collection using the "index" method in schema defination

   *Example:-
        const mongoose = require('mongoose');
        const mySchema = mongoose.Schema({
                name: String,
                age: Number,
                email: String
            });

        // create an index on the 'name' field

        mySchema.index({ name: 1 });

        const MyModel = mongoose.model('MyModel', mySchema);

      -- The value of 1 indicates that the index should be created in ascending order. You can also use -1 to indicate descending order.
      -- Once we defined the indexes, Mongoose create the index automatically when the model is first used.

       * we can create compund indexes by passing an objects with multiple fields to the 'indexs' method'
      
      -- eg. mySchema.index({name:1,email:1})

      => Types of indexs,

      (1) Regular index:

          var User = new Schema({
          name: { type: String, index: true }
          });

      (2) Sparse indexes:

            var User = new Schema({
            name: { type: String, sparse: true }
            });

      (3)Unique indexes:

          var User = new Schema({
          name: { type: String, unique: true }
          })
            // or

          var User = new Schema({
          name: { type: String, index: { unique: true } }
          });

-------------------------------------------------------------------------------

## What are pre and post hooks? 

-> Pre and post also known as" middlewares" are the functions which are passes control during execution of asynchronous functions. 
->" middleware is specified on the schema level" and is usefull for writin plugins.
-> Pre and post are functions that are executed before or after a certain action that we specify.
  -eg. We want run a function everytime before we save a document in the DB, we use pre-hook and if we want something to be executed after that we use post hooks.
-> These hooks allow us to add custom logic to a Mongoose schema, such as validating data before it saved to database or updating related document when a document is deleted.

->Example:

    (1) Pre-Hooks:

          const userSchema = new mongoose.Schema({
              name: String,
              email: String,
              password: String
          });
        
        userSchema.pre('save', function(next) {

        // Hash the user's password before saving it to the database

          bcrypt.hash(this.password, 10, (err, hash) => {
          if (err) return next(err);
          this.password = hash;
          next();
          });
        });

      (2)Post-Hooks:

          const postSchema = new mongoose.Schema({
          title: String,
          content: String
          });

          postSchema.post('remove', function(doc, next) {

          // Remove all comments associated with the deleted post

          Comment.deleteMany({ post: doc._id }, (err) => {
          if (err) return next(err);
          next();
          });
          });

----------------------------------------------------------------------------

##  What is Authentication? 

    -> Authentication is a term that refers to the process of proving that some fact or some document is genuine.
    -> In computer science, this term is typically associated with proving a user's identity. Usually, a user proves their identity by providing their credentials, that is, an agreed piece of information shared between the user and the system.
    -> Username and password combination is the most popular authentication mechanism, and it is also known as password authentication.
    -> A specific category of credentials, like username and password, are usually said an authentication factor. Even if password authentication is the most well-known type of authentication, other authentication factors exist.
    There are three types of authentication factors typically classified as follows:

      (1) password->something you know
      (2) smartphone -> something you have
      (3) biometric authentication -> something you are

    -> The process of authentication based on just one factor is called Single-factor authentication.
    -> You can combine multiple authentication factors, further increasing your identity security. In this case, you are using a Multiple-factor authentication (MFA). Of course, 2FA is just a form of MFA.

-----------------------------------------------------------------------------

## What is Authorization? 

  =>Authorization is the process of granting or denying access to specific resources or actions to specific users or groups of users.
  -> It is a critical component of security in any application or system that requires access control.
  -> It is most usefull at "Roll Based Access Control". 
  -> Authentication generally involves granting or denying access to specific route or pages based on user's role or permissions.
      eg. an e-commerce application allow only authenticated person to checkout page or an HR management system might allow HR administrator to view or modify employees records.
  -> Authorization is ussually implemented by checking the user's credentials against a set of predefined rules or policies that specify what resources or actions they are alloiwed to access.

  => In many cases, authorization is implemented in conjunction with authentication, which is the process of verifying a user's identity. Together, these two processes provide a secure and reliable mechanism for controlling access to sensitive resources and data in a system.

      eg.

       // we are using closures here.

          const authorise=(roles_array)=>{

            return (req,res,next)=>{
              const userrole=req.body.userrole;

              if(roles_array.includes(userrole)){

                next();

              }else{

                res.send({"Message":"You are not authorised to this action"});

              }
            }
          }

          module.exports={authorise};

-------------------------------------------------------------------------------

## How do you do role-based authentication? 

=> Role-based access control (RBAC) refers to the idea of assigning permissions to users based on their role within an organization. It offers a simple, manageable approach to access management that is less prone to error than assigning permissions to users individually.
=> When using RBAC for Role Management, we analyze the needs of our users and group them into roles based on common responsibilities. 
=> we can assign one or more roles to each user and one or more permissions to each role.
=>The user-role and role-permissions relationships make it simple to perform user assignments since users no longer need to be managed individually, but instead have privileges that conform to the permissions assigned to their role(s).

=> Benefits of Rol Bases Access Controll(RBAC):

    (1) Create systematic, repeatable assignment of permissions.
    (2) Easiy audit user privileges an correct indentified issues.
    (3) Quickly add and changes roles, as well as implement them across API's.
    (4) Cut down on the potential for error when assigning user permissions.
    (5) More effectively comply with regularity and statutory requirements for confidentially and privacy.

------------------------------------------------------------------------------

## What is hashing? 

=> Hashing is a technique used in computer science to transform data of any size or format into a "fixed size value or key", which represents the original data in a compact and standardized format.
=> A "hash function is a mathematical algorithm" that takes in data as input and produces a fixed-length output, which is refered to as a hash code, hash value, or simply hash.
=> Its widely used fields like cryptography, data retrieval, data compression, and digital signatures.
=> It is commonly used to verify the integrity of data and to store passwords security by hashing them before storing them in a database,
=> The hash function is designed in such a way that "even a small change in the input data will result in a completely different hash value". This property is referred to as the "avalanche effect" and is important for ensuring the security and integrity of the data.

=> Examples:

    (1) Password Storage: 

        When a user creates an account on a  website, their password is often hashed using a hash function before being stored in a database. When the user logs in, the password they enter is hashed and compared to the stored hash value to verify their identity. If the hash values match, the user is granted access.

        const bcrypt=require('bcrypt');
        async function createHash(name,email,password){

          bcrypt.hash(password,5,(err,hash)=>{
            const user=new User({name,email,password:hash});
            await user.save();

          })
          
        }
        createHash('Dilip','dilip@gmail.com','dilip123')

    (2) Data Retrieval:

        Hashing can also be used for "efficient data retrieval in large databases". A hash table is a data structure that uses a hash function to map keys to indices in an array. This allows for quick access to data based on its key value.

    (3) Digital Signatures:

        Hashing is used in digital signatures to ensure that a message has not been tampered with during transmission. A hash of the message is created, and then the hash is encrypted using the sender's private key. The receiver can then decrypt the hash using the sender's public key and verify that the message has not been altered.

    (4) File Integrity Checking:

        Hashing can also be used to verify the integrity of files. A hash of the file is created and stored. Later, when the file is accessed again, a new hash is created and compared to the stored hash. If the two hash values match, the file has not been altered. This technique is commonly used in software distribution to ensure that downloaded files have not been modified.

--------------------------------------------------------------------------------

## What is encryption? 

    => Encryption is the process of converting plain text or data into a coded format that is unintelligible without a key or a password, making it unreadable to unauthorized users.
    => It is used to protect the confidentiality, integrity, and authenticity of sensitive data during transmission or storage.
    => Encryption uses mathematical algorithms to scramble the original data into ciphertext that can only be decrypted using a specific key or password.

    => There are two main types of encryption:

      (1)Symmetric Encryption:

          -> In symmetric encryption, the "same key is used for both encryption and decryption".
          -> The sender and the receiver must both possess the same secret key to be able to decrypt the ciphertext. 
          ->Examples of symmetric encryption algorithms include AES (Advanced Encryption Standard), DES (Data Encryption Standard), and 3DES (Triple Data Encryption Standard).

      (2)Asymmetric Encryption:

          -> In asymmetric encryption, also known as public-key encryption, two different keys are used for encryption and decryption.
          -> The sender encrypts the message using the receiver's public key, and the receiver decrypts the message using their private key. This allows for secure communication without sharing a secret key. 
          ->Examples of asymmetric encryption algorithms include RSA (Rivest–Shamir–Adleman) and ECC (Elliptic Curve Cryptography).
    
    => Encryption is widely used to secure sensitive data such as credit card information, personal identification numbers, medical records, and government secrets. It is also used to protect communication channels such as email, messaging, and virtual private networks (VPNs).

    ----------------------------------------------------
    
    ## What is Advanced Encryption Standard(AES):

    => AES encryption, or advanced encryption standard, is a type of cipher that protects the transfer of data online.

    => Currently, AES is one of the best encryption protocols available, as it flawlessly combines speed and security, letting us enjoy our daily online activities without any disruption.

    =>  "AES is a symmetric type of encryption", as it uses the same key to both encrypt and decrypt data.

    => It also uses the SPN (substitution permutation network) algorithm, applying multiple rounds to encrypt data. These encryption rounds are the reason behind the impenetrability of AES, as there are far too many rounds to break through.

    => here are three lengths of AES encryption keys. Each key length has a different number of possible key combinations:

        -- 128-bit key length: 3.4 x 1038
        -- 192-bit key length: 6.2 x 1057
        -- 256-bit key length: 1.1 x 1077 

    => AES encryption is very appealing to those who work with it, Because the encryption process of AES is relatively easy to understand. This allows for easy implementation, as well as really fast encryption and decryption times.

    => AES requires less memory than many other types of encryption (like DES), which makes it a true winner when it comes to choosing your preferred encryption method.

    => Examples of AES usage, 

      Here are a few notable examples of where developers can use the AES encryption:

      -- VPNs (Virtual Private Networks): As the job of a VPN is to securely connect you to another server online, only the best methods of encryption can be considered so that your data wouldn't leak. The VPNs that use the advanced encryption standard with 256-bit keys include NordVPN, Surfshark, and ExpressVPN.

      -- Wi-Fi:  That's right - wireless networks also use AES encryption (usually, together with WPA2). This is not the only type of encryption Wi-Fi networks can use, however, most of the other encryption methods are far less safe.

      -- Mobile applications: Many popular apps (like Snapchat and Facebook Messenger) use AES encryption in order to safely send info like photos and messages.

      -- Archive and compression tools: All major file compression programs use AES to prevent data from leaking. These tools include 7z, WinZip, and RAR.

      -- OS system components: Some operating system components (like file systems) use the advanced encryption standard for an extra layer of safety.

      -- Programming language libraries: The libraries of such coding languages like Java, Python, and C++ implement AES encryption.

      -- Password managers. These are the programs that carry a lot of sensitive information. That’s why password managers like LastPass and Dashlane don't skip the important step of AES implementation.


    ------------------------------------------

    ## How is hashing and encryption different? **
    
    (1) Encryption:
        -> Encryption is the process of converting a normal readable message known as 'plaintext' into a garbage message or not readable message known as 'ciphertext'. 
        -> The ciphertext obtained from the encryption can easily be transformed into plaintext using the encryption key.
        -> The purpose of encryption is to transform data to keep it secret from others.
        -> The original information can be easily retrieved if we know the encryption key and algorithm used for encryption. 
        -> It is "less secure in comparison to hashing".
        -> It will always generate a new key for each information.
        -> The "encrypted information is not of fixed length". It "grows" with the increase in length of information.
        -> Some of the examples of encryption algorithms are RSA, AES, and Blowfish.

    (2) Hashing:
        -> Hashing is the process of converting the information into a key using a hash function.
        -> The purpose of hashing is indexing and retrieving items from the database. The "process is very fast".
        -> The "hash code or key can not be reversed to the original information by any means". It can only be mapped and the hash code is checked if the hash code is the same the information is the same otherwise not. "The original information can not be retrieved."
        -> It is "more secure" in comparison to encryption.
        -> The original information cannot be retrieved from the hash key by any means.
        -> Generally, the hash keys are stored in the database and they are compared to check whether the original information matches or not.
        -> Generally, it tries to generate a new key for each information passed to the hash function but on rare occasions, it might generate the same key popularly known as a 'collision'.
        -> They are generally used to store the passwords for login.
        -> The "hashed information is generally of small and fixed length". It does "not grow" with the increase in the information length of information.
        -> Some of the examples of a hashing algorithm are MD5, SHA256.

---------------------------------------------------------------------------------

## what is salt in encryption? **

  => In the context of encryption, a "salt is a random or pseudo-random string that is added to the plaintext before it is hashed or encrypted".
  -> The purpose of the salt is to make the resulting hash or ciphertext more secure by introducing additional randomness to the process.
  -> Salt is commonly used in password hashing to make it more difficult for attackers to guess or crack passwords.
  -> When a user creates a password, a salt is generated and appended to the password before it is hashed.
  -> This ensures that even if two users have the same password, their hashed passwords will be different because they have different salts.
  -> Without salt, attackers can use "pre-computed hash tables, known as rainbow tables", to quickly match the hashed password with a corresponding plain text password.
  -> By adding salt, the pre-computed hash tables are no longer effective because each salt produces a unique hash value, which makes it much harder for attackers to crack the passwords.

--------------------------------------------------------------------------------

## What is JWT? 

    => JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.
    -> This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.
    => JSON Web Tokens can be used in,

    -> Authorization: 
        -- This is the most common scenario for using JWT. Once the user is logged in, each subsequent request will include the JWT, allowing the user to access routes, services, and resources that are permitted with that token.
        -- Single Sign On is a feature that widely uses JWT nowadays, because of its small overhead and its ability to be easily used across different domains.

    -> Information Exchange:
        -- JSON Web Tokens are a good way of securely transmitting information between parties. Because JWTs can be signed—for example, using public/private key pairs—you can be sure the senders are who they say they are. 
        -- Additionally, as the signature is calculated using the header and the payload, you can also verify that the content hasn't been tampered with.

    => JSON Web Tokens consist of three parts separated by dots (.), which are:

        (1) Header:
            -> The header typically consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as HMAC, SHA256 or RSA.

        (2) Payload:
            -> It contains the claims. Claims are statements about an entity (typically, the user) and additional data. There are three types of claims: registered, public, and private claims.

        (3) Signature:
            -> To create the signature part you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that.
    
    => In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned. Since tokens are credentials, great care must be taken to prevent security issues. In general, you should not keep tokens longer than required.

    Authorization: Bearer <token>

----------------------------------------------------------------------------

## How is JWT different and list the pros and cons of using JWT tokens? **

    =>JWT : It is a definitely a "clever way to securely get the identity of the client". In simple language there is a secret Key used to encrypt the JSON formatted Data which primarily includes the user-id. Now an encryption of data with the Key generates the token that is sent to the client and used in every request. Every time, the client sends in the request with the token the server tries to decrypt it with the Key, if it can, it gets the user-id from the JSON Data which corresponds to the user.
    =>The actual working of the JWT is not so straight forward : it actually involves Public/Secret Key, where the Payload is signed by the secret.

    * Here are some ways that JWT is different from other types of tokens:

        (1) Self-contained: 
          => JWT tokens are self-contained, meaning that all the necessary information is included within the token itself. This makes it easy to transmit the token between different systems without needing to reference an external database or lookup system.

        (2) Stateless:
          -> Because all the necessary information is included within the token, JWT tokens are stateless. This means that the server does not need to store any information about the token on the server side, making it easier to scale horizontally.

        (3) Signed and encrypted:
          -> JWT tokens can be signed and encrypted, which provides an additional layer of security. This helps to prevent tampering or unauthorized access to the token.

    *=> Pros:
        (1) No Database Table :
        -> This implies fewer DB queries, which implies faster response time. In case you are using paid services like DynamoDb that charge per query basis, JWT might reduce the costs marginally.

        (2) Simpler to use if careful : 
          ->If your architecture doesn't user client Sessions and your security basics are clear, the development time in case of JWT is faster using the existing libraries.

        (3) Used across services : 
          -> You can have one authorization server that deals with the Login/Registration and generates the token, all the subsequent requests will need not have to go to the authorization server as the only the Auth-server will have have the private key, and rest of the severs will have the public-key to verify the signature.

        (4) Scalability: 
          -> Because JWT tokens are stateless and self-contained, they can be easily distributed across multiple servers, making it easier to scale horizontally.

        (5) Flexibility: 
          ->JWT tokens can be used for a variety of purposes, such as authentication, authorization, and data exchange.
    
    *=> Cons:

        (1) Compromised Secret Key : 
          -> The best and the worst thing about JWT is that it relies on just one Key. Consider that the Key is leaked by a careless or a rogue developer/administrator, the whole system is compromised!
          -> The attacker(who has access to the Key) can easily access all user data if he has the user-id which can be easily acquired.

        (2) Cannot manage client from the server: 

          -> We had several cases where we wanted the users at HelpTap to logout by cleaning up the cookies, but we cannot ask them to do so every time.
          -> As well consider the case that a user’s mobile is stolen, and he wants to logout of all existing sessions(e.g. Gmail's logout other sessions feature). Well its not possible in case of JWT.

        (3) Cannot push Messages to clients (Identifying clients from server) :
          -> As we have no record about the logged-in clients on the DB end, we cannot push messages to all the clients.

        (4) Crypto-algo can be deprecated: 
          -> JWT relies completely on the Signing algorithm. Now, though it is not frequent, but in the past many Encryption/Signing algorithms have been deprecated.

        (5) Data Overhead : 
          -> The size of the JWT token will be more than that of a normal Session token. The more data you add in the JWT token, the longer it gets linearly. Remember, each request needs the token in it for request verification. So say, a 1 KB JWT token implies each request will have 1KB over-head upload which is really bad in cases of low speed net connectivity.

        (6) Complicated to understand: 
          ->JWT uses cryptographic Signature algorithms to verify the data and get the user-id from the token. Understanding the Signing Algo in itself requires basics of cryptography. So, in case if the developer is not completely educated s/he might introduce security loopholes in the system.

        (7) Revocation: 
          ->Because JWT tokens are stateless, there is no built-in mechanism for revoking a token. This can be a challenge if you need to revoke a token for security reasons, such as if a user's account is compromised.

-------------------------------------------------------------------------------------

## What are the different ways to manage authentication? 

  => Authentication methods are the "first line of defence".
  => Authentication is purely the means of confirming digital identification, so users have the level of permissions to access or perform a task they are attempting.

  =>The ways of Authentication:

  (1) Session-based-authentication:
    -- In session based authetication , server creates a session for the user after they log in , and sends a session ID back to the client. The Client then includes this session ID in subsequent requests, allowing the server to identify the user and verify their identity. Sessions are typically stored on the server side and can be used to store user-specific data.

  (2) Token-based authentication:
    -- In token-based authentication, the server generates a token (such as a JWT) after the user logs in, which is then sent back to the client. The client includes this token in subsequent requests, allowing the server to identify the user and verify their identity. Tokens can be stored client-side (in cookies or local storage) or server-side (in a database or cache).

  (3) OAuth2: 
    --> OAuth2 is an open standard for authentication and authorization, used by many large web applications such as Google, Facebook, and Twitter.
    --> OAuth2 allows users to grant third-party applications access to their resources without sharing their login credentials. The third-party application receives an access token, which it can use to access the user's resources.

  (4) OpenID Connect:
    --> OpenID Connect is a standard built on top of OAuth2 that provides additional features for authentication, such as user profile information and single sign-on (SSO) capabilities. OpenID Connect is widely used by enterprise applications for SSO.

  => Authentication methods:

  (1) Single-Factor / Primary Authentication:

      -> Historically the most common form of authentication, Single-Factor Authentication, is also the least secure, as it only requires one factor to gain full system access.
      -> It could be a username and password, pin-number or another simple code.
      -> While user-friendly, Single-Factor authenticated systems are relatively easy to infiltrate by phishing, key logging, or mere guessing. As there is no other authentication gate to get through, this approach is highly vulnerable to attack.

   (2) Two-Factor Authentication (2FA):

      ->By adding a second factor for verification, two-factor authentication reinforces security efforts.
      -> It is an added layer that essentially double-checks that a user is, in reality, the user they're attempting to log in as—making it much harder to break.
      -> With this method, users enter their primary authentication credentials (like the username/password mentioned above) and then must input a secondary piece of identifying information.
      -> The secondary factor is usually more difficult, as it often requires something the valid user would have access to, unrelated to the given system.
      -> Possible secondary factors are a one-time password from an authenticator app, a phone number, or device that can receive a push notification or SMS code, or a biometric like fingerprint (Touch ID) or facial (Face ID) or voice recognition.
      ->2FA significantly minimizes the risk of system or resource compromise, as it’s unlikely an invalid user would know or have access to both authentication factors. ->While two-factor authentication is now more widely adopted for this reason, it does cause some user inconvenience, which is still something to consider in implementation.

  (3) Single Sign-On (SSO):

      ->With SSO, users only have to log in to one application and, in doing so, gain access to many other applications. This method is more convenient for users, as it removes the obligation to retain multiple sets of credentials and creates a more seamless experience during operative sessions.
      -> Organizations can accomplish this by identifying a central domain (most ideally, an IAM system) and then creating secure SSO links between resources. This process allows domain-monitored user authentication and, with single sign-off, can ensure that when valid users end their session, they successfully log out of all linked resources and applications. 
    
    (4) Multi-Factor Authentication (MFA):
      -> Multi-factor authentication is a high-assurance method, as it uses more system-irrelevant factors to legitimize users.
      -> Like 2FA, MFA uses factors like biometrics, device-based confirmation, additional passwords, and even location or behavior-based information (e.g., keystroke pattern or typing speed) to confirm user identity. 
      -> However, the difference is that while 2FA always utilizes only two factors, MFA could use two or three, with the ability to vary between sessions, adding an elusive element for invalid users.

---------------------------------------------------------------------------

## What is cookie-based auth? 
    
    => Cookies are pieces of data used to identify the user and their preferences. The browser returns the cookie to the server every time the page is requested. Specific cookies like HTTP cookies are used to perform cookie-based authentication to maintain the session for each user.
    => When a user logs in to a web application, the "server creates a unique session ID and stores it in a cookie", which is then sent to the user's browser. The browser will then automatically send this cookie back to the server with each subsequent request, allowing the server to identify the user and maintain their session.

    => The coockie bases authentication work like,

    (1) The user gives a username and password at time of login. Once the user fills the login form, the browser(client) sends a login request to the server.
    (2) The server verifies the data by querying the user data. If authentication request valid, the server generate the following:
      (a) A 'session' by utilizing the user information.
      (b) A unique ID, known as 'session ID'.
      The server then passes the session ID to the browser that keeps it. The server also keeps track of the active sessions.
    (3) The browser has to submit this generated session ID while sending a subsequent request. Every time the server validates the session ID. The session ID helps the authentication process identify the user and provides access accordingly.
    (4) When the user logs out of the application, the session gets destroyed from both client (browser) and the server. It discontinues the authentication process from happening again through the respective session ID.

    => Benefits of Cookie-based Authentication
       (1) Availability:
          In cookies-based authentication, cookies can be made available for an extended period, maintaining a session for a long time.
       (2) Easy Configuration:
          Websites can deliver cookies by configuring them as per requirement. For example, a website can send cookies that will expire as the users close the browser tab. It is also possible to configure cookies for a specified length of time on the client-side.
       (3) User-friendly:
          Cookie-based authentications are simple, and the cookies used in this method are user-friendly. Users can choose what to do with cookie files that have kept user credentials. All modern browsers come with settings to clear the cookies. Users can find cookies in the hard drive and delete them manually.

    =>Challenges of Cookie-based Authentication

        (1) Vulnerable to CSRF:
          Cookie-based authentications are prone to Cross-site Request Forgery (CSRF) attacks. Hence, they often require additional security postures for protection.
        (2) Less Mobile-friendly:
          Cookie-based authentication does not work well with all native applications.
        (3) Limitations: There are certain limitations and concerns such as size limit (not more than 4KB of information per cookie), browser limitations on cookies, user privacy, etc., come with cookies and cookie-based authentication.
        (4) Less Scalable: Cookie-based authentication is less scalable, and the overhead rises when the user count increases on a particular site.

---------------------------------------------------------------------------------------

## What is session management ?

  => Session management manages sessions between the web application and the users. The communication between a web browser and a website is usually done over HTTP or HTTPS. When a user visits a website, a session is made containing multiple requests and responses over HTTP.
  => A session is a period of time in which a user interacts with an application, from the moment they log in to the moment they log out or their session expires. During this time, the application needs to keep track of the user's identity and state in order to provide a personalized experience and maintain security

  => The Steps involved in session management:

    (1)  Session Creation:
        When a user logs in to the application, the server creates a new session for that user. This typically involves generationg a unique session ID, which is associated with the users account and stored on server.

    (2)  Session tracking:
        As the user interacts with the application, the session ID is used to track their identity and state. This includes the data like preferences or shoping cart contents on server and associating with users session id.

    (3) Session Expiration:
        To prevent inactive sessions from being used by unauthorised users, sessions typically have an expiration time. This means that after a certain period of inactivity, the user will be required to log in again to create a new session.

    (4)  Session termination: 
        When a user logs out of the application, their session should be terminated to prevent any further access to their account.

    => Proper session management is critical for ensuring the security and privacy of user data in a web application. 
    => Without proper session management, an attacker may be able to hijack a user's session and gain access to their account or sensitive data. Common attacks on session management include session fixation, session hijacking, and session replay attacks
    => To prevent these attacks, web applications should use secure session ID generation, implement appropriate session expiration policies, and ensure that session data is properly secured and encrypted.

---------------------------------------------------------------------------------

## What is OAuth? 

    => OAuth (short for "Open Authorization") is an open standard for access delegation, commonly used as a way for internet users to grant websites or applications access to their information on other websites but without giving them the passwords
    => OAuth works by allowing a user to grant a third-party application access to their resources (such as profile information, contacts, or photos) by authenticating with the service that owns the resources.
    =>  The user is redirected to the service's authentication page, where they can log in and grant the application permission to access their resources. Once the user has granted permission, the service generates an access token, which the third-party application can use to access the user's resources for a limited time period.
    => This mechanism is used by companies such as Amazon, Google, Facebook, Microsoft, and Twitter to permit the users to share information about their accounts with third-party applications or websites.
    => Generally, OAuth provides clients a "secure delegated access" to server resources on behalf of a resource owner.
    => It specifies a process for resource owners to authorize third-party access to their server resources without providing credentials. 
    =>  Designed specifically to work with Hypertext Transfer Protocol (HTTP), OAuth essentially allows access tokens to be issued to third-party clients by an authorization server, with the approval of the resource owner. The third party then uses the access token to access the protected resources hosted by the resource server.
    => OAuth is considered to be more secure than sharing login credentials, as the user's password is never shared with the third-party application.
    => However, OAuth does come with its own set of "security risks, such as phishing attacks and malicious applications that may trick users into granting access to their resources." To mitigate these risks, web applications should implement strong security measures, such as verifying the identity of the third-party application and providing clear and transparent user consent processes.

------------------------------------------------------------------------------------

##  What is REST api? 

  => REST stands for the ("Representational State Transfer") API is a "web-based architectural style for creating API's".
  => REST APIs "allow communication between different systems on the internet".

  => REST API's allows data to be exchanged between systems using HTTP and other standard web protocols.

-- In REST, resources are identified by URLs.
URL is https://api.todoapp.com

-- Each resource has a unique URL that clients can use to interact with it.

-- Data can manipulated using a standardized set of HTTP methods, such as
    GET (to retrieve a data),
    POST (to create a new data),
    PUT (to update an existing data),
    and DELETE (to remove a data).

# Endpoints

Assuming your To-Do API has tasks, here are some example endpoints:

GET /tasks: Retrieve a list of all tasks.
GET /tasks/{id}: Retrieve a specific task by its ID.
POST /tasks: Create a new task.
PUT /tasks/{id}: Update an existing task.
DELETE /tasks/{id}: Delete a task.

# Request
{
    "title": "Buy groceries",
    "due_date": "2023-08-15"
}


# Respose:
200,300,400,500

-- REST API is a "stateless architecture", means  "server does not store any information about the client's state between requests".
-- Instead, each request contains all the information necessary for the server to understand and process the request.

  => REST API's  allows developers to "create flexible and scalable APIs" that can be used by a variety of different applications and systems.

  => REST API's are "easy to implement", as it uses standard web protocols and HTTP methods, and can be used with a wide range of programming languages and frameworks.
  => Advantages of REST API include its scalability, flexibility, and compatibility with a wide range of systems and devices.
  => However, designing and implementing a REST API can be complex, as it requires careful consideration of resource design, URL structure, and HTTP method usage. Additionally, security is also a concern, and proper authentication and authorization mechanisms must be implemented to prevent unauthorized access to resources.

----------------------------------------------------------------------------------

## What is gRPC(global Remote Procedure Call)? 

-- gRPC is a modern remote procedure call (RPC) framework "developed by Google".
-- It's "used for building efficient and fast communication between different services in a network", often in microservices architectures.

1. RPC (Remote Procedure Call):

Like a function call, but it happens across different machines on a network.
Allows applications to request services or data from other applications as if they were local.

8. Example:

Let's say you have an e-commerce system.
The client (web app) wants to get product information from the server.
Using gRPC, the client sends a request for product information, and the server responds with the details.
9. Benefits:

Efficient binary serialization for smaller payloads and faster transmission.
Language-agnostic, so services can be written in different languages.
Supports different communication styles, making it versatile for various use cases.
10. Use Cases:

Microservices communication.
Real-time applications.
Distributed systems where performance matters.
11. Drawbacks:

Requires code generation, which can be a bit complex at first.
May not be the best choice for all scenarios, such as simple data sharing.
In simple terms, gRPC is a way for different programs to talk to each other efficiently and quickly, even if they're running on different computers. It's like having a phone call between programs, but instead of voices, they exchange data in a structured and fast manner.

----------------------------------------------------------------------------------------

## What is GraphQL? **

    => GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. 'Developed by Facebook'. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.
    => With GraphQL, the client specifies the data it needs and the server returns only the requested data, reducing over-fetching and under-fetching of data. This is achieved through a single endpoint and a strongly typed schema that defines the available data and operations.
    =>  It provides a more efficient, powerful, and flexible way to retrieve and manipulate data from APIs compared to traditional REST APIs.
    => GraphQL is also highly adaptable and can be used with a wide range of programming languages and frameworks. It can also be used alongside existing REST APIs, allowing for a gradual migration to a more efficient and flexible data retrieval system.
    => However, implementing GraphQL can require more upfront investment in design and development compared to traditional REST APIs. Additionally, proper caching and security measures must be implemented to prevent overloading and unauthorized access to the API.

---------------------------------------------------------------------------------------

## What is HTTP ? 

-- HTTP stands for the ("Hypertext Transfer Protocol");
-- HTTP is a protocol for fetching resources such as HTML documents. 

-- It is the foundation of data communication on the World Wide Web and is used to transfer data and files over the internet.
-- HTTP is a "stateless protocol", which means that "each request and response is independent of any previous request or response". 
-- A client sends an HTTP request to a server, which responds with an HTTP response. The client and server can communicate multiple times, but each time, the client must include all necessary information in the request.
-- HTTP requests and responses are made up of a set of "headers" and an "optional message body".
-- The headers contain information about the request or response, such as the method (GET, POST, etc.), the content type, and the length of the message body. The message body contains the actual data being transmitted, such as an HTML document or an image file.

-- HTTP is a "text-based protocol" and can be accessed and manipulated using a variety of tools, such as web browsers, command-line utilities like cURL, and programming libraries like Python's requests.
-- One of the key advantages of HTTP is its simplicity and flexibility

----------------------------------------------------------------------------------------

## What is a web socket ? 

-- The WebSocket API (WebSockets):

      -- The WebSocket API is an  "two-way interactive communication session between the user's browser and a server".
  -- With this API, you can send messages to a server and receive event-driven responses "without having to poll the server for a reply".
-- Both the client and server can send messages to each other at any time without the need for a new request.
-- The WebSocket protocol is a "TCP-based network protocol" that specifies how data is transferred between networks.
-- "Dependable and efficient". 
-- TCP is a protocol that "establishes communication between two endpoints, often known as sockets". 
-- "Two-way connections":  WebSocket allows for data to flow in both ways at the "same time", causing data to be retrieved more rapidly. 
-- WebSocket, in particular, allows a web application to communicate directly with a WebSocket server.
-- "UseCase":
applications that require real-time data updates or frequent data exchange, such as
-- chat applications,
-- online gaming, and 
-- financial trading platforms.
--With WebSockets, updates can be pushed to clients immediately, without the need for polling or long-polling techniques, reducing latency and improving performance.
-- WebSockets can be used alongside other web technologies, such as REST APIs, to provide a full-featured, real-time web application. They are supported by most modern web browsers and can also be used with mobile and desktop applications.

-----------------------------------------------------------------------------------------

## Polling in node.js :

=> In the Node.js, polling refers to a "technique used to periodically check for changes or updates in a particular resource or data source".
--It "involves repeatedly sending requests or queries to the resource at predefined intervals to determine if any new information is available".

  Polling can be implemented in various scenarios, such as:

  (1) File System Monitoring: Polling can be used to check if a file or directory has been modified, created, or deleted. By periodically querying the file system, you can detect changes and trigger appropriate actions.

  (2) Database Updates: In a database-driven application, polling can be employed to check for new records or updates in a database table. This can be useful for real-time data synchronization or triggering events based on database changes.

  (3) API Integration: When integrating with external APIs, polling can be used to fetch updates or changes from the API endpoints. For example, if you're building a weather application, you might periodically poll the weather API to fetch the latest forecast data.

  => "Long Polling": 
    Long polling is a variant of polling where the client sends a request to the server, and the server keeps the connection open until new data or an event is available. This technique "reduces the frequency of requests" and provides near-real-time updates.

  It's worth noting that polling can be resource-intensive, as it involves constantly sending requests, even if there are no updates. To mitigate this, alternative techniques such as webhooks or event-driven architectures are often preferred, where the server or data source notifies clients of updates instead of the clients polling for them.


-----------------------------------------------------------------------------------------

## What is Caching? **

1. Cache Basics:
A cache is like a fast storage space that holds a part of data temporarily.
It stores data that is often needed to make future requests faster.
Caching is about using previously fetched or calculated data efficiently.

2. Cache Location:
-- Data in a cache is stored in fast-access hardware like "RAM" (computer memory).
-- The main goal of a cache is to make data retrieval faster by avoiding slower storage.

3. Trade-Off and Use:
Caches store a subset of data temporarily, unlike complete and durable databases.
Caches trade storage capacity for speed, making data access faster.
RAM and In-Memory Engines:

RAM and in-memory systems offer quick data retrieval.
Caching with them improves performance and lowers costs compared to traditional databases.

4. Applications:
Caches can be used in different layers like operating systems, networks, web apps, and databases.
They reduce delays and improve performance for tasks like Q&A, gaming, media sharing, and social networking.

5. Design Patterns:

A dedicated caching layer lets systems work independently without affecting the cache.
It's a central layer accessible by various systems with separate lifecycles.

6. Caching Best Practices:
Understand data validity for successful caching; cache hit is data present, cache miss is not.
Use controls like TTLs (expiration time) to manage cache data.
Consider high availability using systems like Redis for a reliable cache environment.

7. Benefits of Caching:
-- "Better Performance": Caching in-memory makes data access super fast, boosting app speed.
-- "Cost Savings": A cache can replace multiple databases, cutting costs significantly.
-- "Lighter Backend Load": Caching eases database load, preventing crashes during high demand.
-- "Predictable Performance": Cache helps handle spikes in app use, maintaining stable performance.
-- "No Hotspots": Caching avoids overloading specific data, keeping predictable performance.
-- "Increased Throughput": In-memory systems handle lots of requests quickly, enhancing read speed.


----------------------------------------------------------------------------------

## What are ways to cache on the backend?

    => Caching on the backend refers to "storing frequently accessed data" in cache on server-side to reduce the amount of time and resources required to regenerate the same data again. 

    => Methods/ways to store the cache:

    (1) In-Memmory-Caching:
    -- The cache is stored in servers RAM, making it faster to retrive data. However this method has limitation of storage and cached data is lost if server is restarted.
    (2) File-based caching:
    -- The cache is stored in files on servers hard disk. This provide more storage capacity but slower to retrieve data.
    (3) Database caching:
    -- Chache is stored in database, providing a balance between storage capacitty and retrieval speed. 
    -- Suitable for the applications which need to store large amount of cache.
    (4) Distributed Chaching:
    -- Cache is distributed across the multiple servers, provides the high level of scalability and fault-tolerance.
    -- suitable for large-scale web applications that need to handle high traffic and large amounts of data.
    (5) Proxy caching: 
    -- Proxy server is placed betwwen the client and the backend server, and the proxy server cache frequently access the data

----------------------------------------------------------------------------------------

##  What is LRU cache? **

    => A Least Recently Used (LRU) Cache organizes items in order of use, allowing you to quickly identify which item hasn't been used for the longest amount of time.
    => LRU cache is often "implemented by pairing a doubly linked list" with a hash map.
    =>  In LRU caching, the cache is designed to store a fixed number of items, and whenever a new item is added to the cache, if the cache is already full, the least recently used item in the cache is removed to make space for the new item.
    => The LRU cache keeps track of the usage of items in the cache and sorts them based on the order of their last access time. When the cache is full, the least recently used item is evicted from the cache to make space for the new item. This helps to ensure that the cache contains the most frequently accessed items and reduces the number of cache misses, which can improve the performance of the system.

    => Pros:

    -> Super fast accessess: LRU caches store items in order from most-recently used to least-recently used. i.e. both can be accessed in O(1) time.
    -> super-fast updates: Each time an item is accessed, updating the cache takes O(1) time.

    => Cons:

    -> Space heavy: An LRU cache tracking n times requires a linked list of length n,,and a hash map holding n items. Thats O(n) space. But its still two data structures.
    => Overall, LRU caching is a simple and effective technique for managing the cache in a wide range of applications, including databases, web servers, and operating systems.

---------------------------------------------------------------------------------------

##  What is Redis? Why do we use it? 

    => Redis is an open source , "in-memory data structure store" used as a 'database', 'cache', 'message broker', and 'streaming engine'.  
    -- Redis has built-in replication, LUA scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.

    => Redis also reffered as 'data structure server' because it allows users to store and manupulate data structure, such as  strings, hashes, lists, sets, and sorted sets, using simple commands.

    => Redis is popular for its "high performance, scalability and low latency."

    => Its basically a data-structure that stores the data in the primary memmory of a computer system. So the reading and writing are made faster than than database that stores data on disks. Thats why redis used as cache in many applications, to provide results rapidly.

    =>  suitable for applications that require fast data access, such as real-time analytics, session management, and message queuing.

    => "Redis also supports persistence", which means that data can be saved to disk for long-term storage.

    => "Redis-store the data in key-value pair" and has ability to store the data using a variety of data structures like:
      -- strings
      -- lists
      -- sets
      -- sorted sets
      -- hashes
      -- bit arrays 
      -- hyperlogs
      -- streams

    => Use cases of Redis:

      (1) Caching: Redis can be used as cache for frequently accesed data, such as web pages, API responses, and session data, to reduce the load on primary data store and improve performance.
      (2) Real-time analytics: Redis can be used to store and process real time data, such as user activity, clickstream data, and sensor readings, to generate real-time insights and alerts.
      (3) Message queuing: Redis can be used as message broker to support asynchronous communication between applications and services, using publish/subscribe or queuing models.
      (4)Leaderboards and rankings: Redis can be used to maintain sorted sets of scores and rankings, such as game leaderboards, stock tickers, and social media trends.
      (5) Session management: Redis can be used to store and manage session data for web applications, such as user login sessions and shopping cart contents.

---------------------------------------------------------------------------------------

## How can we implement caching on frontend? **
    
Benefits of Browser Caching:
-- Caching in the browser is preferable due to cost and unpredictability of API requests.
-- Retrieving data from local cache is faster and more predictable than network requests.

"Frontend Caching Techniques":

1. Browser Caching:
-- Browsers cache static assets (images, styles, JS files) to reduce server requests.
-- Control cache duration using logic and headers to determine cache expiration.

2. Service Worker Caching:
-- Service workers intercept network requests and can store responses in cache.
-- Cache API is used to save and serve responses from cache, reducing network traffic.

3. In-Memory Caching (React):
-- React and other frameworks support in-memory caching using memorization.
-- Memorization caches function/component results based on input, improving reusability.

4. Local Storage Caching:
-- Local storage caches data for persistence across page loads (e.g., preferences).

5. CDN Caching:
-- CDNs cache assets and content globally, minimizing latency for users.
Content is served from nearby edge servers, improving performance.
CDN offloads caching and content delivery to third-party providers.

----------------------------------------------------------------------------------

## What is a CDN ? 

-- A Content Delivery Network (CDN) is a "geographically distributed group of servers" that caches content "close to end users".
-- CDNs allow for the "quick transfer of assets needed for loading internet content", including HTML pages, JS files, stylesheets, images, and videos.
-- Majority of web traffic, including from major sites like Netflix, Amazon, and Facebook, is served through CDNs.
-- Properly configured CDNs can "help protect websites from common malicious attack"s like Distributed Denial of Service (DDOS) attacks.

"Benefits of using CDNs":

1. Reduce Latency:
-- Content distribution to nearby CDN servers leads to faster page loading times.
-- Improved website speed enhances user experience and engagement.

2. Reduce Bandwidth Costs:
-- CDNs employ caching and optimizations to decrease data provided by origin servers.
-- Reduced data transfer lowers expenses for website owners.

3. High Availability and Redundancy:
-- CDNs handle high traffic volumes and hardware failures better than origin servers.
-- Distributed nature ensures uninterrupted website function.

4. Improve Security:
-- CDNs offer DDoS mitigation and security certificate enhancements.
-- Various security optimizations contribute to improved website safety.
At its core, a "CDN is a network of servers aiming to deliver content quickly, inexpensively, reliably, and securely".

== "CDNs place servers at Internet exchange points (IXPs)" where different Internet providers connect, enhancing speed and connectivity.
IXPs facilitate access to traffic originating from various networks, reducing costs and transit times for CDN providers.
CDNs optimize standard client/server data transfers, placing Data Centers globally for strategic coverage, enhanced security, and survival against failures and congestion.

-------------------------------------------------------------------------------------

## What is DNS? **

-- DNS stands for ("Domain Name System");
-- DNS is the phonebooks of internet.
-- we access the information online through internet Protocol(IP) address. 
-- DNS translates domain names to IP addresses so browsers can load Internet resources.
-- Each device connected to Internet has unique IP address which other machines use to find the device.
-- DNS servers eliminate the need for human to memmorise IP addresses.
-- DNS converts the hostname (eg.www,google.com) into computer friendly IP address. BY that IP addess computer can find the right device.

----------------------------------------------------------------------------------------

## How does the internet work? **

    => Internet is backbone of Web, its technical infrastructure the makes the Web possible.
    => Internet is large network of computers which communicate all together.
    => internet works by usibg the packate routing network the follows Internet Protocol(IP) and Transport Control Protocol(TCP).
    => "Data sent over the internet is called , message. Before messages get sent they're broken up into tiny parts called packets"
    => IP: Rules that govern how info is sent from one computer to another computer over an internet. Its numerical address consist four digits (eg.192.168.2.10).
    => TCP works with IP to ensure transfer of data is dependable and reliable.
    => The device is connected to web through a modem or router, which allows it to connect to each other networks around the globe.
    => Router allows for multipe computers to join the same network while a modem connects to your Internet Servive Provider(ISP).
    => Client computer connected to internet through ISP.
    => Servers are the computer directly connected to internet.
    => URL (Uniform Resource Locator) enter into browser this query is processed and pushed to ISP.
    => ISP has multiple servers which store and send data like NAP SERVER(Network Access Protection) and DNS(Domain Name Server).
    => DNS Translate the text based domain name into number-based IP address.(eg Google.com becomes 64.233.191.255)
    => Browser sends a HTTP request to the target server to send a copy of page to client using TCP/IP.
    => HTTP is the language for internet communication.
    =>server approves the request and sends a "200 ok" message to client computer. Then server sends web page files to browser in the form of data packets.
    => Web page loads as your browser reassembles packets.

---------------------------------------------------------------------------------------

## How do Browsers work ?

  => Browsers are software applications that allow users to access and view content on the internet. 
  => Steps that browser takes to get data:

  (1) User Input: The user types a URL or a search term into the browser's address bar.

  (2)Domain Name System (DNS) Lookup: The browser sends a request to a DNS server to translate the domain name into an IP address. The DNS server responds with the IP address of the server that hosts the website.

  (3)Establishing a Connection: The browser opens a connection to the server using the HTTP or HTTPS protocol.

  (4)Sending a Request: The browser sends a request to the server for the webpage or resource that the user requested.

  (6)Server Response: The server sends a response to the browser with the requested webpage or resource.

  (7)Rendering the Page: The browser processes the HTML, CSS, and JavaScript code to render the webpage. This involves parsing the HTML to create a Document Object Model (DOM) and interpreting the CSS to create a Render Object Model (ROM). The browser then uses these models to create a visual representation of the webpage.

  (8)Displaying the Page: The browser displays the rendered webpage on the user's screen.

  (9)Handling User Interaction: The browser listens for user interaction, such as clicks or scrolls, and responds accordingly by updating the DOM and ROM and redrawing the page.

  (10)Caching: The browser may cache certain resources, such as images or scripts, to improve performance and reduce the number of requests to the server.

  (11)Security: The browser may perform security checks, such as checking for SSL/TLS certificates and verifying that the webpage is not known to be malicious.

  (12)Overall, browsers use a combination of network protocols, markup languages, scripting languages, and rendering engines to provide a seamless user experience when browsing the web.

---------------------------------------------------------------------------------

## What is a URLs full form? Explain what a url is and the four parts it consists of The protocol in use The hostname of the server The location of the file The arguments to the file

URL stands for Uniform Resource Locator. It is a standardized address used to identify resources on the internet. A URL provides the necessary information for a client (such as a web browser) to locate and retrieve a specific resource, such as a webpage, image, or file.

A URL consists of four main parts:

  (1) Protocol: The protocol indicates the method or protocol that the client uses to communicate with the server and retrieve the resource. The most common protocol is "http://" or "https://" for web pages, but there are other protocols like "ftp://" for file transfer, "mailto://" for email, and "file://" for local files.

  (2) Hostname: The hostname represents the domain name or IP address of the server that hosts the resource. It identifies the location where the resource is stored. For example, in the URL "https://www.example.com", "www.example.com" is the hostname.

  (3) File Path: The file path specifies the location of the specific file or resource on the server. It includes the directory structure leading to the file. For example, in the URL "https://www.example.com/images/pic.jpg", "/images/pic.jpg" is the file path.

  (4) Query Parameters: Query parameters, also known as arguments or query strings, are optional parts of a URL that provide additional information or data to the server. They are appended to the end of the URL and separated by a question mark (?) from the rest of the URL. Query parameters are in the form of key-value pairs, with each pair separated by an ampers and (&). For example, in the URL "https://www.example.com/search?q=keyword&page=1", the query parameters are "q=keyword" and "page=1".

Together, these four parts form a complete URL that allows the client to communicate with the server, specify the desired resource, and provide any necessary additional information.

Note: It's important to mention that URLs can have additional components such as fragments (indicated by a hash sign #) for specifying a specific section within a webpage and port numbers for non-standard port usage. However, the four parts mentioned above are the fundamental components of a URL.
--------------------------------------------------

## What is Stateless Backend ? 

  => As the name suggests, the stateless server has "no state with regard to the user's information".
  => It means when the "user access any web resource, the server does not keep a track of the user's identity or actions performed on the page". So "every time, the user has to prove the identity to gain access".
  => It is opposite the statefull server in which servers store user's state information in the form of sessions. It stores information like profile, preference, user’s action and gives personalized experience on next visit. The user does not need credentials every time during the valid session.  
  => While "stateless server treats each request as independent" and "demand user credentials". It requires no knowledge of previous interactions and stores no session information. So, there’s no difference between previous, current, and next requests.
  => Features of Stateless Server:

        (1) Performance: The server is not bound to maintain and synchronize any session data.  So it can serve any number of requests to the user. It helps in achieving a "better response time with experience".  

        (2) Design: This is regarded as an advantage for developers as the stateless servers" are easy to design and code" because there is "no need to dynamically maintain storage units in the backend". Stateless authentication uses REST (Representational State Transfer) to design the web APIs.  

        (3) Data Crash: Since no data is stored on the server, there is nill impact of a system crash on the performance of the web resource. The user can simply recover the access by reauthenticating. "Stateless uses a different server on each request. So there's a scope of horizontal expansion".

        (4)Technology: The stateless servers can identify users based on a unique ID called Tokens. Tokens accommodate essential information of the user grants access after receiving the secret token. 
        -- The validity of the token depends either on session duration or it gets destroyed once the user logs out of resource. This technology is known as cryptography which is going to be a major driver of future security mechanisms.

----------------------------------------------------------------------------------

## What is the client server model ?
    
    => Client-server model is distributed application structure that partition tasks or workload between the providers of resource or service called servers and service requesters called clients.
    => In the client-server architecture, when the client computer sends a request for data to the server through the internet, the server accepts the requested process and deliver the data packets requested back to the client.
    => Clients do not share any of their resources. Examples of Client-Server Model are Email, World Wide Web, etc.
    
    * How the Client-Server Model works ?

    -> Client: When we talk the word Client, it mean to talk of a person or an organization using a particular service. Similarly in the digital world a Client is a computer (Host) i.e. capable of receiving information or using a particular service from the service providers (Servers).
    -> Servers: Similarly, when we talk the word Servers, It mean a person or medium that serves something. Similarly in this digital world a Server is a remote computer which provides information (data) or access to particular services.
    -> So, its basically the Client requesting something and the Server serving it as long as its present in the database.

=> Advantages of Client-Server model:

    (1) Centralized system with all data in a single place.
    (2) Cost efficient requires less maintenance cost and Data recovery is possible.
    (3) The capacity of the Client and Servers can be changed separately.

=> Disadvantages of Client-Server model:

    (1) Clients are prone to viruses, Trojans and worms if present in the Server or uploaded into the Server.
    (2) Server are prone to Denial of Service (DOS) attacks.
    (3) Data packets may be spoofed or modified during transmission.
    (4) Phishing or capturing login credentials or other useful information of the user are common and MITM(Man in the Middle) attacks are common.



-----------------------------------------------------------------------------------------

## What is HTTP vs HTTPS? 

  -- HTTPS stands for "HyperText Transfer Protocol Secure", and it is a secure version of HTTP.
  -- "HTTP" transmits data in "plain text", while "HTTPS" "encrypts" data to protect privacy and integrity.
  -- "HTTPS" uses "SSL / TLS certificates" to establish a secure connection between a client and a server.
  -- HTTPS provides data privacy, integrity, and authentication, preventing data breaches and unauthorized access.
  -- Websites using HTTPS display a padlock icon, building trust with users.
  -- HTTPS is increasingly required for websites handling sensitive data or conducting online transactions.
  -- Main differences between HTTP and HTTPS are encryption, SSL/TLS certificates, and advantages of data protection, trust, and compliance.
      eg. 
            const https = require('https');

            // Define the options for your HTTPS request, including the URL, method, headers, and any other necessary parameters:
            
            const options = {
              hostname: 'api.example.com',
              path: '/endpoint',
              method: 'GET',
              headers: {
                'Content-Type': 'application/json'
              }
            };
        
        // callback 
        
            const req = https.request(options, (res) => {
            // Handle the response here
            let data = '';

            res.on('data', (chunk) => {
              data += chunk;
            });

            res.on('end', () => {
              console.log(data);
              // Process the received data
            });
          });

          // Handle any errors that occur during the request
          req.on('error', (error) => {
            console.error(error);
          });
--------------------------------------------------------

## What is a cluster?

=>At a high level, a computer cluster is a "group of two or more computers", or nodes, that "run in parallel to achieve a common goal". This allows workloads consisting of a high number of individual, parallelizable tasks to be distributed among the nodes in the cluster. As a result, these tasks can leverage the combined memory and processing power of each computer to increase overall performance.

-- Enterprise computing. In a computer system, a cluster is a group of servers and other resources that act like a (single system and enable high availability, load balancing and parallel processing). These systems can range from a two-node system of two personal computers (PCs) to a supercomputer that has a cluster architecture.

--------------------------------------------------------

## What is throughput? 
      
      Defination:-Throughput refers to "the amount of data or work that can be processed by a system or component" within a given period of time. "It is a measure of the efficiency and capacity of a system" to handle a certain workload or traffic.
      -It can understood by number of transactions or operations that can be processed by a system or application in a given period of time. For example, a database system may have a high throughput if it can handle a large number of transactions per second.
      In the context of Node.js backend development, throughput refers to the rate or volume of work that a Node.js server can handle within a given timeframe. It is a measure of the server's ability to process incoming requests and respond to them efficiently.

      Throughput in a Node.js backend can be influenced by several factors, including processing power of the system, available memory, network bandwidth, and software architecture.

      => Techniques can used to high throughput,

      => Event-driven, non-blocking I/O: Node.js uses an event-driven, non-blocking I/O model, which allows it to handle multiple concurrent connections without blocking the main event loop. This enables high concurrency and efficient handling of incoming requests, leading to higher throughput.

      => Asynchronous programming: Node.js encourages the use of asynchronous programming techniques, such as callbacks, Promises, and async/await, which allow for efficient handling of I/O-bound operations. This prevents the server from blocking while waiting for I/O operations to complete, resulting in higher throughput.

      => Scalability: Node.js applications can be designed to scale horizontally, meaning they can be distributed across multiple instances or nodes to handle increased load. This can be achieved using techniques such as clustering or load balancing, which allow for better utilization of resources and increased throughput.

      => Performance optimizations: Properly optimizing the performance of a Node.js application can significantly impact its throughput. This may involve techniques such as optimizing database queries, caching, minimizing unnecessary computations, and optimizing resource utilization.

      => Hardware resources: The hardware resources available to the Node.js server, such as CPU, memory, and network bandwidth, can also impact throughput. Higher hardware resources generally allow for higher throughput, as the server has more capacity to handle incoming requests.

      => Application design: The design of the Node.js application, including its architecture, code organization, and efficiency, can also impact throughput. Well-structured and optimized code, along with efficient handling of requests and responses, can contribute to higher throughput.

      It's important to note that throughput is not the only consideration in evaluating the performance of a Node.js backend. Response time, latency, and resource utilization are also important factors to consider. A well-optimized Node.js backend should aim for a balance between high throughput, low latency, and efficient resource utilization to provide optimal performance for the specific requirements of the application.

--------------------------------------------------------------------------------------

## What is availability ?
    => Availability refers to the ability of a system or service to be operational and accessible when it is needed. In other words, availability is the measure of the percentage of time that a system or service is available and usable by users.
    =>Example: if an online store has an availability of 99%, it means that the store is accessible and operational for 99% of the time and inaccessible for the remaining 1%.
    =>Availability is often measured in terms of uptime, which is the amount of time that a system or service is operational and available to users. Uptime is typically expressed as a percentage of total time, such as 99.9% uptime, which means that the system or service is operational and accessible for 99.9% of the time.
    =>Availability is an important metric for businesses and organizations that rely on systems and services to operate, as downtime can result in lost revenue, decreased productivity, and damage to reputation. Therefore, it is important for these organizations to ensure that their systems and services are designed with availability in mind and that appropriate measures are taken to minimize downtime and maximize uptime.
    => A "service level agreement" (SLA) is a "contract between a service provider and a customer that states the amount of time the service will be available for use". Big cloud providers such as Amazon, Google, and Microsoft promise an availability rate of 99.9% or higher.
    => The availability rate is often described using "nines," with more nines meaning a higher level of availability.

-----------------------------------------------------------------------------------

## What is latency ?
    
    => Latency is the time it takes for data to pass from one point on a network to another.
    => Suppose Server A in New York sends a data packet to Server B in London. Server A sends the packet at 04:38:00.000 GMT and Server B receives it at 04:38:00.145 GMT. The amount of latency on this path is the difference between these two times: 0.145 seconds or 145 milliseconds.
    => High latency can result in delays in the transmission of data and can affect the performance of real-time applications such as online gaming, video conferencing, and VoIP (Voice over Internet Protocol) calls. In these applications, even small delays can be noticeable and can affect the user experience.
    => Low latency is particularly important for financial trading, where a few milliseconds can make a significant difference in the outcome of a trade. High-frequency trading systems often use specialized networks and hardware to reduce latency and gain an advantage over competitors.
    => Most often, latency is measured between a user's device (the "client" device) and a data center. This measurement helps developers understand how quickly a webpage or application will load for users.
    => Factors responsible for latency:
      (1) Distance between client and server.
      (2) Internet speed
    =>Reducing latency involves optimizing the network infrastructure, reducing the number of network devices the data packet has to pass through, and reducing the processing time at each device. This can be achieved through various techniques, such as using faster hardware, reducing the distance between the source and destination, and implementing data compression and caching(CDN).

----------------------------------------------------------------------------------

##  What is rate-limiting ?

    => Rate limiting is a strategy for "limiting network traffic." 
    -- It "puts a cap on how often someone can repeat an action within a certain timeframe".
    -- Rate limiting can help stop certain kinds of malicious bot activity. 
    -- It can also reduce strain on web servers. 

    => Rate limiting "runs within an application", "rather than running on the web server itself". 
    -- It's based on tracking the IP addresses that requests are coming from, and tracking how much time elapses between each request. 

    => A rate limiting solution measures the amount of time between each request from each IP address, and also measures the number of requests within a specified timeframe. If there are too many requests from a single IP within the given timeframe, the rate limiting solution will not fulfill the IP address's requests for a certain amount of time.

        
        eg.
            const rate = (limit, time, blockedTime) => {
            // We define a new function called `rateLimit` which takes three arguments: 
            // `limit` - the maximum number of requests allowed within `time` milliseconds.
            // `time` - the duration of the time window in milliseconds.
            // `blockedTime` - the duration of the blocked period in milliseconds.

            const requests = {};

            // We create an object called `requests` to store the request history for each IP address.

            setInterval(() => {

                // We create a new interval which runs every `time` milliseconds to remove old requests from the `requests` object.

                for (const ip in requests) {

                    // We loop through each IP address in the `requests` object.

                    const now = Date.now();

                    // We get the current timestamp.

                    const timeWindow = requests[ip].timeWindow;

                    // We get the start of the current time window for the IP address.

                    const blockedUntil = requests[ip].blockedUntil;

                    // We get the end of the blocked period for the IP address.

                    requests[ip].requests = requests[ip].requests.filter((time) => {
                        return time > now - timeWindow;
                    });

                    // We remove all requests from the `requests` array for the IP address which are older than `timeWindow` milliseconds.

                    if (requests[ip].requests.length === 0) {
                        requests[ip].timeWindow = now;
                    }
                    // If there are no requests in the current time window for the IP address, we reset the start of the time window to the current timestamp.

                    if (blockedUntil && blockedUntil <= now) {
                        requests[ip].blockedUntil = null;
                        requests[ip].requests = [];
                        requests[ip].timeWindow = now;
                    }
                    // If the blocked period for the IP address has ended, we reset the request count, the start of the time window, and the end of the blocked period.
                }
            }, time);
            // We set the interval to run every `time` milliseconds.

            return (req, res, next) => {
                // We return a middleware function which takes three arguments: `req`, `res`, and `next`.

                const ip = req.ip;

                // We get the IP address of the client making the request.

                const now = Date.now();
                // We get the current timestamp.

                requests[ip] = requests[ip] || { requests: [], timeWindow: now };
                // If there is no entry in the `requests` object for the IP address, we create one and initialize the `requests` array and the start of the time window.

                if (requests[ip].blockedUntil && requests[ip].blockedUntil > now) {
                    const remainingTime = Math.ceil((requests[ip].blockedUntil - now) / 1000);
                    res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
                }
                // If the client is currently blocked, we send a 429 status code with a message indicating how long the client must wait before making more requests.

                else if (requests[ip].requests.length >= limit) {
                    requests[ip].blockedUntil = now + blockedTime;
                    const remainingTime = Math.ceil(blockedTime / 1000);
                    res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
                    // If the client has reached the request limit, we block the client for the specified `blockedTime` and send a 429 status code with a message indicating how long the client must wait before making more requests
                } else {
                    requests[ip].requests.push(now);
                    next();
                }
            }
        };

module.exports={rate}   

            

            // make request
                
                app.get('/api/getdata',rate(10,60*1000,60*1000), (req,res) => {
                  res.status(200).send({"Message":"Get a Data "})
              })

------------------------------------------------------------------------------------

## What are the different ways to do rate limits ?
    
    Strategies to rate a limit:

    (1) "Token Bucket": In this method, a token bucket is used to store tokens that represent the number of requests that can be made within a given time period. Each request consumes a token from the bucket, and if the bucket is empty, the request is denied until new tokens are added. This method provides a constant rate of requests over time.

    (2) "Fixed Window": In this method, the number of requests allowed within a fixed time window, such as a minute or an hour, is limited. Once the limit is reached, further requests are denied until the window resets.

    (3) "Rolling Window": In this method, the number of requests allowed is limited within a sliding time window. The window moves forward with each request, and requests outside of the window are ignored.

    (4) "Adaptive Rate Limiting": In this method, the rate limit is dynamically adjusted based on the current load and usage patterns. For example, if the server is under heavy load, the rate limit can be reduced to prevent overload, while during periods of low load, the rate limit can be increased to allow more requests.

    (5) "Token Bucket with Bursting": In this method, the token bucket has a burst capacity that allows a certain number of requests to be made at once, even if the bucket is empty. This allows for short bursts of requests to be made, while still maintaining a consistent rate over time.

    (6) "IP-based Rate Limiting": In this method, rate limits are applied based on the IP address of the requester. This can be useful in preventing abuse and ensuring fair usage of the service.

------------------------------------------------------------------------------------

## What is a load balancer? **

  => A load balancer is a "device or software component that distributes incoming network traffic across multiple servers or nodes in a server cluster". The primary purpose of a load balancer is to evenly distribute the load or traffic among the servers, improving the overall performance, availability, and scalability of the system.
  => When client send request the load balancer received the request and distributes it to one of the availble server based on various algoriths such as "round-robin"," least connections or IP hash". This ensures that no single server is overloaded and all servers are utilized efficiently.
  => Load balancers can be implemented as hardware devices or software applications that run on dedicated servers or in the cloud. They can also provide additional functionalities such as SSL termination, session persistence, and health checks to ensure that only healthy servers receive traffic.

------------------------------------------------------------------------------------

## Describe how you design an API? **
    
API Purpose:

-- An API (Application Programming Interface) in the backend defines rules for software applications to communicate.
-- Acts as an intermediary for interaction between different software components, systems, or services.

Key API Purposes:

1. Data Exchange:

-- Facilitates data exchange between apps or systems.
-- Structured method to request and deliver data from backend.

2. Integration:

-- Allows combining different software components.
-- Utilizes capabilities without needing complex details.

3. Interoperability:

-- Standard interface for different apps to work together.
-- Hides complexities, regardless of language or platform.

4. Modularity and Scalability:

-- Develops modular and scalable systems.
-- Exposes specific functions for easier updates and scaling.

4. Third-party Access:

-- Permits third-party developers to use backend functions.
-- Creates an ecosystem for building on existing platforms.

5. Security and Control:

-- Ensures secure access to backend functions or data.
-- Implements authentication, authorization, and rate limiting.

6. Overall API Purpose:
Enables smooth communication, integration, and data exchange.
Supports interoperability, modularity, and extensibility among software components, systems, or services.

-------------------------------------------------------------
    => What are requirements?
    => What are technologies can be used in system?
    => define the endppoints including  path, query parameters, request headers, and response body
    => design a data model-> data fields, data types, and relationships between the data objects.
    => Authentication and authorization
    => implement the API
    => Test the API
    => Documentation 
    => Publish the API

------------------------------------------------------------------------------------

## What is a horizontal and vertical scaling?
-- Scaling alters the size of a system. In the scaling process, we either compress or expand the system to meet the expected needs
-- Scaling can be categorized into 2 types: 

      (1) Vertical Scaling:
          -> When" new resources are added to the existing system to meet the expectation", it is known as vertical scaling. 
          -> Consider a rack of servers and resources that comprises the existing system. Now when the existing system fails to meet the expected needs, and the expected needs can be met by just adding resources, this is considered vertical scaling. Vertical scaling is based on the idea of adding more power(CPU, RAM) to existing systems, basically adding more resources.
          -> Vertical scaling is not only easy but also cheaper than Horizontal Scaling. It also requires less time to be fixed.

          (1) When new resources are added in the existing system to meet the expectation, it is known as vertical scaling
          (2) It expands the size of the existing system vertically.
          (3) It is harder to upgrade and may involve downtime.
          (4) It is easy to implement
          (5) It is cheaper as we need to just add new resources
          (6) It takes less time to be done
          (7) Single point of failure
          (8) Examples  of databases that can be easily scaled- "MySQL", "Amazon RDS"

    (2) Horizontal Scaling: 

          (1) When new server racks are added to the existing system to meet the higher expectation, it is known as horizontal scaling.
          (2) It expands the size of the existing system horizontally.
          (3) It is easier to upgrade.
          (4) It is difficult to implement
          (5) It is costlier, as new server racks comprise a lot of resources
          (6) It takes more time to be done
          (7) "High resilience and fault tolerance"
          (8) Examples of databases that can be easily scaled- "Cassandra", "MongoDB", "Google Cloud Spanner"

------------------------------------------------------------------------------------

## How do you build a system which is reliable? **

Building a Reliable System: Key Steps

1. Define Requirements:
Define reliability requirements clearly.
Include factors like availability, fault tolerance, scalability, and performance.

2. Identify Failure Points:
Recognize potential failure areas (hardware, software, network, human errors).
Evaluate their impact and component criticality.

3. Implement Redundancy:
Introduce redundancy in critical parts.
Enable system function despite component failures.

4. Use Monitoring and Alerting:
Set up monitoring and alerts for failure detection.
Observe metrics, error rates, logs, and send alerts.

5. Implement Backups and Recovery:
Create backup and recovery strategies.
Ensure data and systems can be restored after a disaster.

6. Utilize Automation:
Automate tasks to minimize human errors.
Incorporate automated testing, scaling, deployment, and recovery.

7. Perform Regular Maintenance:
Keep system updated, secure, and optimized.
Apply patches, conduct tests before changes.

8. Test and Validate:
Thoroughly test to meet reliability goals.
Conduct unit, integration, and load testing.

9. Result:
Following these steps results in a system that's reliable, resilient, and meets availability and performance goals.

--------------------------------------------------------

## How does your Node.js application handle scale? Elaborate.

-- Scaling Reasons:
  -- Workload is a primary reason for scaling applications.
  -- Scaling also enhances availability and failure tolerance.

-- Cluster Module for Load Balancing:
  -- Cluster module balances load across CPU cores.
  -- Uses fork method from child process module.
  -- Forks main app process to match CPU cores.
  -- Distributes requests across all forked processes.

-- Three Application Scaling Approaches:
1. Cloning:
    -- Easiest method to scale a big app.
    -- Clone app instances, each handles part of workload.
    -- Effective and low development cost.
    -- Node.js cluster module aids in single--server cloning.

2. Decomposing:
    -- Scale by breaking app into functional services.
    -- Different apps with distinct code bases.
    -- May have dedicated databases and UIs.
    -- Embraces microservices concept for loose coupling.

3. Splitting:
    -- Divide app into instances, each manages part of data.
    -- Known as horizontal partitioning or sharding.
    -- Requires lookup to decide which instance to use.
    -- Example: Partitioning users based on country/language.

-- Summary:
  -- Scaling methods include cloning, decomposing, and splitting.
  -- Choose based on complexity and desired outcomes.
  -- Proper implementation offers improved performance and resilience.


-------------------------------------

## What is the difference between JS on the browser and node?

1. Execution Environment:
    Browser:
      --JavaScript runs within the web browser environment.
      --Access to Document Object Model (DOM) for manipulating web page structure.
      --Access to Browser Object Model (BOM) for browser-related tasks.

    Node.js:
      --JavaScript runs outside browsers, on servers.
      --Provides built-in APIs for file system, network interactions, etc.
      --No access to DOM or BOM.

2. Module Handling:
    Browser:
      --JavaScript modules loaded using <script> tag in HTML.
      --Dependency management often requires tools like Webpack or Browserify.

    Node.js:
      --Modules loaded using `require()` function.
      --Dependency management through Node Package Manager (NPM).

3. System Access:
    Browser:
      --Limited system access due to browser security restrictions.
      --Can't perform direct file I/O or network communication.

  Node.js:
    -- JavaScript can access system resources directly.
    -- Enables tasks like file I/O, network communication.

4. Version Control:
  Browser:
    -- Can't control browser versions users employ.

  Node.js:
    -- Control over Node.js version used for applications.
    -- Allows utilization of modern JavaScript features without compatibility concerns.

5. Module Standards:
  Node.js:
    -- Supports both CommonJS and ES modules for importing/exporting code.

  Browser:
    -- Supports ES modules for module system.

6. Overall:
  -- JavaScript syntax consistent across environments.
  -- Execution environment and APIs differ, influencing coding approach and capabilities.


-------------------------------------------

## What is V8 Engine:

1. V8 JavaScript Engine for Google Chrome:
  -- Powers Google Chrome browser.
  -- Executes JavaScript while browsing with Chrome.

2. Role of V8:
  -- Parses and executes JavaScript code.
  -- DOM and Web Platform APIs provided by the browser.

3. Browser--Independent JavaScript Engine:
  -- V8 is independent of hosting browser.
  -- Facilitated rise of Node.js.

4. V8 and Node.js:
  -- Chosen as Node.js engine in 2009.
  -- Powers a significant portion of server-side JavaScript code due to Node.js popularity.

5. V8 Characteristics:
  -- Written in C++, continuously improved.
  -- Portable: Runs on Mac, Windows, Linux, and more.

6. JavaScript Compilation and Execution:
  -- V8 compiles JavaScript internally with just-in-time (JIT) compilation.
  -- JIT compilation speeds up execution.


------------------------------------------

## What are message queues?

Message Queues: Asynchronous Communication in Distributed Systems

1. Introduction to Message Queues:
    Message queues facilitate asynchronous service-to-service communication.
    Often employed in serverless and microservices architectures.
    Messages are stored temporarily and processed independently.

2. Message Storage and Processing:
    Messages remain in the queue until processed and then deleted.
    Each message is processed only once by a single consumer.
    Ensures reliable, ordered, and nonduplicate processing.

3. Benefits and Use Cases:
    Decouples heavyweight processing from main application logic.
    Buffers and batches work, mitigating spikes in resource demands.
    Common in systems with unpredictable or varying workloads.

4. Cloud Architectures and Message Queues:
    Modern applications structured as smaller, independent components.
    Message queues act as coordinators for distributed applications.
    Simplify coding complexity and enhance system performance.

5. Asynchronous Communication:
    System components communicate without direct interaction.
    Message queue offers a lightweight buffer for messages.
    Endpoints allow software components to send and receive messages.

6. Message Types and Processing:
    Messages are typically small units like requests, replies, errors.
    Producer adds messages to the queue; consumer retrieves and acts.
    Each message is processed by a single consumer, guaranteeing order.

7. OnetoOne Communication and Fanout Design:
    One message processed by one consumer (pointtopoint).
    Message queues can be combined with Pub/Sub for fanout patterns.
    Enables multiple consumers to process the same message.

8. Load Balancing and Scalability:
    Message queues distribute work among multiple consumers.
    Balances system load, enhancing scalability and resource usage.

9. Guaranteed Message Delivery:
    Message queues ensure reliable message delivery.
    Messages remain until successfully processed, minimizing data loss risk.

10. Fault Tolerance and Data Integrity:
    -- Messages persist in the queue even during system failures.
    -- Contributes to fault tolerance and maintains data integrity.

11. Complex Workflows and Multi-Step Processes:
    -- Message queues support complex workflows across services.
    -- Coordinate multi-step processes, ensuring order and reliability.

12. Backpressure Handling:
    -- Prevents consumer overload in high-demand situations.
    -- Consumers signal backpressure when overwhelmed.
    -- Queue adjusts message flow to match consumer capacity.

13. Use Cases for Message Queues:
    -- E-commerce: Order processing, inventory updates, notifications.
    -- IoT: Sensor data processing, device communication, alerts.
    -- Finance: Transaction handling, event-driven trading.

14. Popular Message Queue Systems:
    -- Apache Kafka: High-throughput, distributed, fault-tolerant.
    -- RabbitMQ: Open-source, supports various messaging patterns.
    -- Amazon SQS: Managed message queuing service on AWS.

15. Considerations When Choosing a Message Queue:
    -- Throughput and latency requirements.
    -- Compatibility with message formats and serialization.
    -- Scalability and clustering capabilities.

  -------------------------------------------------

  ## Which service by Amazon Web Services (AWS) can you use for Queues?

  Amazon Simple Queue Service (SQS) lets you send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.

  ------------------------------------------------

  # What is Pub Sub ?

  => Publish/subscribe messaging, or pub/sub messaging, is a "form of asynchronous service-to-service communication" used in "serverless and microservices architectures".
  => In a pub/sub model, any message published to a topic is immediately received by all of the subscribers to the topic.
  => Pub/sub messaging can be used to enable event-driven architectures, or to decouple applications in order to increase performance, reliability and scalability.
  => Publish/Subscribe (Pub/Sub) messaging provides instant event notifications for these distributed applications.  
  => The Publish / Subscribe model allows messages to be broadcast to different parts of a system asynchronously.
  => A sibling to a message queue, a message topic provides a lightweight mechanism to broadcast asynchronous event notifications, and endpoints that allow software components to connect to the topic in order to send and receive those messages.
  => To broadcast a message, a component called a publisher simply pushes a message to the topic.
  => Unlike message queues, which batch messages until they are retrieved, message topics transfer messages with no or very little queuing, and push them out immediately to all subscribers.
  => All components that subscribe to the topic will receive every message that is broadcast, unless a message filtering policy is set by the subscriber.

  -------------------------------------------

## What is Webhooks?
  
  => A webhook is one of the many ways to facilitate communication between online services.

  => E-commerce apps need to communicate with payment systems, payment systems need to communicate with banking systems, banking systems need to communicate with customer accounts.

  => A webhook is an HTTP request, triggered by an event in a source system and sent to a destination system, often with a payload of data.
  => Webhooks are automated, in other words, they are automatically sent out when their event is fired in the source system.
  => This provides a way for one system (the source) to “speak” (HTTP request) to another system (the destination) when an event occurs, and share information (request payload) about the event that occurred.
  => Webhooks are used for,
  -- Let’s say you subscribe to a streaming service. At the beginning of each month, your credit card is charged and your bank needs to communicate this to you via SMS or email. The banking system (the source) can use a webhook to call a mailing or SMS service (the destination) to immediately send you a debit notification each time your card is charged.

  => For a system to send webhooks, the system has to be able to support the process. You can build your system to send webhooks by triggering HTTP requests for different types of events, however, webhooks are most common in SaaS platforms like GitHub, Shopify, Stripe, Twilio, and Slack.

  => These platforms support different types of events based on the activities that happen within them.

  => To receive webhook requests, you have to register for one or more of the events (also known as topics) for which the platform offers a webhook. A webhook request will be sent to a destination endpoint on your application so you need to build one for it and register the URL as the *Webhook URL* for that event.

  => Once the webhook registration for an event is complete with the endpoint added, you will receive webhook requests at the destination URL you provided each time the event occurs.

  ## Webhooks vs polling
    => Polling is when your application periodically calls an API to check if an event has occurred or new data exists. Webhooks, on the other, hand push data down to your application when an event occurs in real-time.

    => To capture the difference between these two approaches with a relatable example, polling is like going to the post office to check if you have new mail. Using webhooks is basically having mail delivered to your house every time you have new mail simply by giving the postman your house address.

    => Polling is more resource-heavy compared to webhooks, as it can take multiple network requests before new information is discovered, while webhooks only make network requests when there is new information.

  ## When to use webhooks
    The keyword here is real-time. Use webhooks when you want to:

    Be aware of an event in a connected system in real-time
    Send information to a destination in real-time
    Find a cheaper alternative to polling
    Examples of these types of scenarios include:

    An e-commerce store notifying your invoicing application about a sale
    E-commerce stores notifying merchants when a particular item is out of stock
    Payment gateway notifying merchants about a payment
    Version control systems notifying team members about a commit to a repository
    Monitoring systems alerting administrators about an error or unusual activity in a system
    Synchronizing information across systems — for example when a user changes their email in your HR or CRM system, their email is also changed in the payroll or invoicing system

  -------------------------------------------

  ##  What is Docker? Why do we use it?

1. Docker and Containers:
    -- Docker is a "container runtime".
    -- Containers (Linux Containers) isolate kernel processes, "creating the illusion of a new computer".

  2. Contrast with Virtual Machines:
    -- Containers share the OS kernel.
    -- Isolated binaries/libraries loaded for each container.
    -- Avoids need for separate guest OS installations.

  3. OS Independence:
    -- No need for separate guest OS.
    -- Multiple containers run within a single OS.

  4. Docker's Popularity:
    -- In 2020, Docker gained global recognition as the container solution of choice.

  5. Container Packaging and Dependencies:
    -- Docker packages applications with dependencies in a virtual container.
    -- These self--contained units can run on any Linux server.

  6. Components of Docker:
    -- Daemon: Builds, runs, manages containers.
    -- High--level API: User--Daemon communication.
    -- CLI: User interface for interaction.

7. Docker Workflow:
    -- Developers create container images containing application code and dependencies.
    -- Docker Daemon builds images from Dockerfiles, defining image configurations.
    -- These images are portable and can run consistently on various systems.

  8. Docker Hub and Repositories:
    -- Docker Hub is a centralized registry for Docker images.
    -- Developers can push their images to Docker Hub, allowing others to pull and use them.

  9. Microservices and Scalability:
    -- Docker facilitates the deployment of microservices, enabling scalable architectures.
    -- Services are containerized, simplifying deployment, scaling, and updates.

  10. Application Isolation and Security:
      -- Containers provide process isolation, reducing security risks.
      -- They limit potential damage from security breaches by isolating processes.

  11. Ecosystem Integration:
      -- Docker integrates with various tools for orchestration and management.
      -- Kubernetes, Docker Swarm, and other platforms manage containerized applications.

  12. Hybrid and Multi--Cloud Compatibility:
      -- Docker supports hybrid and multi--cloud deployments.
      -- Containers can run consistently across different cloud providers and on-premises environments.

  13. Continuous Integration and Continuous Deployment (CI/CD):
    -- Docker aids CI/CD pipelines by ensuring consistent environments across development and deployment stages.
    -- Containers reduce "works on my machine" discrepancies.

  14. Challenges and Complexity:
    -- While Docker simplifies certain aspects, container orchestration introduces complexity.
    -- Proper configuration, security measures, and monitoring are crucial for successful container deployments.



-------------------------------------------------------

## What is AWS S3?

=> Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. 
=> Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.

-------------------------------------------------------

## What is EC2 Instance in AWS?

  => Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the Amazon Web Services (AWS) Cloud.
  => "Using Amazon EC2 eliminates your need to invest in hardware up front", so you can develop and deploy applications faster.
  => You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage.
  => Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.

  => Amazon EC2 provides the following features:

      -- "Virtual computing environments, known as instances"

      -- "Preconfigured templates for your instances, known as Amazon Machine Images (AMIs)", that package the bits you need for your server (including the operating system and additional software)

      -- Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types

      -- Secure login information for your instances using key pairs (AWS stores the public key, and you store the private key in a secure place)

      -- Storage volumes for temporary data that's deleted when you stop, hibernate, or terminate your instance, known as instance store volumes

      -- Persistent storage volumes for your data using Amazon Elastic Block Store (Amazon EBS), known as Amazon EBS volumes

      -- Multiple physical locations for your resources, such as instances and Amazon EBS volumes, known as Regions and Availability Zones

      -- A firewall that enables you to specify the protocols, ports, and source IP ranges that can reach your instances using security groups

      -- Static IPv4 addresses for dynamic cloud computing, known as Elastic IP addresses

      -- Metadata, known as tags, that you can create and assign to your Amazon EC2 resources

      -- Virtual networks you can create that are logically isolated from the rest of the AWS Cloud, and that you can optionally connect to your own network, known as virtual private clouds (VPCs)

------------------------------------------------------

## What is Cloudfront in AWS?

  => Amazon CloudFront is a web service that "speeds up distribution of your static and dynamic web content", such as ".html, .css, .js, and image files, to your users".
  => CloudFront delivers your content through a "worldwide network of data centers" called "edge locations".
  => When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.
  => If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.
  => If the content is not in that edge location, CloudFront retrieves it from an origin that you've defined—such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.

    => As an example, suppose that you're serving an image from a traditional web server, not from CloudFront. For example, you might serve an image, sunsetphoto.png, using the URL https://example.com/sunsetphoto.png.

    Your users can easily navigate to this URL and see the image. But they probably don't know that their request is routed from one network to another—through the complex collection of interconnected networks that comprise the internet—until the image is found.

  => CloudFront speeds up the distribution of your content by routing each user request through the AWS backbone network to the edge location that can best serve your content.
  => Typically, this is a CloudFront edge server that provides the fastest delivery to the viewer.
  => Using the AWS network dramatically reduces the number of networks that your users' requests must pass through, which improves performance.
  => Users get lower latency—the time it takes to load the first byte of the file—and higher data transfer rates.
  => You also get increased reliability and availability because copies of your files (also known as objects) are now held (or cached) in multiple edge locations around the world.

-------------------------------------------------

## What is Route 53 In AWS?
1. Amazon Route 53 Overview:
-- Scalable DNS web service, routes users to web apps via domain names.
-- Converts human--readable names to numeric IP addresses.

2. Functions:
-- Registers domain names for websites.
-- Redirects users to healthy resources on failure.
-- Cost--effective, secure, and reliable.
-- Supports various routing policies.

3. Routing Policy Types:
-- Simple: Routes to a single resource.
-- Failover: Redirects from unhealthy to healthy resources.
-- Geolocation: Routes based on user location.
-- Geoproximity: Routes based on user location and content.
-- Latency: Routes to lowest--latency AWS region.
-- Multivalue: Returns multiple values for healthy resources.
-- Weighted: Routes proportionally to resources.

4. Benefits:
-- Highly reliable, built on AWS infrastructure.
-- Scalable for high traffic and large queries.
-- Easy to use, user-friendly setup.
-- Monitors health, redirects on failure.
-- Flexible routing policies.
-- Integrates with AWS services.
-- Secure via IAM integration.

--------------------------------------------------------------

## What is ELB (Elastic Load Balancing) in AWS?

=> Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones.
=>  It monitors the health of its registered targets, and routes traffic only to the healthy targets.
=> Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic.
=> A load balancer distributes workloads across multiple compute resources, such as virtual servers.
=> Using a load balancer increases the availability and fault tolerance of your applications.

----------------------------------------------------------------

# What is a reverse proxy?

=> A reverse proxy is a server that sits in front of web servers and forwards client (e.g. web browser) requests to those web servers.
=> Reverse proxies are typically implemented to help increase security, performance, and reliability.
=> In order to better understand how a reverse proxy works and the benefits it can provide, let’s first define what a proxy server is.

=> What is a proxy server?

=>  forward proxy, often called a proxy, proxy server, or web proxy, is a server that sits in front of a group of client machines.
=> When those computers make requests to sites and services on the Internet, the proxy server intercepts those requests and then communicates with web servers on behalf of those clients, like a middleman.

For example, let’s name 3 computers involved in a typical forward proxy communication:

A: This is a user’s home computer
B: This is a forward proxy server
C: This is a website’s origin server (where the website data is stored)

=> In a standard Internet communication, computer A would reach out directly to computer C, with the client sending requests to the origin server and the origin server responding to the client.
=> When a forward proxy is in place, A will instead send requests to B, which will then forward the request to C. C will then send a response to B, which will forward the response back to A.

=> Why would anyone add this extra middleman to their Internet activity? There are a few reasons one might want to use a forward proxy:

    -- "To avoid state or institutional browsing restrictions" - Some governments, schools, and other organizations use firewalls to give their users access to a limited version of the Internet. A forward proxy can be used to get around these restrictions, as they let the user connect to the proxy rather than directly to the sites they are visiting.
    -- "To block access to certain content"- Conversely, proxies can also be set up to block a group of users from accessing certain sites. For example, a school network might be configured to connect to the web through a proxy which enables content filtering rules, refusing to forward responses from Facebook and other social media sites.
    -- "To protect their identity online" - In some cases, regular Internet users simply desire increased anonymity online, but in other cases, Internet users live in places where the government can impose serious consequences to political dissidents. Criticizing the government in a web forum or on social media can lead to fines or imprisonment for these users. If one of these dissidents uses a forward proxy to connect to a website where they post politically sensitive comments, the IP address used to post the comments will be harder to trace back to the dissident. Only the IP address of the proxy server will be visible.


## How is a reverse proxy different?

    => A reverse proxy is a server that sits in front of one or more web servers, intercepting requests from clients.
    => This is different from a forward proxy, where the proxy sits in front of the clients.
    => With a reverse proxy, when clients send requests to the origin server of a website, those requests are intercepted at the network edge by the reverse proxy server.
    => The reverse proxy server will then send requests to and receive responses from the origin server.

 ** The difference between a forward and reverse proxy is subtle but important.
    A simplified way to sum it up would be to say that,

    => A forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client.

    => On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.

Once again, let’s illustrate by naming the computers involved:

D: Any number of users’ home computers
E: This is a reverse proxy server
F: One or more origin servers
Reverse proxy flow: traffic flows from user's device (D) to Internet to reverse proxy (E) to origin server (F)
Typically all requests from D would go directly to F, and F would send responses directly to D. With a reverse proxy, all requests from D will go directly to E, and E will send its requests to and receive responses from F. E will then pass along the appropriate responses to D.

** Below we outline some of the benefits of a reverse proxy:

    -- Load balancing - A popular website that gets millions of users every day may not be able to handle all of its incoming site traffic with a single origin server. Instead, the site can be distributed among a pool of different servers, all handling requests for the same site. In this case, a reverse proxy can provide a load balancing solution which will distribute the incoming traffic evenly among the different servers to prevent any single server from becoming overloaded. In the event that a server fails completely, other servers can step up to handle the traffic.

    -- Protection from attacks - With a reverse proxy in place, a web site or service never needs to reveal the IP address of their origin server(s). This makes it much harder for attackers to leverage a targeted attack against them, such as a DDoS attack. Instead the attackers will only be able to target the reverse proxy, such as Cloudflare’s CDN, which will have tighter security and more resources to fend off a cyber attack.

    -- Global server load balancing (GSLB) - In this form of load balancing, a website can be distributed on several servers around the globe and the reverse proxy will send clients to the server that’s geographically closest to them. This decreases the distances that requests and responses need to travel, minimizing load times.

    -- Caching - A reverse proxy can also cache content, resulting in faster performance. For example, if a user in Paris visits a reverse-proxied website with web servers in Los Angeles, the user might actually connect to a local reverse proxy server in Paris, which will then have to communicate with an origin server in L.A. The proxy server can then cache (or temporarily save) the response data. Subsequent Parisian users who browse the site will then get the locally cached version from the Parisian reverse proxy server, resulting in much faster performance.

    -- SSL encryption - Encrypting and decrypting SSL (or TLS) communications for each client can be computationally expensive for an origin server. A reverse proxy can be configured to decrypt all incoming requests and encrypt all outgoing responses, freeing up valuable resources on the origin server.

--------------------------------------------------

(1) Introduce yourself


----------------------------------------------------------------

Extra Info(
  Octal literals are a way of representing numbers in the octal numeral system. The octal system is a base-8 numbering system, which means it uses eight digits (0 to 7) to represent numbers. In contrast, the more commonly used decimal system is a base-10 system that uses ten digits (0 to 9).

In programming languages and computer science, octal literals are often used to represent integer values in octal form. In most programming languages, an octal number is represented by prefixing it with a leading zero (0) followed by a sequence of digits ranging from 0 to 7.

For example:

In decimal: 1234
In octal: 02322 (Equivalent to 18^3 + 28^2 + 38^1 + 28^0 = 666 in decimal)+
)

------------------------------------------------------

(1) How does the strict mode affect the hoisting behaviour in js? Are there any diff b/w hoisting in strict mode and non-strict mode?

    => In JavaScript, hoisting is a behavior where "variable and function declarations are moved to the top of their containing scope during the compilation phase", allowing them to be accessed before they are actually declared in the code.

    => The concept of hoisting remains the same in both strict mode and non-strict mode. However, strict mode introduces some changes in the behavior of hoisting. Here are the key differences between hoisting in strict mode and non-strict mode:

      (1) Variable declarations: In non-strict mode, if a variable is "referenced before it is declared, it is automatically assigned a value of undefined" during the hoisting process. "In strict mode, referencing a variable before its declaration results in a ReferenceError".

      (2) Function declarations: In non-strict mode, function declarations are hoisted to the top of the scope, and "you can call a function before its declaration". In strict mode, function declarations are also hoisted, but they are not accessible before their declaration. If "you try to call a function before its declaration in strict mode, it will result in a TypeError".

      (3) Duplicate parameter names: In non-strict mode, defining a function with duplicate parameter names does not produce an error. The last parameter name takes precedence. In strict mode, "defining a function with duplicate parameter names results in a SyntaxError".

      (4) Octal syntax: In non-strict mode, using the legacy octal syntax (e.g., 0123) is allowed, and the number is interpreted as an octal value. In strict mode, using the octal syntax is not allowed, and it results in a "SyntaxError".

      (5) 'this' value in functions: In non-strict mode, if a function is invoked without an explicit receiver object, the 'this' value is set to the global object (e.g., window in browsers). In strict mode, invoking a function without an explicit receiver sets the this value to "undefined".

      It's important to note that hoisting can lead to confusing code and is generally considered a best practice to declare variables and functions before using them, regardless of whether you are in strict mode or non-strict mode. This helps improve code readability and maintainability.
        

-----------------------------------------------------------------------------------------

(2) Are there any potential pitfalls need to know when we are using destructuring in js?

    Yes, there are a few potential pitfalls to be aware of when using destructuring in JavaScript:

    (1) "Variable declaration and assignment": 
        When using destructuring assignment, be careful with variable declarations and assignments. If you accidentally omit the var, let, or const keyword, JavaScript treats it as a regular assignment expression, potentially causing unintended side effects by modifying an existing variable or creating a global variable.

    (2) "Default values and falsy values":
        When using default values in destructuring assignments, be aware of values that are considered falsy in JavaScript (e.g., 0, '', false, null, undefined). If the destructured value is one of these falsy values, the default value will be used instead. This behavior can sometimes lead to unexpected results if you're not careful.

    (3) "Nested destructuring":
        Destructuring can also be used with nested objects or arrays. When dealing with deeply nested structures, make sure the corresponding nested patterns match the structure correctly. Otherwise, you may encounter unexpected errors or receive undefined values.

    (4)" Renaming variables":
        Destructuring allows you to assign a new variable name during the assignment. While this can be useful for avoiding naming conflicts, be cautious when renaming variables to ensure code readability. Renaming variables excessively or using cryptic names can make the code harder to understand and maintain.
    eg.
    const numbers = [1, 2, 3];
    const [first, second, third] = numbers;

    console.log(first);  // Output: 1
    console.log(second); // Output: 2
    console.log(third);  // Output: 3

    (6) "Destructuring arrays need to fallow index": 
        When destructuring arrays, keep in mind that the position matters. The destructured variables are assigned values based on their position in the array, rather than matching them by name. If you need to skip values or access specific elements, use commas to indicate the skipped positions.
        eg.
        const numbers = [1, 2, 3, 4, 5];
        const [first, , third] = numbers;

        console.log(first);  // Output: 1
        console.log(third);  // Output: 3


    (7) "Mixing destructuring with rest syntax":
        The rest syntax (...) allows you to gather remaining elements into a new array or object. Be aware that when using both destructuring and the rest syntax, the rest element must be the last element in the pattern. Placing it anywhere else will result in a syntax error.

        eg.
        const numbers = [1, 2, 3, 4, 5];
      // Using rest syntax to collect remaining elements into the 'restNumbers' array
      const [first, second, ...restNumbers] = numbers;
      console.log(first);       // Output: 1
      console.log(second);      // Output: 2
      console.log(restNumbers); // Output: [3, 4, 5]
      
      const person = { name: 'John', age: 30, city: 'New York', profession: 'Engineer' };

      // Using rest syntax to collect remaining properties into the 'restInfo' object
      const { name, age, ...restInfo } = person;

      console.log(name);      // Output: John
      console.log(age);       // Output: 30
      console.log(restInfo);  // Output: { city: 'New York', profession: 'Engineer' }

    (8) "Performance considerations":
        Destructuring can improve code readability, but it's worth noting that it may have a slight impact on performance compared to traditional assignment. In most cases, the performance difference is negligible, but it's something to consider for performance-critical sections of code.

    By understanding these potential pitfalls and using destructuring with care, you can leverage its benefits while avoiding unexpected issues in your JavaScript code.

---------------------------------------------------

(3) Define a simple class myClass.js with a constructor that takes a name paramerer and greet method that returns a greeting message using the name property. Create a test file called myClass.test where we use jests describe and it functions to define two test cases. The First test case checks that a new instance of Myclass is created with the correct names. The second test case checks that greet method returns the correct greeting message.

// npm install mocha chai --save-dev

    class MyClass{
    constructor(name) {
        this.name=name
    }

    greet() {
        return`Hello, ${this.name}!`
    }
}

let person = new MyClass('Dilip');
person.greet()
module.exports=MyClass



// npm i mocha chai --save

const { expect } = require('chai');

const MyClass = require('./index');

describe('MyClass', () => {
    it('shoud create a new instance with the name', () => {
        const name = 'Dilip';
        const city = 'Nashik'
        const person = new MyClass(name, city);
        expect(person).to.be.an.instanceOf(MyClass);
        expect(person.name).to.equal(name);
        expect(person.city).to.equal(city);
    });

    it('should return the correct greeting message', () => {
        const name = 'Dilip';
        const city = 'Nashik'
        const person = new MyClass(name, city);
        const greetings=person.greet()
        expect(greetings).to.equal(
            `Hello, I am ${name} from ${city}`
        );
    });
});

// npx mocha test.js

---------------------------------------------

(4) Write a program to send a message to a specific client in Socket.io.

--------------------------------------------
(5) How do you use cookies in an Express application.


## How do you manage sessions in express?

=> Session management is an important aspect of web application development, as it "allows you to keep track of user activity and maintain user state across requests". In Express.js, session management can be implemented using the express-session middleware.

    //npm i express-session

    app.use(session({
      secret: 'secret-key',
      resave: false,
      saveUninitialized: false
    }));

-> In the example above, we are using the MemoryStore as the session store, which stores the session data in the server's memory. "secret is used to sign the session ID cookie" and "should be a long random string". resave and saveUninitialized are optional options that control the behavior of the session middleware.

    //To set a value in the session, use the req.session object.

    app.get('/login', (req, res) => {
        req.session.username = 'John Doe';
        res.send('Logged in successfully');
    });


    full code snippet;
          
          const session = require('express-session');

          const express = require('express');

          const app = express();

          // app.set('trust proxy', 1)

          app.use(session({
              secret: 'keyboard',
              resave: false,
              saveUninitialized: true,
              // cookie: { secure: true }
          }));

          app.get('/login', (req, res) => {
              req.session.username = 'Dilip';                    
              res.send('logged in successfully')                 Output   // logged in successfully
          });


          app.get("/profile", (req, res) => {
            const username = req.session.username;
            res.send(`Username: ${username}`);                    Output  // Username:Dilip
          });
          
          app.listen(8000, () => {
              console.log('Server is running on port 8000');
          })

---------------------------------------------------------------

## How do you manage cookies with express?

Cookies can be managed using the cookie-parser middleware. This "middleware parses cookie headers and populates the req.cookies object with cookie values".

    //steps involved

    (1)
      -npm i cookie-parser

    (2) Initialise the cookie-parser

    var express = require('express');
    var cookieParser = require('cookie-parser');
    var app = express();
    app.use(cookieParser());

    (3) To set a cookie, use the res.cookie() method.
        app.get('/set-cookie', (req, res) => {
        res.cookie('username', 'John Doe', { maxAge: 900000, httpOnly: true });
        res.send('Cookie has been set');
        });

   //The httpOnly option ensures that the cookie cannot be accessed using JavaScript

  (4) To get a cookie, use the req.cookies object.

        app.get('/get-cookie', (req, res) => {
        const username = req.cookies.username;
        if (username) {
            res.send(`Hello, ${username}`);
        } else {
            res.send('Cookie not found');
        }
    });

  (5) To delete a cookie, use the res.clearCookie() method.
  app.get('/clear-cookie', (req, res) => {
      res.clearCookie('username');
      res.send('Cookie has been cleared');
  });

-- "By default, cookies are stored in the client's browser memory".

-------------------------------------------

(6) How does middlewares differ from an API gateway ?

    Middlewares and API gateways are both commonly used components in web application architectures, but they serve different purposes and have distinct characteristics:

      (A) Middlewares:

          (1) Functionality: Middlewares are pieces of code that "sit between the client and the server/application logic". They intercept and process requests and responses within an application.

          (2) Scope: Middlewares "operate at the application level", providing reusable functionality that can be "applied to multiple routes or endpoints within the application".

          (3) Function: Middlewares typically "focus on a specific task or concern, such as logging, authentication, input validation, error handling, or request/response transformations".

          (4) Modularity: Middlewares can be "chained or combined to perform multiple sequential tasks", each middleware building on the previous one.

          (5) Flexibility: Middlewares are often framework-specific and tied closely to the application's programming language and framework.

          (6) Granularity: Middlewares "operate at a granular level", allowing for fine-grained control and customization of request/response processing within an application.
          
          
      (B) API Gateways:

          (1) Functionality: An API gateway is a "specialized server that acts as an entry point for client requests", providing a "centralized access point to a collection of backend services or microservices".

          (2) Aggregation: API gateways can "aggregate multiple backend services", allowing clients to interact with a single entry point instead of making multiple requests to different services.

          (3) "Routing and Load Balancing": API gateways handle request routing and load balancing across multiple instances of backend services, distributing the traffic efficiently.

          (4) "Security": API gateways often include security features like "authentication, authorization, rate limiting, and request validation" to ensure the security and integrity of the API.

          (5) "Protocol Translation": API gateways can handle protocol translation, allowing clients to use different protocols (e.g., HTTP, WebSocket, gRPC) while communicating with the backend services.

          (6) "Scalability and Performance": API gateways help improve scalability and performance by caching responses, aggregating requests, and handling traffic spikes.

          (7) "Cross-cutting Concerns": API gateways address cross-cutting concerns like monitoring, analytics, and logging at the API level, providing centralized visibility into the API traffic.

          In summary, middlewares focus on enhancing and customizing the functionality of an application at a granular level, while API gateways act as a centralized entry point, providing routing, aggregation, security, and scalability capabilities for a collection of backend services or microservices.

----------------------------------------------------

(7) How to handle error in Socket.io?

    Socket.IO is a real-time communication library for web applications. It allows you to send and receive data between clients and servers in a bi-directional manner.

    While Socket.IO is a powerful library, it can also be prone to errors. Here are some tips on how to handle errors in Socket.IO:

    (1) Use error events. Socket.IO emits error events whenever an error occurs. You can listen for these events and handle them accordingly.

          eg. 
                const io = require('socket.io')(server);

                  io.on('connection', (socket) => {

                    // Error handling within event handler

                    socket.on('customEvent', (data) => {

                      try {

                        // Your event handling logic here

                      } catch (error) {

                        // Handle the error

                        console.error('An error occurred:', error.message);
                        socket.emit('error', { message: 'An error occurred' });
                      }
                    });

                    // Error handling using the 'error' event
                    socket.on('error', (error) => {
                      console.error('Socket error:', error);
                    });
                  });

    (2)You can also use try/catch blocks to handle errors. This is a good way to catch errors that may not be emitted as events.

    (3) Log errors. It is always a good idea to log errors so that you can track them down and fix them.

    (4) Display error messages to users / Cliet Side Error handling. If an error occurs that prevents your application from working properly, you should display an error message to the user. This will help them understand what went wrong and what they can do to fix it.

      eg.

          var socket = io();

          socket.on('error', function(error) {

            console.log(error);

            // Display an error message to the user
            alert('An error occurred: ' + error);
          });

        This code will listen for error events emitted by the socket instance. When an error event is emitted, the error will be logged and an error message will be displayed to the user.

By following these tips, you can help to ensure that your Socket.IO applications are robust and handle errors gracefully.

----------------------------------------------

(8) Can an object methods access and modify the objects properties in Js?

=> Yes, an object's method in JavaScript can access and modify the object's properties. When a method is invoked on an object, it has access to the object's properties through the 'this' keyword. The this keyword refers to the current instance of the object that the method is being called on.

    Here's an example to illustrate how an object method can access and modify object properties:

    eg.
          const person = {
            name: 'Alice',
            age: 30,

            // method to greeting

            greet() {
              
              console.log(`Hello, my name is ${this.name}.`);
          },

            celebrateBirthday() {
              this.age++;
              console.log(`Now I am ${this.age} years old.`);
            }
          };

         person.greet(); // Output: Hello, my name is Alice.
         person.celebrateBirthday(); // Output: Happy birthday! Now I am 31 years old.


        eg.(2)

          // Define an object

                const person = {
                  firstName: 'John',
                  lastName: 'Doe',
                  age: 30,

          // Method to get full name

                  getFullName() {
                    return `${this.firstName} ${this.lastName}`;
                  },

          // Method to update age

                  updateAge(newAge) {
                    this.age = newAge;
                  }
                };

          // Accessing object properties

                console.log(person.firstName); // Output: John

                console.log(person.age); // Output: 30

          // Calling object methods

                console.log(person.getFullName()); // Output: John Doe

          // Modifying object properties using methods

                person.updateAge(35);
                console.log(person.age); // Output: 35

---------------------------------------------

(9) What is purpose of ZADD and SADD commands in redis ?

      In Redis, the ZADD and SADD commands are used to add elements to sorted sets and sets, respectively.
      Here's a brief explanation of each command:

      (1) ZADD:
        -> This command is used to add one or more members to a sorted set in Redis.
        -> A sorted set is a data structure that stores a collection of unique elements, each associated with a score. 
        -> The elements in a sorted set are ordered based on their scores, allowing efficient retrieval of elements based on their position in the sorted set. The ZADD command takes the following form:

                ZADD key score member [score member ...]
                  
        key: The key of the sorted set.
        score: The score associated with the member. The score is used to determine the ordering of elements in the sorted set.
        member: The element to be added to the sorted set.

          Example usage:

            ZADD myset 1 "member1"
            ZADD myset 2 "member2"
            ZADD myset 3 "member3"



    (2) SADD: This command is used to add one or more members to a set in Redis. A set is an unordered collection of unique elements. The SADD command takes the following form:

        SADD key member [member ...]

    key: The key of the set.
    member: The element to be added to the set.

        Example usage:

        SADD myset "member1"
        SADD myset "member2"
        SADD myset "member3"

    Both ZADD and SADD commands are useful for storing and managing collections of elements in Redis. They allow you to add elements to sorted sets and sets, respectively, enabling efficient retrieval, membership checks, and set operations on the stored data.

--------------------------------------------

(1) Can an object method access and modify the objects's properties in Js?

-------------------------------------------

(2) What is JWT authentication and how does it works?

JWT (JSON Web Token) authentication is a method of "securely transmitting information between parties as a JSON object". It is commonly used for authentication and authorization in web applications and APIs.

JWT authentication works by allowing a user or client to authenticate themselves and receive a digitally signed token in return. This token is then included in subsequent requests to the server to verify the user's identity and access rights.

Here's a step-by-step breakdown of how JWT authentication works:

User Authentication: The user provides their credentials, such as a username and password, to the authentication server.

Token Generation: Upon successful authentication, the authentication server generates a JWT that contains the user's identity (claims), such as their user ID, roles, or any other relevant information. The token consists of three parts: a header, a payload, and a signature. The header typically contains the algorithm used for signing the token, while the payload holds the claims.

Token Issuance: The authentication server sends the JWT back to the user as a response to their authentication request.

Token Storage: The user or client application stores the received JWT securely, commonly in local storage or a cookie.

Token Transmission: For subsequent requests to protected resources, the user includes the JWT in the request headers, typically using the "Authorization" header with the "Bearer" scheme.

Token Verification: The server receiving the request extracts the JWT from the header and verifies its integrity and authenticity using the signature and the server's secret key.

Claims Validation: Once the token's signature is verified, the server decodes the JWT and examines the claims to determine if the user has the necessary permissions to access the requested resource.

Grant Access: If the token is valid and the user's claims are sufficient, the server grants access to the requested resource and processes the request accordingly.

Token Expiration and Renewal: JWTs have an expiration time (defined in the token's payload), after which they are no longer considered valid. If a token expires, the user needs to reauthenticate and obtain a new JWT.

By using a JWT-based authentication mechanism, the server can securely authenticate and authorize users without relying on session state or storing user information on the server-side. Instead, all the necessary information is encapsulated within the token itself, simplifying the process and enabling stateless authentication.

------------------------------------------------

(3) Can you explain middleware concept in Express using an example?

In Express, middleware refers to functions that are executed in the request-response cycle. They play a crucial role in handling and processing incoming requests before they reach the final route handler. Middleware functions have access to the request (req) and response (res) objects and the ability to modify them, invoke additional functions, or terminate the request-response cycle.

Here's an example to illustrate the concept of middleware in Express:

const express = require('express');
const app = express();

// Middleware function
const logger = (req, res, next) => {
  console.log(`[${new Date().toISOString()}] ${req.method} ${req.url}``);

  next(); // Call next() to pass control to the next middleware or route handler
};

// Registering the middleware
app.use(logger);

// Route handler
app.get('/', (req, res) => {
  res.send('Hello, World!');
});

// Starting the server
app.listen(3000, () => {
  console.log('Server listening on port 3000');
});


In this example, we create an Express application and define a middleware function called logger. The logger function logs the current date, HTTP method, and URL of each incoming request.

By calling app.use(logger), we register the logger middleware globally, meaning it will be executed for every incoming request.

When a request is made to the root URL ("/"), the logger middleware will be invoked first, logging the request details to the console. It then calls next() to pass control to the next middleware or the route handler.

In this case, there is a single route handler defined using app.get('/', ...) that sends a simple "Hello, World!" response.

When you start the server and make a request to the root URL, the logger middleware will execute first, logging the request details. Then, the route handler will be invoked, sending the response back to the client.

Middleware allows you to perform various tasks such as logging, authentication, error handling, parsing request bodies, and more. They provide a modular and flexible way to handle common functionality across multiple routes in an Express application.

---------------------------------------------------

(4) Write a simple implementation of Watchman middleware that checks if the user is authenticated before allowing access to protected route.

-------------------------------------------------

(5) How can you limit the number of messages that can be sent to a client from the server in socket.io? Demonstrate one such way througth working code.
    
    const express = require("express");
const app = express();

const server = require('http').createServer(app);

const io = require('socket.io')(server);

const Message_limit = 10;

const clientMessageCounts = new Map();


const rateLimitHandlerMiddleware = (socket,next) => {
  const clientId = socket.id;
  const currTime = Date.now();

  if (!clientMessageCounts.has(clientId)) {
    clientMessageCounts.set(clientId, {
      count: 1,
      lastMessageTime: currTime
    });
  } else {
    const clientInfo = clientMessageCounts.get(clientId);
    const elapsedTime = currTime - clientInfo.lastMessageTime;

    if (elapsedTime < 10000) { //from last time is less than 10 sec
      if (clientInfo.count >= Message_limit) {
        return next(new Error("Messsage rate limit exceeded"));
      } else {
        clientInfo.count++;
        clientInfo.lastMessageTime = currTime;
      }
    } else {
      clientInfo.count = 1;
      clientInfo.lastMessageTime = currTime;
    }
  }
  next();
}


io.on('connection', (socket) => {
  console.log(`Client connected : ${socket.id}`);

  socket.use(rateLimitHandlerMiddleware);

  socket.emit('message',data)

})

io.on('disconnect', (socket) => {
  console.log('Socket is disconnected');
})


server.listen(8000, () => {
  console.log('Server is listening to port 8000');
})
--------------------------------------------------

(6) Write a function query records in a one to many relationship in Node.js and MongoDb. Write the code using the user and post example. Assume the relevant structure/schema wherever applicable.

const usersWithPosts = await User.find().populate('posts').exec();

// app.js
const mongoose = require('mongoose');
const User = require('./user');
const Post = require('./post');

// Assume you have connected to your MongoDB database here

// Function to query posts for a specific user

async function queryUserPosts(userId) {
  try {
    // Find the user based on the userId
    const user = await User.findOne({ userId });

    if (!user) {
      throw new Error('User not found');
    }

    // Find all posts associated with the user's userId
    const posts = await Post.find({ userId });

    // Return the user and their posts
    return {
      user,
      posts
    };
  } catch (error) {
    throw new Error(`Error querying user posts: ${error.message}`);
  }
}

// Usage example

queryUserPosts('123456789')
  .then(result => {
    console.log('User:', result.user);
    console.log('Posts:', result.posts);
  })
  .catch(error => {
    console.error(error);
  });

In this example, the queryUserPosts function takes a userId as an argument and performs the following steps:

It finds the user with the given userId using User.findOne.
If the user is not found, it throws an error.
If the user is found, it proceeds to find all the posts associated with that user's userId using Post.find.
It returns an object containing the user and their posts.

-----------------------------------------------

(7) What is impact of schema on SQl and NoSQL database?

The impact of schema in SQL and NoSQL databases differs significantly. Let's explore the impact of schema in each type of database:

SQL Databases:

"Defined Schema": SQL databases enforce a rigid schema that "defines the structure, data types, and relationships between tables in advance". The schema is typically created using DDL (Data Definition Language) statements like CREATE TABLE. This ensures data integrity and enforces consistency.

"Data Consistency": SQL databases enforce strict data "consistency rules through constraints, such as primary keys, foreign keys, unique constraints, and data types". All records in a table must conform to the defined schema. If any data violates the schema or constraints, the database rejects the operation, maintaining data integrity.

"Scalability Challenges": SQL databases can face scalability challenges due to the strict schema and ACID (Atomicity/availability, Consistency, Isolation, Durability) properties. Altering the schema or making significant changes to the database structure can be complex, requiring careful planning and potential downtime during migrations.

NoSQL Databases:

"Flexible Schema": NoSQL databases offer a flexible schema, allowing you to store and retrieve data without a predefined structure. They can handle various data formats, including documents, key-value pairs, graphs, etc. Each document or record can have a different structure, allowing for schema evolution over time.

"Agility and Speed": NoSQL databases excel at handling large volumes of unstructured or semi-structured data. The absence of a fixed schema enables rapid development and iteration since changes to the data structure can be made without altering the entire database. This agility makes NoSQL databases well-suited for agile development and fast-paced environments.

"Lack of Data Consistency Enforcement": NoSQL databases often sacrifice strong data consistency in favor of high availability and partition tolerance (as per the CAP theorem). They may offer eventual consistency, where data changes propagate asynchronously, and there may be a delay before all replicas are updated. This trade-off allows for high scalability but can result in data inconsistencies in certain scenarios.

"Scalability and Performance": NoSQL databases are designed to scale horizontally, allowing them to handle massive amounts of data and high read/write throughput. The flexible schema and distributed nature of NoSQL databases make it easier to scale by adding more nodes to the cluster, providing excellent performance for certain use cases.

In summary, the impact of schema in SQL databases is more structured, rigid, and focused on data integrity, while NoSQL databases offer flexibility and scalability at the expense of strong data consistency. The choice between the two depends on the specific requirements of your application, such as data structure, scalability needs, and consistency trade-offs.

-------------------------------------------------------

(8) What is difference between server side caching and client side caching?

Server-side caching and client-side caching are two different approaches to caching data in a web application. Here's an explanation of each:

(A) Server-side caching: 

Server-side caching involves "storing and retrieving data on the server", typically "by the web server or an intermediate caching layer" between the web server and the application server. The 'purpose' of server-side caching is to "store frequently accessed or computationally expensive data closer to the server", reducing the need to regenerate or retrieve it from the original source every time it's requested.

The server-side caching mechanism can operate at various levels, such as:

(1) "Database-level caching": Caching "query results or entire database objects" in memory to minimize expensive database operations.
(2) "Application-level caching": Caching the results of "computationally intensive operations or frequently accessed data within the application itself".
(3) "Reverse proxy caching": Using a caching layer, such as "Varnish or NGINX", to store and serve cached responses to incoming requests before they reach the application server.
(4) Server-side caching is transparent to the client and can benefit all users accessing the same resources, as the cached data is shared across requests.

(B) Client-side caching: Client-side caching involves storing and retrieving data on the client-side, typically within the user's web browser. The purpose of client-side caching is to store resources locally, so subsequent requests for the same resources can be served from the cache instead of making a round trip to the server.

Client-side caching is commonly used for:

(1) Caching static assets: Storing JavaScript files, CSS stylesheets, images, and other resources locally on the client's device to avoid repeated downloads.
(2) Storing API responses: Caching the results of API requests on the client-side to reduce network traffic and improve performance.
(3) Client-side caching is controlled by the client, and each user's cache is independent of others. It allows for faster page loading and reduces the load on the server, but it requires careful cache management to ensure the cached data remains up-to-date.

In summary, server-side caching focuses on caching data on the server to optimize performance and reduce resource consumption, while client-side caching aims to cache resources locally on the client's device to minimize round trips to the server and improve user experience.

--------------------------------------------------------------------------------------

(9) What is react? It is library or framework?

React is a JavaScript library for building user interfaces. It is often referred to as a library, although some people may also consider it a framework due to its comprehensive features and ecosystem.

React was developed by Facebook and was first released in 2013. It provides a declarative approach to building UI components, allowing developers to create reusable and interactive UI elements. React uses a virtual DOM (Document Object Model) to efficiently update and render components, resulting in optimized performance.

Here are a few key "characteristics of React":

(1) "Component-based architecture": React encourages a modular approach to UI development by breaking down the user interface into reusable components. Each component can manage its own state and properties, making it easier to build complex UIs.

(2) "Virtual DOM": React uses a virtual representation of the DOM, which is an in-memory representation of the actual HTML DOM. When a component's state changes, React efficiently updates the virtual DOM and determines the most optimal way to update the actual DOM, minimizing unnecessary re-rendering and improving performance.

(3) "JSX": React introduces JSX, a syntax extension that allows developers to write HTML-like code within JavaScript. JSX enables the combination of UI markup and JavaScript logic, making it easier to create and understand React components.

(4)"Unidirectional data flow": React follows a unidirectional data flow pattern, also known as 'one-way binding'. Data flows from parent components to child components, making it easier to track and manage data changes. This pattern enhances predictability and reduces side effects.

While React itself "focuses primarily on the view layer", it can be complemented with other libraries and tools to create full-fledged web applications. React's ecosystem includes additional libraries like React Router for routing, Redux or MobX for state management, and many others.

In summary, React is a JavaScript library that simplifies the process of building interactive user interfaces. It provides a component-based architecture, virtual DOM, JSX syntax, and promotes a unidirectional data flow. These features make React a popular choice for creating dynamic and responsive web applications.

------------------------------------------------

What is useEffect? What is useEffect dependencies?

useEffect is a hook in React. It allows you to perform side effects in your functional components. "Side effects can include things like fetching data from an API, subscribing to events, manipulating the DOM, or setting up timers".

The useEffect hook accepts two arguments: a "function" and an "optional array of dependencies". Here's the basic syntax:

useEffect(() => {
  // Side effect code goes here
}, [dependencies]);

The function you pass to useEffect will be executed after the component renders and re-renders. It runs after the render is committed to the screen. The effect function can contain code that interacts with the outside world or modifies the component's state.

The dependencies array is an "optional second argument". It "allows you to control when the effect should be executed". If the array is empty ([]), the effect will only run once, after the initial render. If you "include dependencies in the array, the effect will re-run whenever any of the dependencies change".

Here's an example that demonstrates the usage of useEffect:


import React, { useState, useEffect } from 'react';

function MyComponent() {
  const [data, setData] = useState(null);

  useEffect(() => {
    // Fetch data from an API
    fetch('https://api.example.com/data')
      .then(response => response.json())
      .then(data => setData(data))
      .catch(error => console.log(error));
  }, []);

  return (
    <div>
      {data ? (
        <ul>
          {data.map(item => <li key={item.id}>{item.name}</li>)}
        </ul>
      ) : (
        <p>Loading data...</p>
      )}
    </div>
  );
}


In this example, the useEffect hook is used to fetch data from an API. It runs only once, after the initial render, because the dependencies array is empty ([]). The fetched data is stored in the component's state using the useState hook (setData(data)). The component renders a list of items if the data is available, or displays a loading message otherwise.

If you include dependencies in the array, like [dependency1, dependency2], the effect will re-run whenever any of those dependencies change. This allows you to control when the effect should run based on specific values or props in your component.

(a) "State variables": You can include state variables that are used within the effect. When any of these state variables change, the effect will be triggered. For example:


const [count, setCount] = useState(0);

useEffect(() => {
  console.log('Count changed:', count);
}, [count]);

In this example, the effect will be executed whenever the count state variable changes.

(b) "Props": If your component relies on certain props, you can include them in the dependency array. When any of the props change, the effect will run. For example:


useEffect(() => {
  console.log('Name changed:', name);
}, [name]);
In this case, the effect will run whenever the name prop changes.

(c) "Variables or constants": You can include regular variables or constants from the component's scope in the dependency array. If any of these values change, the effect will be triggered.

For example:

const someValue = calculateSomeValue();

useEffect(() => {
  console.log('Value changed:', someValue);
}, [someValue]);

In this case, if the calculateSomeValue function returns a different value, the effect will run.

(d) "External functions or APIs": If your effect relies on an external function or API, you can include them in the dependency array. If any of these external dependencies change, the effect will be triggered. For example:


useEffect(() => {
  const fetchData = async () => {
    const response = await fetch('https://api.example.com/data');
    const data = await response.json();
    console.log('Fetched data:', data);
  };

  fetchData();
}, []);

In this case, the effect runs only once, but if the fetchData function changes, it will trigger the effect again.

Note that if you omit the dependency array (useEffect(() => { ... })), the effect will run after every render of the component. If you provide an empty dependency array (useEffect(() => { ... }, [])), the effect will run only once after the initial render.

------------------------------------------------

(10) How can we handle browser "window resize events using useEffect" in react?

    import React, { useEffect,useState } from 'react';

function MyComponent() {
  const [windowSize,setWindowSize]=useState({
    width:window.innerWidth,
    height:window.innerHeight
  })
  const handleResize=()=>{
    setWindowSize({
      width:window.innerWidth,
      height:window.innerHeight
    })
  }
  useEffect(() => {
    window.addEventListener('resize', handleResize);

    return () => {
      window.removeEventListener('resize', handleResize);
    };
  }, []); 

  // Empty dependency array to ensure the effect runs only once on component mount

  return (
    <div>
    <h1>Window Size</h1>
    <p>Width:{windowSize.width}</p>
    <p>Height:{windowSize.height}</p>
    </div>;
  );
};

export default MyComponent;

---------------------------------------------------

What is 'this' keyword in js?


In JavaScript, the "this" keyword refers to the context in which a function is executed. It is a special identifier that allows access to the current object or context within the code.

The value of "this" depends on how a function is called. Here are the main cases:

Global Scope: When "this" is used outside of any function, it refers to the global object, which is typically the window object in a web browser or the global object in Node.js.

Object Method: When a function is invoked as a method of an object, "this" refers to the object itself.

For example:

var myObject = {
  name: "John",
  sayHello: function() {
    console.log("Hello, " + this.name);
  }
};

myObject.sayHello(); // Output: Hello, John

Constructor Function: When a function is used as a constructor with the "new" keyword, "this" refers to the newly created object.

function Person(name) {
  this.name = name;
}

var person1 = new Person("Alice");
console.log(person1.name); // Output: Alice

Event Handlers: When an event handler is triggered, "this" typically refers to the element that triggered the event.

document.getElementById("myButton").addEventListener("click", function() {
  console.log(this); // Output: HTMLButtonElement
});

Explicit Binding: The "this" keyword can also be explicitly bound using the call(), apply(), or bind() methods. These methods allow you to specify the value of "this" within a function.

function greet() {
  console.log("Hello, " + this.name);
}

var person = {
  name: "Jane"
};

greet.call(person); // Output: Hello, Jane

The behavior of the "this" keyword can sometimes be tricky, especially when using arrow functions, nested functions, or when using it in different contexts. It's important to understand the specific context in which "this" is being used to ensure the correct behavior and avoid any unexpected results.

------------------------------------------------------------

(11) How does the 'this' keyword relate to lexical scope in Js?

In JavaScript, the 'this' keyword refers to the context in which a function is executed. It is a special identifier that allows you to access properties and methods within an object or class.

The value of "this is determined dynamically based on how a function is called or invoked", rather than being determined by lexical scope alone. However, understanding lexical scope is crucial for understanding how this behaves in different situations.

Lexical scope refers to the visibility and accessibility of variables and functions based on their location within the source code. It is determined at the time of code compilation and remains fixed throughout the code execution.

When a function is defined, it captures the lexical scope in which it is defined. This captured scope is sometimes called the function's "scope chain" or "lexical environment." It includes variables, functions, and the value of this at the time of the function's creation.

However, the value of this is not determined by lexical scope but by how the function is invoked. The way this is assigned depends on the invocation context:

"Global context": When a function is invoked in the global scope (outside of any object or function), this refers to the global object, which is window in a browser environment or global in Node.js.

"Object context": When a function is called as a method of an object, this refers to the object itself. The value of this is determined at the time of invocation based on the object that precedes the dot notation.

"Constructor context": When a function is used as a constructor with the 'new' keyword, this "refers to the newly created object that the constructor is constructing."

"Event context": When a function is used as an event handler, 'this' usually refers to the "element that triggered the event".

"Explicit binding": You can explicitly set the value of this using methods like call(), apply(), or bind(), which allow you to specify the context in which a function should be executed.

It's important to note that arrow functions (() => {}) have a different behavior for this. They don't have their own this binding but inherit this from the surrounding lexical scope.

In summary, while 'this' is not determined solely by lexical scope, understanding lexical scope is essential for determining the context in which this is evaluated. The value of this is dynamic and depends on how a function is invoked or the context it is called in.

----------------------------------------------------

(12) In strict mode the value of 'this' inside a module is 'undefined' by default.

In JavaScript, 'strict mode' is a feature "introduced in ECMAScript 5 (ES5)" that enforces stricter rules and provides better error handling. When strict mode is enabled, it changes certain behaviors of JavaScript to prevent common mistakes and make the code more predictable.

One of the changes introduced by strict mode is that it "modifies the behavior of the this keyword". Specifically, in non-strict mode, when a function is called in the global scope, the value of this defaults to the global object (window in a browser or global in Node.js). However, "in strict mode, when a function is called in the global scope, the value of this is set to undefined".

This change also applies to functions defined within modules. In JavaScript, modules are typically implemented using the import and export statements, which allow you to define and use reusable pieces of code. When strict mode is enabled within a module, the value of this inside that module is set to undefined by default.

The rationale behind this change in strict mode is to "prevent accidental use of the global object" (window or global) within a module. By setting this to undefined, it helps to identify and avoid potential errors that may occur when referencing the global object unintentionally.

In strict mode, if you explicitly want to access the global object within a module, you need to reference it explicitly, for example, by using window or global depending on the environment.

To summarize, strict mode modifies the behavior of this in JavaScript by setting it to undefined in the global scope and within modules. This change aims to prevent accidental use of the global object and promotes safer coding practices.

----------------------------------------------

(13) How does destructuring work with nested object or arrrays in Js, and what syntax is used to access deeply nested values?

const person = {
  name: 'John',
  age: 30,
  address: {
    street: '123 Main St',
    city: 'New York',
    country: 'USA'
  }
};

// Destructuring nested objects
const { name, age, address: { city, country } } = person;

console.log(name);    // Output: John
console.log(age);     // Output: 30
console.log(city);    // Output: New York
console.log(country); // Output: USA

----------------------------------------------------

(14) Can you use object shorthand syntax to declare a methode in an object? If so how?

const myObject = {
  name: 'John',
  age: 30,
  greet() {
    console.log(`Hello, my name is ${this.name} and I'm ${this.age} years old.`);
  }
};

myObject.greet(); // Output: Hello, my name is John and I'm 30 years old.

----------------------------------------------------------

(15) How can you use socket.io to create a chat app?

  Write socket.io server using Node.js and socket.io package and aslo create a client side HTML page that includes the socket.io client library.

----------------------------------------------------------

(16) Create a node.js app that uses the 'Chalk' module to display a coloured text message in the console.

npm install chalk

const chalk=requrie('chalk');

const message = chalk.blue('Hello, colored text!');

// Display the message in the console

console.log(message);

-------------------------------------------------------------

(17) What is diff b/w required and import statements in Node.js?

In Node.js, both require and import statements are used to include external modules or files into your application. However, there are some differences between them in terms of syntax and behavior:

Syntax:

'require' is the 'CommonJS' syntax used in Node.js "prior to version 13".
import is the ES modules (ESM) syntax introduced in ECMAScript 2015 (ES6) and supported in Node.js starting from version 13.

(1) File Extension:
"require does not require file extensions" and can load both JavaScript files (.js) and JSON files (.json) by default.

"import requires the use of file extensions", typically .js, and it does not directly support importing JSON files. However, you can use 'additional plugins' or tools like 'babel' or 'jsonc' to enable importing JSON files with import.

(2) Dynamic vs. Static:

"require allows for dynamic imports", which means that modules can be loaded conditionally at runtime based on variables or conditions.
"import is statically analyzed during the compilation process", meaning it should be used for static imports where the imported module or file is known at the time of writing the code. It does not support dynamic imports directly.

(3) Top-level vs. Local Scope:

"require is executed at the top-level scope of a module", meaning it can be placed anywhere within the module file, and the imported module's exports are available throughout the file.
import is scoped to the block or module where it is used. It must be placed at the top-level scope of a module file, and the imported module's exports are only available within the block or module where the import is used.

(6) Asynchronous vs. Synchronous:

"require is synchronous by default", meaning it will block the execution of further code until the required module is loaded and executed.
"import is asynchronous by default", which allows for better performance in certain scenarios. However, it requires the use of await or then to handle the asynchronous loading of modules.

(7) Default Exports:

require supports both named exports and default exports, and you can choose to import either of them using the require statement.
import also supports named exports and default exports, but the syntax for importing default exports is different. You need to use the import statement followed by a variable name, rather than curly braces for named exports.


-------------------------------------------------------------

(18) Write a jest test case to ckeck if the given array has milk in it.

    const shopList=['diapers','trash bags','milk'];

  Ans:

    const shopList = ['diapers', 'trash bags', 'milk'];

      test('Check if array has milk', () => {

        expect(shopList).toContain('milk');

      });

-------------------------------------------------------

(19) Whai is RESTful API?

=> REST stands for the ("Representational State Transfer") API is a web-based architectural style for creating API's.
  => REST APIs allow communication between different systems on the internet.
  => REST API's allows data to be exchanged between systems using HTTP and other standard web protocols.
  => "A REST API is designed around resources, which are identified by unique URLs".
  => Data can be any type of object, such as a user, a blog post, or a product. Each data is represented as a collection of properties or attributes, which can be manipulated using a standardized set of HTTP methods, such as GET (to retrieve a data), POST (to create a new data), PUT (to update an existing data), and DELETE (to remove a data).
  => "REST API is a stateless architecture", which means that the server does not store any information about the client's state between requests. Instead, "each request contains all the information necessary" for the server to understand and process the request.
  => REST API's  "allows developers to create flexible and scalable APIs" that can be used by a variety of different applications and systems.
  => REST API's are "easy to implement", as it uses standard web protocols and HTTP methods, and can be used with a wide range of programming languages and frameworks.
  => Advantages of REST API include its "scalability, flexibility, and compatibility "with a wide range of systems and devices.
  => However, designing and implementing a REST API can be complex, as it requires careful consideration of resource design, URL structure, and HTTP method usage. Additionally, security is also a concern, and proper authentication and authorization mechanisms must be implemented to prevent unauthorized access to resources.

--------------------------------------------------------

(20) What are HTTP methods?

HTTP (Hypertext Transfer Protocol) defines several methods or verbs that indicate the desired action to be performed on a resource. Each HTTP method has a specific purpose and behavior. Here are the most common HTTP methods:

    (1) GET: The GET method is used to retrieve a representation of a resource from the server. It requests the server to send back the specified resource in the response. "GET should be used for safe and idempotent operations, meaning 'it should not have any side effects on the server or the resource'."

    (2) POST: The POST method is used to submit data to the server to create a new resource. It sends data in the request payload, typically used for form submissions, file uploads, or other actions that result in a change on the server. "POST is not idempotent as multiple requests may create multiple resources."

    (3) PUT: The PUT method is used to update or replace an existing resource on the server. It sends the entire updated representation of the resource in the request payload. If the resource does not exist, PUT can create a new resource with the specified identifier. "PUT is idempotent, meaning that multiple identical requests should have the same result as a single request".

    (4) DELETE: The DELETE method is used to delete a specified resource on the server. It requests the server to remove the resource identified by the URL. "DELETE is idempotent, meaning that multiple identical requests should have the same result as a single request".

    (5) PATCH: The PATCH method is used to partially update a resource on the server. It sends a set of changes or instructions to be applied to the existing resource. Unlike PUT, which requires sending the entire updated representation, PATCH allows sending only the modified fields or instructions. "PATCH is not guaranteed to be idempotent."

    (6) HEAD: The HEAD method is similar to GET but only retrieves the response headers, without the response body. It is often used to retrieve metadata or check the status of a resource without downloading its entire contents.

    (7) OPTIONS: The OPTIONS method is used to retrieve the communication options available for a resource or server. It allows the client to determine the supported methods, headers, and other capabilities of the server.

    These are the most commonly used HTTP methods. It's important to note that the usage of these methods should align with their intended purpose and follow the principles of RESTful web services.


-------------------------------------------------------

(21) Difference between PUT and PATCH?

-----------------------------------------------------------

(22) How to handle error in websockets/socket.io?

Socket.IO is a real-time communication library for web applications. It allows you to send and receive data between clients and servers in a bi-directional manner.

    While Socket.IO is a powerful library, it can also be prone to errors. Here are some tips on how to handle errors in Socket.IO:

    (1) Use error events. Socket.IO emits error events whenever an error occurs. You can listen for these events and handle them accordingly.

          eg. 
                const io = require('socket.io')(server);

                  io.on('connection', (socket) => {

                    // Error handling within event handler

                    socket.on('customEvent', (data) => {

                      try {

                        // Your event handling logic here

                      } catch (error) {

                        // Handle the error

                        console.error('An error occurred:', error.message);
                        socket.emit('error', { message: 'An error occurred' });
                      }
                    });

                    // Error handling using the 'error' event
                    socket.on('error', (error) => {
                      console.error('Socket error:', error);
                    });
                  });

    (2) Use try/catch blocks. You can also use try/catch blocks to handle errors. This is a good way to catch errors that may not be emitted as events.

    (3) Log errors. It is always a good idea to log errors so that you can track them down and fix them.

    (4) Display error messages to users / Cliet Side Error handling. If an error occurs that prevents your application from working properly, you should display an error message to the user. This will help them understand what went wrong and what they can do to fix it.

      eg.

          var socket = io();

          socket.on('error', function(error) {

            console.log(error);

            // Display an error message to the user
            alert('An error occurred: ' + error);
          });

        This code will listen for error events emitted by the socket instance. When an error event is emitted, the error will be logged and an error message will be displayed to the user.

By following these tips, you can help to ensure that your Socket.IO applications are robust and handle errors gracefully.

----------------------------------------------------------

(23) How does Redis pub/sub work and what are some use cases for it?

Redis pub/sub (publish/subscribe) is a messaging pattern and feature provided by Redis, an in-memory data structure store. It "enables communication between different components of an application" by allowing publishers to send messages (publish) to specific channels, and subscribers to receive those messages from the channels they are interested in.

Here's a step-by-step overview of how Redis pub/sub works:

(1) "Publishers": Publishers send messages to channels. They don't need to know who, if anyone, is listening to their messages. Publishers only need to establish a connection with Redis and publish messages to the desired channels.

(2) "Channels": Channels act as message distribution centers. Messages sent by publishers are distributed to all subscribers that are currently subscribed to the corresponding channel.

(3) "Subscribers": Subscribers express interest in one or more channels by subscribing to them. Once subscribed, they receive messages sent to the subscribed channels in a "push" manner.

(4) "Message Distribution": Redis handles the distribution of messages from publishers to subscribers. Messages are not persisted; subscribers only receive messages that are published while they are actively subscribed to a channel.

Some common use cases for Redis pub/sub include:

"Real-time Updates": Pub/sub can be used to deliver real-time updates to clients or other parts of an application. For example, a chat application can use Redis pub/sub to deliver messages from one user to all other subscribed users in real-time.

"Event-driven Architecture": Redis pub/sub can be used to build an event-driven architecture, where different components of an application communicate with each other through events. Components can publish events to specific channels, and other components can subscribe to those channels to react to the events.

"Notifications": Pub/sub can be used for sending notifications to interested parties. For instance, a system can publish notifications related to certain events (e.g., new email arrival) to specific channels, and subscribers can receive and process those notifications accordingly.

"Distributed System Coordination": Pub/sub can facilitate coordination between different nodes or services in a distributed system. It enables components to communicate and synchronize their actions without direct dependencies or tight coupling.

"Broadcasting Updates": Pub/sub allows broadcasting updates to multiple clients or services simultaneously. For example, a news service can use Redis pub/sub to broadcast breaking news to all subscribed clients.

It's worth noting that while Redis pub/sub is a simple and effective messaging pattern, it has some limitations. It doesn't provide message persistence, historical message retrieval, or guaranteed delivery. If these features are required for your use case, you might need to consider other messaging systems or combine Redis pub/sub with additional mechanisms.

-------------------------------------------------------------

(24) How can you optimise the performance of a loop in Js, and what are some best practices to loop in mind?

When it comes to optimizing the performance of a loop in JavaScript, there are several strategies and best practices we can follow. Here are some tips:

(1) "Reduce unnecessary computations": Minimize the number of calculations performed within the loop. Move any computations that don't change inside the loop outside of it.

(2) "Cache loop length": If the length of the loop is constant, cache it in a variable outside the loop. This prevents the loop from checking the length on each iteration.

(3) "Use prefix increment/decrement": When incrementing or decrementing loop counters, use the prefix operator (++i or --i) instead of the postfix operator (i++ or i--). This can be slightly more efficient as it avoids creating a temporary copy of the variable.

(4) "Use efficient loop constructs": In general, for loops are more efficient than while or do-while loops. The for-in loop and for-of loop should be used for iterating over objects and arrays, respectively.

(5) "Avoid unnecessary function calls": If possible, avoid making function calls within the loop. Function calls can introduce additional overhead, especially if the function being called is complex or involves I/O operations.

(6) "Batch DOM operations": If you're performing DOM manipulation inside a loop, try to minimize the number of times you access or modify the DOM. Instead, batch the operations together and perform them outside the loop.

(7) "Use array methods": JavaScript provides several array methods like map(), filter(), reduce(), etc. These methods often offer more concise and optimized ways to perform common operations on arrays, reducing the need for manual loops.

(8) "Consider asynchronous operations": In certain scenarios, you might be able to use asynchronous operations like Web Workers or Promises to parallelize or offload heavy computations, improving overall performance.

(9) "Test and measure": Always benchmark and profile your code to determine the actual performance gains. Different optimizations may have varying impacts depending on the specific use case and JavaScript engine.

Remember, optimizing loops for performance should be done when necessary. Premature optimization can lead to code that is harder to read and maintain. Focus on writing clear, readable code first, and then optimize if performance becomes a bottleneck.

-------------------------------------------------------------

(25) If you given an integer value, how can you create buttons for pagination using the same? use example to demonstrate it.

// html
`<div id="pagination-container"></div>
`

const totalItems = 37;
const itemsPerPage = 10;
const container = document.getElementById("pagination-container");

const totalPages = Math.ceil(totalItems / itemsPerPage);
let page=1;
function getData(page){
  // fetch the data from api
}
getData(page);

function showPages(){

for (let i = 1; i <= totalPages; i++) {
  const button = document.createElement("button");
  button.textContent = i;
  container.appendChild(button);
  button.addEventListener("click", () => {
    // Perform pagination logic when a button is clicked
    getData(i);
    showPages(i)
    console.log("Button", i, "clicked");
  });
}
}
showPages()
-------------------------------------------------------------

(26) Write a program to send a message to specific client.

const http = require("http");
const express = require("express");
const app=express()

const server = http.createServer(app);

const { Server } = require("socket.io");

const io = new Server(server);

io.on('connection', (socket) => {
    console.log("a user connected");

    socket.on('chat message', (msg) => {
        io.emit('chat message', msg);
    })

    socket.on('disconnect',()=> {
        console.log('user disconnected');
    })
})

app.get("/", (req, res) => {
    res.sendFile(__dirname+'/index.html');
})
server.listen(3000, () => {
    console.log('server is running on port 3000');
})

<body>
<div id="navbar">
        <h1>Chat App</h1>
    </div>
    <div id="container">
        <ul id="messages"></ul>
        <form action="" id="form">
            <input type="text" id="input" autocomplete="off"/><button>Send</button>
        </form>
    </div>
    <script src="/socket.io/socket.io.js"></script>
    <script>
        var socket=io();
        let messages=document.getElementById('messages')
        let form=document.getElementById('form');
        let input = document.getElementById('input');
        form.addEventListener('submit',function(e){
            e.preventDefault();
            if(input.value){
                socket.emit('chat message', input.value);
                input.value=''
            }
        });

        socket.on('chat message',function(msg){
            let item=document.createElement('li');
            item.textContent=msg;
            messages.appendChild(item);
            window.scrollTo(0,document.body.scrollHeight);
        })
    </script>
</body>

---------------------------------------------------------

(27) What is the difference b/w redis set and sorted sets?

Redis is an in-memory data structure store that offers various data types, including sets and sorted sets. Here's the difference between Redis sets and sorted sets:

(A) Redis Sets:

Redis sets are an unordered collection of unique strings.
Each element in a set must be unique, and duplicate elements are not allowed.
Sets are optimized for adding, removing, and checking the existence of elements in constant time (O(1)).
You can perform set operations like union, intersection, and difference between sets.
Sets are useful for managing unique items, membership checks, and tagging.

(B) Redis Sorted Sets:

Redis sorted sets are similar to sets but each element is associated with a floating-point score.
Sorted sets are ordered based on the scores of the elements.
Each element in a sorted set is unique, but the scores can be repeated.
Sorted sets provide efficient operations like adding, removing, and updating elements in logarithmic time (O(logN)), where N is the number of elements in the sorted set.
You can retrieve elements from a sorted set based on their rank (position) or by specifying a range of scores.
Sorted sets are commonly used for leaderboard systems, ranking, range-based queries, and priority queues.
In summary, while both Redis sets and sorted sets store unique elements, sorted sets add an additional score to each element and maintain an ordered collection based on these scores. This ordering allows sorted sets to support a wider range of operations, such as ranking and range queries, compared to the unordered nature of sets.

------------------------------------------------------------

(28) What are advantages of using redis as a data strucure?

Redis, which stands for "Remote Dictionary Server", is an open-source, "in-memory data structure store". It offers several advantages as a data structure that contribute to its popularity and usefulness in various applications. Here are some advantages of using Redis:

(1) "High-performance in-memory storage": Redis "stores data primarily in memory", allowing for extremely fast read and write operations. It is designed to deliver high performance with low latencies, making it ideal for use cases that require real-time data processing or high-speed data access.

(2) "Versatile data structures": Redis provides a rich set of data structures that go beyond simple key-value pairs. It supports strings, lists, sets, sorted sets, hashes, and more. These data structures enable complex data modeling and allow developers to implement various algorithms efficiently.

(3) "Caching": Redis is commonly used as a caching layer to enhance the performance of applications. By storing frequently accessed data in Redis, you can reduce the load on databases and backend systems. With its fast access times and ability to handle large volumes of data, Redis can significantly improve the responsiveness and scalability of applications.

(4) "Pub/Sub messaging": Redis includes a publish/subscribe (pub/sub) messaging system. It allows different components of an application to communicate with each other asynchronously through channels. This feature is useful for building real-time applications, chat systems, and event-driven architectures.

(5) "Atomic operations":  meaning that" operations on complex data types are performed as a single step". This atomicity ensures data integrity, even in highly concurrent scenarios, and simplifies the development of multi-step operations or transactions.

(6) "Persistence options": While Redis stores data primarily in memory, it also offers persistence options to ensure data durability. It can periodically save snapshots of the in-memory dataset to disk or append every write operation to a log file. These features enable data recovery in case of system failures or restarts.

(7) "Scalability and clustering": Redis supports clustering, allowing you to distribute data across multiple nodes. This horizontal scalability ensures that Redis can handle large datasets and increasing workloads by leveraging multiple servers in a cluster. It also provides built-in replication and failover mechanisms for improved availability.

(8) "Extensive client libraries and ecosystem": Redis has a wide range of client libraries available for various programming languages, making it easy to integrate with different applications and systems. Additionally, Redis has a thriving ecosystem with community-contributed modules, adding functionalities such as full-text search, geospatial indexing, machine learning, and more.

These advantages make Redis a powerful and flexible choice for applications that require fast data access, caching, real-time messaging, and scalable data storage. However, it's important to consider your specific use case and requirements to determine if Redis is the right fit for your application.

------------------------------------------------------------

(29) What is room in socket.io?

In the context of Socket.io, a "room" refers to a feature that allows you to group connected sockets (clients) together so that you can broadcast messages to specific groups of clients. It provides a way to organize and manage the communication between clients based on logical groupings.

Rooms are useful in scenarios where you want to send a message to a specific subset of connected clients instead of broadcasting it to all clients. For example, in a chat application, you may have different chat rooms or channels, and you want to send messages only to the clients that have joined a particular room.

Socket.IO provides a simple API to create, join, and leave rooms. Here's an example of how you can use rooms in Socket.IO:


// Server-side code

io.on('connection', (socket) => {
  // Joining a room
  socket.join('room1');
# io.to('room1').emit('message', 'Welcome to room1!');
  // Broadcasting to a room
  socket.on('message',(msg)=>{
    io.to('room1').emit('message', msg);
  })
  

  // Leaving a room
  socket.leave('room1');
});

In the above example, when a client connects, it joins the 'room1' room using the join method. The server then sends a message only to the clients in the 'room1' room using the to method. Finally, the client can leave the room using the leave method.

Rooms are especially useful in scenarios where you need to implement features like private messaging, chat rooms, real-time updates for specific groups of users, or any situation where you want to target a subset of clients with specific messages.

By utilizing rooms, you can efficiently manage and control the flow of data between clients based on logical groupings, enhancing the flexibility and scalability of your Socket.IO-based applications.

-------------------------------------------------------------

(30) What is the value of 'this' in a callback function passed at an event listener?

const button = document.querySelector('button');

button.addEventListener('click', function() {
  console.log(this); // 'this' refers to the button element
});

const button = document.querySelector('button');

button.addEventListener('click', () => {
  console.log(this); // 'this' refers to the lexical context where the arrow function is defined
});
------------------------------------------------------------

(31) diff bw require and import?

------------------------------------------------------------

(32) Create an error handling middleware.

function errorHandler(err, req, res, next) {
  // Handle the error
  console.error(err);

  // Set an appropriate status code for the error
  res.status(500);

  // Send an error response to the client
  res.json({
    error: 'Internal Server Error'
  });
}

module.exports = errorHandler;

app.use(errorHandler);

---------------------------------------------------

(33) How can you delete the MongoDB shell?

Step 1
List out all the available databases by using the show dbs command.

Syntax:

show dbs

Step 2
Select and connect to the database which is to be deleted using the use command in the MongoDB shell.

Syntax:

use <database_name>

Step 3
Drop the connected database using the dropDatabase() function.

Syntax:

db.dropDatabase()

Step 4
Check the list of databases again now using the show command to confirm the deletion of the database.

Syntax:

show dbs

---------------------------------------------------

(34) What are advantages and Disadvantages of MongoDB referencing in relationships?

MongoDB offers two approaches for modeling relationships between documents: referencing and embedding. Let's focus on the advantages and disadvantages of referencing in MongoDB relationships.

(A) "Advantages of MongoDB Referencing":

(1) "Data Consistency": Referencing allows you to maintain consistency by avoiding data duplication. When multiple documents reference the same data, you only need to update the referenced document once, and the changes will be reflected in all the referring documents.

(2) "Query Performance": Referencing can be beneficial for certain types of queries. If you frequently need to query the referenced data separately from the referring document, referencing allows you to retrieve the referenced data without fetching the entire document hierarchy. This can result in better query performance.

(3) "Scalability": Referencing can help with scalability when dealing with large datasets. By separating the referenced data into separate collections, you can distribute the workload across different servers or shards, improving performance and allowing for easier horizontal scaling.

(4) "Flexibility": Referencing provides flexibility in managing relationships between documents. You can establish relationships between documents of different types and update them independently without affecting the structure of the referencing document.


(B) "Disadvantages of MongoDB Referencing":

(1) "Query Complexity": Referencing can introduce additional complexity to your queries. In order to retrieve the referenced data, you may need to perform additional queries or use complex aggregation pipelines. This can make queries more challenging to write and maintain.

(2) "Performance Impact": While referencing can improve performance for certain types of queries, it can also have a performance impact in other cases. Joining referenced data from multiple collections may require more computational resources and increase the query execution time.

(3) "Read Performance vs. Write Performance": Referencing can optimize read operations but may negatively impact write operations. When updating referenced data, you need to perform multiple operations across different collections, which can introduce additional overhead and potentially impact the write performance.

(4) "Lack of Atomicity": MongoDB does not provide built-in support for transactions across multiple collections. When using referencing, maintaining atomicity becomes more challenging as updates to multiple documents must be handled manually, increasing the risk of inconsistent data in case of failures or concurrent updates.

It's important to note that the choice between referencing and embedding in MongoDB depends on various factors, including the nature of your data, the types of queries you perform, and the scalability requirements of your application. It's recommended to carefully analyze your specific use case before deciding on the appropriate approach.

---------------------------------------------------

(35) How does react differ other Js frameworks?

"React is a JavaScript library for building user interfaces", and it differs from other JavaScript frameworks in several ways:

(1) "Component-Based Architecture": React is built around the concept of reusable components. Components are self-contained, independent units that encapsulate the structure and behavior of a part of the user interface. This modular approach makes it easier to build and maintain complex UIs.

(2) "Virtual DOM": React uses a virtual representation of the actual browser DOM. When changes are made to the state of a component, React creates a virtual DOM tree and efficiently updates only the necessary parts of the actual DOM, minimizing the number of direct manipulations to the browser DOM. This approach improves performance and enhances the user experience.

(3) "Unidirectional Data Flow": React follows a unidirectional data flow pattern, also known as one-way binding. Data flows in a single direction, from parent components to child components. "This makes it easier to understand how data changes propagate throughout the application", making "debugging and testing more manageable".

(4) "JSX": React introduces JSX, which is a syntax extension for JavaScript. JSX allows you to write HTML-like code directly in your JavaScript files, blending markup and logic together. It provides a concise and intuitive way to define the structure of React components.

(5) "Community and Ecosystem": React has a large and active community, with extensive third-party libraries and tools available for various tasks, such as state management (Redux, MobX), routing (React Router), and testing (Jest, Enzyme). The ecosystem around React is robust, which can provide additional support and resources for developers.

(6) "Focus on UI": Unlike some full-fledged frameworks like Angular, React focuses solely on the user interface layer. "It doesn't provide out-of-the-box solutions for features like HTTP requests, form validation, or dependency injection. React can be easily integrated with other libraries or frameworks to handle these aspects as needed".

Overall, React's component-based architecture, virtual DOM, unidirectional data flow, JSX syntax, thriving community, and UI-centric approach differentiate it from other JavaScript frameworks, allowing developers to build dynamic and efficient user interfaces with ease.

--------------------------------------------------------------

(36) How you will use useEffect to clean up resources when an components unmounts?

In React, you can use the useEffect hook to clean up resources when a component unmounts by returning a cleanup function. Here's an example of how you can accomplish this:


import React, { useEffect } from 'react';

function MyComponent() {

  useEffect(() => {
    // Perform setup and side effects here

    return () => {
      // Clean up resources when the component unmounts
      // This function will be called before the component is removed from the DOM
      // Perform cleanup tasks such as cancelling subscriptions, removing event listeners, etc.
    };
  }, []);

  // Rest of the component code

  return (
    // JSX markup for the component
  );
}

In this example, the useEffect hook is used with an empty dependency array ([]) as the second argument. This empty dependency array ensures that the effect is only run once, when the component mounts, and the cleanup function is called only when the component unmounts.

Inside the useEffect callback function, you can perform any setup or side effects that you need. This can include things like event listeners, subscriptions, timers, or other resource allocations.

The cleanup function is returned from the useEffect callback function. This function will be called when the component is about to unmount, right before the new render cycle starts. Inside the cleanup function, you can perform any necessary cleanup tasks such as unsubscribing from event listeners, canceling timers, or releasing any other resources that were allocated during the component's lifetime.

By including the cleanup logic in the cleanup function, you ensure that the resources are properly cleaned up and any potential memory leaks or unwanted side effects are avoided when the component is unmounted.


eg.

import React, { useEffect } from 'react';

function TimerComponent() {

  useEffect(() => {
    const timer = setInterval(() => {
      console.log('Timer ticked!');
    }, 1000);

    return () => {
      clearInterval(timer);
      console.log('Timer cleaned up!');
    };
  }, []);

  return (
    <div>
      <h1>Timer Component</h1>
      {/* Component JSX */}
    </div>
  );
}

export default TimerComponent;

In this example, we have a TimerComponent that starts a timer using setInterval in the useEffect hook. The timer logs a message every second.

When the component mounts, the useEffect hook is called and the timer is set up. Since we're passing an empty dependency array ([]), this effect runs only once during the initial component mount.

When the component is about to unmount, the cleanup function specified within the useEffect hook is called. In this case, the cleanup function clears the interval using clearInterval and logs a message to indicate that the timer has been cleaned up.

This ensures that the timer is properly stopped and any associated resources are cleaned up when the TimerComponent is unmounted, preventing memory leaks or unwanted side effects.

Remember that the cleanup function defined within the useEffect hook is optional. You can choose to omit it if you don't have any resources to clean up when the component unmounts.

---------------------------------------------------------------

(37) What are nested objects in Js and how can you access their properties, explain with an example?

const person = {
  name: 'John',
  age: 30,
  address: {
    street: '123 Main St',
    city: 'New York',
    country: 'USA'
  }
};

// Accessing nested object properties using "dot notation"

console.log(person.name); // Output: John
console.log(person.age); // Output: 30
console.log(person.address.street); // Output: 123 Main St
console.log(person.address.city); // Output: New York
console.log(person.address.country); // Output: USA

// Accessing nested object properties using "bracket notation"

console.log(person['name']); // Output: John
console.log(person['age']); // Output: 30
console.log(person['address']['street']); // Output: 123 Main St
console.log(person['address']['city']); // Output: New York
console.log(person['address']['country']); // Output: US

---------------------------------------------------------------

(38) How do you freeze an Object?

const obj = {
  prop1: 'value1',
  prop2: 'value2'
};

Object.freeze(obj);

// Trying to modify properties

obj.prop1 = 'new value'; // This change will not take effect

delete obj.prop2; // This deletion will not take effect

obj.prop3 = 'value3'; // This addition will not take effect

console.log(obj.prop1); // Output: value1
console.log(obj.prop2); // Output: value2
console.log(obj.prop3); // Output: undefined

---------------------------------------------------------------

(39) What are some "common mistakes" to avoid when "using query and route params in Node.js applications"?

When using query and route parameters in Node.js applications, it's important to be aware of potential pitfalls and mistakes that can lead to security vulnerabilities or incorrect behavior. Here are some common mistakes to avoid:

(1) "Insufficient input validation": Failing to validate and sanitize user input can leave your application vulnerable to attacks like SQL injection( SQL injection is a type of security vulnerability that occurs when an attacker can manipulate or inject malicious SQL  statements into an application's database query. It is a common web application security risk, especially when user-supplied input is not properly validated or sanitized before being used in SQL queries.) or cross-site scripting (XSS)(Cross-Site Scripting is another common web application security vulnerability that allows attackers to inject malicious scripts into web pages viewed by other users. It occurs when an application fails to properly validate or sanitize user-supplied input that is dynamically included in web pages.). Always validate and sanitize query and route parameters before using them in your code.

(2) "Insecure concatenation": Avoid directly concatenating query or route parameters into database queries or other sensitive operations. Instead, use parameterized queries or prepared statements to prevent SQL injection attacks.

(3) "Inconsistent parameter naming": Be consistent with your parameter naming conventions. Mixing different naming styles or using ambiguous names can make your code harder to understand and maintain.

(5) "Not handling missing parameters": Always handle scenarios where query or route parameters are missing. Failing to do so can lead to unexpected errors or crashes in your application.

(6) "Failure to handle type coercion": "Query parameters are often passed as strings", so be cautious when performing operations that "expect specific data types". Use appropriate type coercion or validation to ensure the correct data type is used.

(7) "Incorrect use of URL encoding": When passing special characters or spaces in query or route parameters, make sure to properly encode them using URL encoding (e.g., encodeURIComponent() in Node.js). Failing to do so can lead to malformed URLs and incorrect parameter values.

(8) "Overly complex routes or query strings": Avoid creating overly complex or lengthy routes and query strings. This can make your code harder to read, understand, and maintain. Consider breaking down complex queries into smaller, more manageable pieces.

(9) "Ignoring security considerations": Always consider security implications when handling query and route parameters. Implement measures such as "rate limiting, authentication, and authorization" checks to protect against malicious attacks.

(10) "Lack of documentation": Document the expected format and purpose of query and route parameters in your API documentation or code comments. This helps other developers understand how to use and interact with your endpoints correctly.

By being mindful of these common mistakes, you can enhance the security, reliability, and maintainability of your Node.js applications that use query and route parameters.

---------------------------------------------------------------

(40) Set up an Express server than can handle HTTP GET request to the '/old-url' path and redirect them to the '/new-url' path. The server must also handle HTTP get request to the '/new-url' path and send a response with the message "This is new URL".

const express = require('express');
const app = express();
const PORT = 3000;

// Redirect from /old-url to /new-url

app.get('/old-url', (req, res) => {
  res.redirect('/new-url');
});

// Handle GET request to /new-url

app.get('/new-url', (req, res) => {
  res.send('This is new URL');
});

// Start the server
app.listen(PORT, () => {
  console.log(`Server is running on port ${PORT}`);
});

---------------------------------------------------------------

(41) Write code that uses middleware to implement rate limiting in an API.

const rate = (limit, time, blockedTime) => {
    // We define a new function called `rateLimit` which takes three arguments: 
    // `limit` - the maximum number of requests allowed within `time` milliseconds.
    // `time` - the duration of the time window in milliseconds.
    // `blockedTime` - the duration of the blocked period in milliseconds.

    const requests = {};
    // We create an object called `requests` to store the request history for each IP address.

    setInterval(() => {
        // We create a new interval which runs every `time` milliseconds to remove old requests from the `requests` object.

        for (const ip in requests) {
            // We loop through each IP address in the `requests` object.

            const now = Date.now();
            // We get the current timestamp.

            const timeWindow = requests[ip].timeWindow;
            // We get the start of the current time window for the IP address.

            const blockedUntil = requests[ip].blockedUntil;
            // We get the end of the blocked period for the IP address.

            requests[ip].requests = requests[ip].requests.filter((time) => {
                return time > now - timeWindow;
            });
            // We remove all requests from the `requests` array for the IP address which are older than `timeWindow` milliseconds.

            if (requests[ip].requests.length === 0) {
                requests[ip].timeWindow = now;
            }
            // If there are no requests in the current time window for the IP address, we reset the start of the time window to the current timestamp.

            if (blockedUntil && blockedUntil <= now) {
                requests[ip].blockedUntil = null;
                requests[ip].requests = [];
                requests[ip].timeWindow = now;
            }
            // If the blocked period for the IP address has ended, we reset the request count, the start of the time window, and the end of the blocked period.
        }
    }, time);
    // We set the interval to run every `time` milliseconds.

    return (req, res, next) => {
        // We return a middleware function which takes three arguments: `req`, `res`, and `next`.

        const ip = req.ip;
        // We get the IP address of the client making the request.

        const now = Date.now();
        // We get the current timestamp.

        requests[ip] = requests[ip] || { requests: [], timeWindow: now };
        // If there is no entry in the `requests` object for the IP address, we create one and initialize the `requests` array and the start of the time window.

        if (requests[ip].blockedUntil && requests[ip].blockedUntil > now) {
            const remainingTime = Math.ceil((requests[ip].blockedUntil - now) / 1000);
            res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
        }
        // If the client is currently blocked, we send a 429 status code with a message indicating how long the client must wait before making more requests.

        else if (requests[ip].requests.length >= limit) {
            requests[ip].blockedUntil = now + blockedTime;
            const remainingTime = Math.ceil(blockedTime / 1000);
            res.status(429).send(`Too many requests, please try again after ${remainingTime} seconds.`);
            // If the client has reached the request limit, we block the client for the specified `blockedTime` and send a 429 status code with a message indicating how long the client must wait before making more requests
        } else {
            requests[ip].requests.push(now);
            next();
        }
    }
};

module.exports={rate}   

---------------------------------------------------------------

(42) Implement an embeded relationship in Node.js and MongoDB using a blog post and comments. So that you can create a new post document with comments. Write the schema.

----------------------------------------------------------------

(43) Which type of database is typically used for trasanctional applications. Such as banking system or e-commerce websites?

For transactional applications such as banking systems or e-commerce websites, relational databases (RDBMS) are typically used. Relational databases provide a structured and robust way to handle transactions and ensure data integrity. The ACID (Atomicity, Consistency, Isolation, Durability) properties of relational databases make them well-suited for applications that require reliable and consistent transaction processing.

Relational databases offer features like" transaction management, concurrency control, and data integrity constraints that are crucial for maintaining the integrity of financial and e-commerce transactions". They provide a "strong schema enforcement mechanism", allowing you to define the structure of your data and enforce relationships between different entities.

Some popular relational database management systems (RDBMS) commonly used for transactional applications include:

-- Oracle Database
-- MySQL
-- PostgreSQL
-- Microsoft SQL Server
-- IBM DB2
-- SQLite

These databases provide robust transactional capabilities, support for complex queries, indexing, and scalability options to handle high volumes of transactions efficiently.

It's worth noting that while relational databases are the traditional choice for transactional applications, there has been an increasing adoption of 'NoSQL' databases (such as MongoDB, Cassandra, or Redis) in certain use cases "where scalability, high-speed data retrieval, and flexible data models are prioritized over strict ACID compliance". However, for applications where transactional integrity and consistency are critical, relational databases remain the preferred choice.
---------------------------------------------------------------

(44) What are main components of HTTP requests in Node.js?

In Node.js, when making HTTP requests, you typically use the http or https module to send the requests. The main components of an HTTP request in Node.js include:

"URL": The Uniform Resource Locator (URL) specifies the address of the resource you want to interact with. It includes the protocol (e.g., "http://" or "https://"), hostname, port, path, query parameters, and fragment identifier.

"HTTP Method": The HTTP method indicates the type of operation you want to perform on the resource. Common HTTP methods are GET, POST, PUT, DELETE, etc.

"Headers": Headers contain additional information about the request or the client making the request. They include details like content type, user agent, authentication credentials, and more. Headers are sent as key-value pairs in the request.

"Body" (optional): The request body carries the payload or data associated with the request. It is used when performing operations like sending data for form submissions, creating or updating resources, or sending JSON/XML payloads.

To make HTTP requests in Node.js, you can use the http or https module's request method. Here's an example of making an HTTP POST request with some basic components:

const http = require('http');

const options = {
  hostname: 'api.example.com',
  port: 80,
  path: '/endpoint',
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer your_token'
  },
};

const data = JSON.stringify({ key: 'value' });

const req = http.request(options, (res) => {
  console.log(`Status Code: ${res.statusCode}`);

  res.on('data', (chunk) => {
    console.log(`Response: ${chunk}`);
  });
});

req.on('error', (error) => {
  console.error(`Error: ${error}`);
});

req.write(data);
req.end();

---------------------------------------------------------------

(45) How can you use the useEffect hook to fetch data from an API when a component mounts?

function MyComponent() {
  const [data, setData] = useState([]);

  useEffect(() => {
    const fetchData = async () => {
      try {

        const response = await fetch('https://api.example.com/data');

        const data = await response.json();

        setData(data);

      } catch (error) {
        console.log(error.message)
      }
    };

    fetchData();
  }, []);

  console.log(data);

  // Rest of the component code, use the 'data' state as needed
}

---------------------------------------------------------------

(46) What is concurrency in Node.js ?

Concurrency in Node.js refers to the "ability of Node.js to handle multiple tasks simultaneously, allowing for efficient utilization of system resources". 
-- Node.js is built on an "event-driven, non-blocking I/O model", which means it "can handle concurrent operations without blocking the execution of other tasks".

"Node.js achieves concurrency through the event loop and asynchronous programming". 
-- The event loop is a mechanism that handles events and callbacks, allowing Node.js to perform I/O operations without blocking the execution of other code. When an asynchronous operation is initiated, such as reading a file or making an HTTP request, Node.js registers a callback function and continues executing other tasks. When the asynchronous operation completes, the callback is invoked, and the result is processed.

Here are some key points to understand concurrency in Node.js:

"Event Loop": The event loop is at the core of Node.js concurrency. It continuously checks for events and executes associated callbacks in a loop. This allows Node.js to efficiently handle multiple I/O operations concurrently.

"Non-blocking I/O": Node.js uses non-blocking I/O operations, which means that when a request is made, it doesn't wait for the response before moving on to the next task. This allows Node.js to process other tasks while waiting for I/O operations to complete.

"Asynchronous APIs": Node.js provides a rich set of asynchronous APIs for performing I/O operations, such as reading/writing files, making network requests, and interacting with databases. These APIs allow developers to write non-blocking code and leverage concurrency effectively.

"Callbacks and Promises": To handle the results of asynchronous operations, Node.js commonly uses callbacks or promises. Callbacks are functions passed as arguments to asynchronous functions, which are invoked once the operation completes. Promises provide a more structured way of handling asynchronous operations, allowing for better error handling and chaining of multiple asynchronous operations.

"Worker Threads": Starting from Node.js version 10, Node.js also provides an experimental feature called Worker Threads. Worker Threads allow developers to spawn additional JavaScript threads, enabling true multithreading in Node.js. This can be useful for CPU-intensive tasks that can benefit from parallel execution.

It's important to note that while Node.js is highly concurrent, it doesn't automatically parallelize CPU-bound tasks across multiple cores. For CPU-intensive operations, using Worker Threads or implementing a cluster of Node.js processes can be considered.

Overall, Node.js's concurrency model, based on the event loop and asynchronous programming, enables efficient handling of concurrent tasks, making it well-suited for I/O-bound applications and systems with high scalability requirements.

------------------------------------------------------------------------------------------------------------------------------------------

(47) Build setTimeout function using the setInterval();

function setTimeout(func, delay, ...args) {
  let timerId = setInterval(() => {
    func.apply(null, args);
    clearInterval(timerId);
  }, delay);
}

// Usage example:
function myFunction() {
  console.log("Hello, world!");
}

setTimeout(myFunction, 2000);


------------------------------------------------------------------

(48) implement setInterval() function using the setTimeout()  in Js.

function setInterval(func, delay, ...args) {
  function interval() {
    func.apply(null, args);
    setTimeout(interval, delay);
  }
  
  setTimeout(interval, delay);
}

// Usage example:
function myFunction() {
  console.log("Hello, world!");
}

setInterval(myFunction, 2000);

---------------------------------------------------------------

(49) What are callback functions in Js and how are they used?

In JavaScript, a callback function is a function that is passed as an argument to another function and is executed later, usually after an asynchronous operation or an event occurs. The callback function allows you to specify what should happen after a certain operation or event completes.

Here's an example to illustrate the concept of callback functions:


function fetchData(callback) {
  
  // Simulating an asynchronous operation (e.g., fetching data from a server)

  setTimeout(() => {
    const data = ['apple', 'banana', 'orange'];
    callback(data);
  }, 2000);
}

function processData(data) {
  console.log('Processing data:', data);
}

fetchData(processData);

In the code above, we have a function fetchData that simulates an asynchronous operation (e.g., fetching data from a server). It takes a callback function as an argument. Inside fetchData, after the data is retrieved (simulated using setTimeout here), the callback function is invoked with the retrieved data as the argument.

The processData function is defined separately as the callback function. It receives the retrieved data and logs a message indicating that it is processing the data.

When we call fetchData and pass processData as the callback function, the fetchData function starts the asynchronous operation and specifies that after the operation completes (in this case, after a delay of 2 seconds), it should invoke the provided callback function (processData) with the retrieved data.

The use of callback functions is common in JavaScript, especially when dealing with asynchronous tasks such as making API requests, reading files, or handling events. By utilizing callbacks, you can control the flow of your program and define the actions to be performed once an operation or event finishes. Callbacks allow you to handle the result or response of asynchronous operations and enable non-blocking behavior in JavaScript applications

---------------------------------------------------------------

(49) How does the 'this' keyword relate to lexical scope in js?

---------------------------------------------------------------

(50) Write a Node.js code snippet to install and use the 'axios' module to make an HTTP GET request to a remote API and log the response.

// npm install axios

const axios = require('axios');

axios.get('https://api.example.com/data')
  .then((response) => {
    console.log(response.data);
  })
  .catch((error) => {
    console.error('Error:', error.message);
  });
----------------------------------------------------------------

(51) What is the purpose of JWT for authentication in Node.js and MongoDB?

JWT (JSON Web Tokens) is a popular method of authentication in Node.js and MongoDB applications. Its purpose is to securely transmit and verify claims or information between two parties—the client (typically a web or mobile application) and the server.

Here are some key purposes of JWT for authentication:

"Stateless Authentication": JWT allows for stateless authentication, meaning the "server does not need to store any session data or authenticate against a session store". Each request from the client contains the necessary information for the server to authenticate and authorize the user.

"Secure Information Exchange": JWTs are digitally signed using a secret or public/private key pair. This signature ensures that the token has not been tampered with during transmission and can be trusted by the server. The server can verify the authenticity and integrity of the token using the provided signature.

"Compact and Portable": JWTs are compact in size and can be easily transmitted as part of an HTTP header or in the URL query parameters. This makes them highly portable and suitable for stateless communication between different systems and microservices.

"Stateless Server": JWT authentication enables the server to remain stateless, which can improve scalability by eliminating the need to store session data on the server. The server can verify the JWT and extract the necessary information from the token itself, such as the user ID or roles, to make authorization decisions.

Flexibility and Extensibility: JWTs can include additional custom claims or metadata, allowing for flexible authentication and authorization scenarios. You can include information like user roles, permissions, or any other relevant data within the JWT payload.

"Decentralized Authentication": With JWT, authentication can be decentralized, meaning that different services or microservices can issue and validate tokens independently. This decentralization enables a more flexible and distributed architecture.

In a Node.js and MongoDB application, JWT can be used as a token-based authentication mechanism. When a user logs in or authenticates, the server issues a JWT containing relevant information about the user. The client receives this JWT and includes it in subsequent requests to the server. The server then verifies the JWT and extracts the necessary information to authenticate and authorize the user for each request.

Overall, JWT provides a secure and efficient way to authenticate and authorize users in Node.js and MongoDB applications while maintaining scalability and flexibility in a stateless manner

----------------------------------------------------------------

(52) What are the advantages and disadvantages of referencing in MongoDb relationship? Write three points for each?

Advantages of Referencing in MongoDB Relationships:

Flexibility and Scalability: Referencing allows for flexible and scalable data modeling in MongoDB. You can easily establish relationships between different collections and manage complex data structures.

Data Consistency: Referencing helps maintain data consistency by avoiding data duplication. Instead of duplicating data across multiple documents, you store references to related documents, ensuring that changes made to a referenced document are reflected across all references.

Efficient Memory Usage: Referencing consumes less memory compared to embedding large amounts of data within a single document. This can be particularly beneficial when dealing with large collections or frequently accessed documents.

Simplified Updates: With referencing, updating related data becomes easier. You only need to update a single document rather than multiple embedded documents, reducing the risk of inconsistencies or errors.

Easier Queries and Indexing: Referencing simplifies querying and indexing in MongoDB. You can use the reference IDs to fetch related data efficiently, allowing for better performance and query optimization.

Disadvantages of Referencing in MongoDB Relationships:

Additional Queries: Referencing requires additional queries to retrieve related data. When you need to access related documents, you have to perform separate queries to fetch the referenced data. This can introduce additional overhead, especially in complex relationships or scenarios where data needs to be fetched from multiple collections.

Possible Performance Impact: While referencing can be efficient for retrieving related data individually, it may introduce performance overhead when fetching data from multiple collections in complex queries. Joining or aggregating data across different collections can result in slower query execution times compared to embedding data within a single document.

Data Integrity Constraints: MongoDB does not provide built-in support for enforcing referential integrity constraints. It is the application's responsibility to ensure that referenced documents are valid and exist. Failure to do so can lead to orphaned references or inconsistencies in the data.

Lack of Atomicity: Updates involving multiple collections can become complex and lack atomicity. If you need to update data across multiple documents in a consistent manner, embedding the related data within a single document may provide better atomicity.

Schema Changes: Referencing can make schema changes more challenging. When modifying a referenced document's schema, you may need to update all references to ensure data consistency. This can be cumbersome in large databases or applications with extensive relationships.

It's important to consider your specific application requirements, data access patterns, and performance considerations when deciding whether to use referencing in MongoDB relationships. The choice between referencing and embedding depends on factors such as data size, data consistency requirements, querying patterns, and the complexity of relationships within your application.

----------------------------------------------------------------

(53) How would you use TimeLogger middleware to identify bottleneck's performance? Demonstrate through working code.

// timeLogger.js
function TimeLogger(req, res, next) {
  const start = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - start;
    console.log(`[${req.method}] ${req.url} - ${duration}ms`);
  });
  next();
}

module.exports = TimeLogger;

app.use(TimeLogger);

// Your routes and middleware definitions
app.get('/', (req, res) => {
  res.send('Hello, world!');
});
------------------------------------------------------------------

(54) Write function query records in a many-to-many relationship in node.js and MongoDB. Write a psuedo-code using tag and post example, assume relevat structure.schema wherever applicable.

// tag.model.js

const mongoose = require('mongoose');

const tagSchema = new mongoose.Schema({
  name: { 
      type: String,
      required: true 
    },
});

module.exports = mongoose.model('Tag', tagSchema);


// post.model.js
const mongoose = require('mongoose');

const postSchema = new mongoose.Schema({
  title: {
      type: String,
      required: true
      },
});

module.exports = mongoose.model('Post', postSchema);



/ postTag.model.js
const mongoose = require('mongoose');

const postTagSchema = new mongoose.Schema({
  postId: { 
    type: mongoose.Schema.Types.ObjectId,
    ref: 'Post', 
    required: true },
  tagId: { 
    type: mongoose.Schema.Types.ObjectId,
    ref: 'Tag',
    required: true }
});

module.exports = mongoose.model('PostTag', postTagSchema);


// postsController.js
const Post = require('./post.model');
const PostTag = require('./postTag.model');

async function getPostsByTag(tagName) {
  try {
    // Find the tag with the given name
    const tag = await Tag.findOne({ name: tagName });

    if (!tag) {
      return []; // If tag not found, return an empty array
    }

    // Find all post-tag relationships for the given tag
    const postTags = await PostTag.find({ tagId: tag._id });

    // Extract the post IDs from the post-tag relationships
    const postIds = postTags.map((postTag) => postTag.postId);

    // Find all posts with the extracted post IDs
    const posts = await Post.find({ _id: { $in: postIds } });

    return posts;
  } catch (error) {
    console.error('Error getting posts by tag:', error);
    throw error;
  }
}


// Usage
const posts = await getPostsByTag('example');
console.log(posts);
-----------------------------------------------------------------

